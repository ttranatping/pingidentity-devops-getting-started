{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>This documentation exists to enable DevOps professionals, administrators, and developers to deploy Ping Identity software using container technologies. Our goal is to provide tools, frameworks, blueprints, and reference architectures in support of running our products in containerized environments.</p> <ul> <li> <p>First time here?  We recommend the Get Started page.</p> </li> <li> <p>New to Kubernetes? See Kubernetes Basics</p> </li> <li> <p>New to Helm?  See Helm Basics</p> </li> <li> <p>Important information about container logging</p> </li> </ul>"},{"location":"#benefits-from-this-program","title":"Benefits from this program","text":"<ul> <li> <p>Streamlined Deployments</p> <p>Deploy and run workloads on our solutions without the need for additional hardware or virtual machines (VMs).</p> </li> <li> <p>Consistent and Flexible</p> <p>Maintain all configurations and dependencies, ensuring consistent environments. Containers are portable and can be used on nearly any machine.</p> </li> <li> <p>Optimized Sizing</p> <p>Orchestration of containers allows organizations to increase fault tolerance and availability and to better manage costs by auto-scaling to application demand.</p> </li> </ul>"},{"location":"#resources","title":"Resources","text":"<p>Resources provided include Docker images of Ping Identity products, deployment examples, and configuration management tools.</p>"},{"location":"#docker-images","title":"Docker Images","text":"Docker Images Docker Builds <p>Ping provides preconfigured Docker images of our products for running as containers. Each of our containers is a complete working product instance that is immediately usable when deployed. Our Docker stacks are integrated collections of Ping products preconfigured to coordinate across all containers in the stack.</p> <p>By default, our Docker images run as an unprivileged user in the container.</p> <p>You can find information about our available Docker images in the pingidentity-docker-builds repository on Github or on the Docker Hub site.  Included in this portal are detailed image specifications on variables, related images and so on.</p> <p>The Docker images are automatically pulled from our repository the first time you deploy a product container or orchestrated set of containers. Alternatively, you can pull the images manually from our Docker Hub site.</p> <p>Removal of older images from Docker Hub</p> <p>Older images based on product versions that are no longer supported under our policy are removed from Docker Hub.  See the support policy page for details.</p>"},{"location":"#deployment-examples","title":"Deployment Examples","text":"DevOps Getting Started <p>The Github repository linked here provides examples for deploying our products as standalone containers or as an orchestrated deployment in Kubernetes (using Helm).</p> <p>Docker Compose is often used for development, demonstrations, and lightweight orchestration. Kubernetes is typically used for enterprise-level orchestration.</p>"},{"location":"#configuration-management","title":"Configuration Management","text":"<p>For configuration management, we use:</p> <ul> <li>Server profiles, for runtime configuration of containers.</li> <li>YAML files for runtime configuration of stacks. YAML file configuration settings complement those provided through server profiles.</li> <li>Environment variables. These can be included in YAML files or called from external files.</li> <li>Shell scripts (hooks) to automate certain operations for a product.</li> <li>Release tags to give you a choice between stable builds or the current (potentially unstable) builds.</li> </ul> <p>More information about how server profiles, variables and these other options coordinate to configure the products can be found on the Configuration Reference page.</p>"},{"location":"#other-resources","title":"Other Resources","text":"All Ping DevOps Github Repos Ping Helm Charts"},{"location":"overview/","title":"Overview","text":"<p>The DevOps resources include Docker images of Ping Identity products, deployment examples, and configuration management tools.</p> <p>When you're ready, begin with our Get Started guide. Our documentation will help set you up and familiarize you with the use of the resources.</p>"},{"location":"overview/#devops-docker-images","title":"DevOps Docker Images","text":"Docker Images Docker Builds <p>We make available preconfigured Docker images of our products in Docker containers. Each of our containers is a complete working product instance, immediately usable when deployed. Our Docker stacks are integrated collections of these containers, preconfigured to interoperate with the containers in the stack.</p> <p>You can find information about our available Docker images in the pingidentity-docker-builds repository or on our Docker Hub site.</p> <p>The Docker images are automatically pulled from our repository the first time you deploy a product container or orchestrated set of containers. Alternatively, you can pull the images from our Docker Hub site.</p>"},{"location":"overview/#deployment-examples","title":"Deployment Examples","text":"DevOps Getting Started <p>We supply examples for deploying our products as standalone containers, as a Docker Compose stack, or as an orchestrated set using Kubernetes.</p> <p>Use Docker Compose for development, demonstrations, and lightweight orchestration. Use Kubernetes for enterprise-level orchestration.</p>"},{"location":"overview/#configuration-management","title":"Configuration Management","text":"<p>For configuration management, we use:</p> <ul> <li>Server profiles, for runtime configuration of containers.</li> <li>YAML files for runtime configuration of stacks. YAML file configuration settings complement that used for server profiles.</li> <li>Environment variables. These can be included in YAML files or called from external files.</li> <li>Shell scripts (hooks) to automate certain operations for a product.</li> <li>Release tags to give you a choice between stable builds or the current (potentially unstable) builds.</li> </ul> <p>By default, our Docker images run as unprivileged within the container.</p>"},{"location":"contact-us/community/","title":"Community","text":"<p>Ping Identity maintains a community where you can ask questions of Ping employees and other users of our products.</p> <p>You can submit your questions at the Cloud DevOps Community.</p>"},{"location":"contact-us/contributing/","title":"Contributing","text":"<p>Thanks for taking the time to help improve our tools!</p>"},{"location":"contact-us/contributing/#how-can-i-contribute","title":"How Can I Contribute?","text":""},{"location":"contact-us/contributing/#reporting-bugs","title":"Reporting Bugs","text":""},{"location":"contact-us/contributing/#how-do-i-submit-a-bug-report","title":"How Do I Submit a Bug Report?","text":"<p>Bugs are tracked as GitHub Issues. You can report a bug by submitting an issue in the Ping Identity DevOps Issue Tracker. To help the maintainers understand and reproduce the problem, please provide information such as:</p> <ul> <li>A clear and descriptive title.</li> <li>A description of what happened and a description of what you expected to happen.</li> <li>An example with the exact steps needed to reproduce the problem. If relevant, provide sample code.</li> </ul> <p>Please understand that bug reports are reviewed and prioritized internally, and we might not be able to address all bug reports or provide an estimated time for resolution.</p>"},{"location":"contact-us/contributing/#suggesting-enhancements","title":"Suggesting Enhancements","text":"<p>As with bugs, requests are tracked as GitHub Issues. You can suggest an enhancement by submitting an issue in the Ping Identity DevOps Issue Tracker.</p> <p>Please understand that enhancement requests are reviewed and prioritized internally, and we might not be able to address all requests or provide an estimated time for resolution.</p>"},{"location":"contact-us/contributing/#alternate-route-for-submitting-bugs-and-suggesting-enhancements","title":"Alternate Route for Submitting Bugs and Suggesting Enhancements","text":"<p>If you would rather not have your issue discussed on the public repository, you can open an issue from Ping Identity's Support Portal.</p>"},{"location":"contact-us/contributing/#contributing-code-changes","title":"Contributing Code Changes","text":"<p>Ping Identity does not accept third-party code submissions.</p>"},{"location":"deployment/deployCompose/","title":"Single product examples only","text":"<p>Deprecation</p> <p>Docker Compose was used by Ping in the past for basic orchestration and examples.  We are no longer maintaining multi-product or clustering docker compose examples. All of those files have been removed from this repository. The only examples remaining are for deploying individual products.  For orchestration of multiple products, clustering and other use cases, use helm to deploy to Kubernetes.</p> <p>Example docker compose files to deploy standalone instances of PingAccess, PingCentral, PingDirectory or PingFederate are in the Github repository. Refer to the comments in each provided file for instructions on accessing the product after running <code>docker compose up</code> from the directory of the product in which you are interested.</p> <p>For more information about the structure of Docker Compose YAML files provided by Ping, see this page</p>"},{"location":"deployment/deployFullK8s/","title":"Deploy a robust local Kubernetes Cluster","text":"<p>Video Demonstration</p> <p>A video demonstrating the manual process outlined on this page is available here.  An updated video using the ansible playbooks is planned.</p> <p>In some cases, a single-node cluster is insufficient for more complex testing scenarios.  If you do not have access to a managed Kubernetes cluster and want something more similar to what you would find in a production environment, this guide can help.</p> <p>This document describes deploying a multi-node cluster using ansible and the kubeadm utility, running under virtual machines. When completed, the cluster will consist of:</p> <ul> <li>3 nodes, a master with two worker nodes (to conserve resources, the master will also be configured to run workloads)</li> <li>(At the time of this writing) Kubernetes 1.28.2 using the containerd runtime (no Docker installed)</li> <li>(Optional but recommended) Load balancer</li> <li>Block storage support for PVC/PV needed by some Ping products</li> <li>(Optional) Ingress controller (ingress-nginx)</li> <li>(Optional) Istio service mesh</li> <li>(Optional) supplementary tools for tracing and monitoring with Istio</li> </ul> <p>Demo Use Only</p> <p>While the result is a full Kubernetes installation, the instructions in this document only create an environment sufficient for testing and learning.  The cluster is not intended for use in production environments.</p>"},{"location":"deployment/deployFullK8s/#prerequisites","title":"Prerequisites","text":"<p>In order to complete this guide, you will need:</p> <ul> <li>64 GB of RAM (32 GB might be enough if you have an SSD to handle some memory swapping and reduce the RAM on the VMs to 12 GB)</li> <li>At least 150 GB of free disk</li> <li>Modern processor with multiple cores</li> <li>Ansible-playbook CLI tool. You can use brew by running <code>brew install ansible</code> or see the ansible site for instructions on how to install and configure this application.</li> <li>Virtualization solution.  For this guide, VMware Fusion is used, but other means of creating and running a VM (Virtualbox, KVM) can be adapted.</li> <li>Access to Ubuntu LTS 22.04.3 server installation media</li> <li>A working knowledge of Kubernetes, such as knowing how to port-forward, install objects using YAML files, and so on. Details on performing some actions will be omitted, and it is assumed the reader will know what to do.</li> <li>Patience</li> </ul>"},{"location":"deployment/deployFullK8s/#virtual-machines","title":"Virtual machines","text":"<p>First, create 3 VMs as described here, using a default installation of Ubuntu 22.04.  For this guide, the user created was <code>ubuntu</code>. You can use any name with a password of your choice.</p> <ul> <li>4 vCPU</li> <li>16 GB RAM</li> <li>2 disks: 80 GB disk for the primary / 60 GB as secondary</li> <li>Attached to a network that allows internet access (bridged or NAT), using a fixed IP address</li> </ul> <p>IP Space</p> <p>192.168.163.0/24 was the IP space used in this guide; adjust to your environment accordingly.</p> VM Hostname IP address Master node k8smaster 192.168.163.60 Worker k8snode01 192.168.163.61 Worker k8snode02 192.168.163.62"},{"location":"deployment/deployFullK8s/#preliminary-operating-system-setup","title":"Preliminary Operating System setup","text":"<p>Perform these actions on all three VMs.</p>"},{"location":"deployment/deployFullK8s/#install-the-operating-system","title":"Install the Operating System","text":"<p>Install the operating system as default, using the first disk (80 GB) as the installation target.  For this guide, the installation disk was formatted to use the entire disk as the root partition, without LVM support.</p>"},{"location":"deployment/deployFullK8s/#create-snapshot-base-os","title":"Create snapshot 'base-os'","text":"<p>Halt each VM by running <code>sudo shutdown -h now</code>.</p> <p>Create a snapshot of each VM, naming it base-os. This snapshot provides a rollback point in case issues arise later. You will use snapshots at several other key points for the same purpose. After installation is complete, these intermediate snapshots can be removed.</p> <p>Power up each VM set after taking the snapshots.</p>"},{"location":"deployment/deployFullK8s/#prepare-for-using-ansible","title":"Prepare for using Ansible","text":"<p>Run on the host machine</p> <p>This block of commands is executed on the host.</p> <pre><code># Add the IP addresses to the local hosts file for convenience\nsudo tee -a /etc/hosts &gt;/dev/null &lt;&lt;-EOF\n192.168.163.60 k8smaster\n192.168.163.61 k8snode01\n192.168.163.62 k8snode02\nEOF\n\n# Copy the SSH key you will use to access the VMs from your host machine to each VM\n# See https://www.ssh.com/academy/ssh/keygen for instructions on generating an SSH key\n# For this guide, the ed25519 algorithm was used\n# Adjust the key name accordingly in the ssh-copy-id command\n\nexport TARGET_MACHINES=(\"k8smaster\" \"k8snode01\" \"k8snode02\")\n\nfor machine in \"${TARGET_MACHINES[@]}\"; do\n    echo \"Copying key to $machine:\"\n    ssh-copy-id -i ~/.ssh/localvms ubuntu@\"$machine\"\n    echo \"======================\"\n    echo \"Confirming access.  You should not be prompted for a password and will be shown the hostname:\"\n    ssh -i ~/.ssh/localvms ubuntu@\"$machine\" 'hostname'\n    echo\ndone\n</code></pre> <p>Sample output: <pre><code>Copying key to k8smaster:\n/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/Users/davidross/.ssh/localvms.pub\"\nThe authenticity of host 'k8smaster (192.168.163.60)' can't be established.\nED25519 key fingerprint is SHA256:qud9m1FRgwzJuwKcEsVVUbZ4bltYmiyKNj5e330ZQCA.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys\nubuntu@k8smaster's password:\n\nNumber of key(s) added:        1\n\nNow try logging into the machine, with:   \"ssh 'ubuntu@k8smaster'\"\nand check to make sure that only the key(s) you wanted were added.\n\n======================\nConfirming access.  You should not be prompted for a password and will be shown the hostname:\nk8smaster\n\n&lt;The output above is repeated for each node.&gt;\n</code></pre></p> <p>After installation and reboot, perform the basic configuration needed for ansible support on the VMs.  The primary change required is to allow <code>sudo</code> commands without requiring a password:</p> <p>Per-VM</p> <p>Run these commands on each VM.</p> <pre><code># Modify /etc/sudoers to allow the ubuntu user to sudo without a password\n# This configuration grants the user full root access with no password\n# DO NOT DO THIS IN PRODUCTION!\n\necho \"ubuntu  ALL=(ALL) NOPASSWD: ALL\" | sudo tee -a /etc/sudoers\n</code></pre>"},{"location":"deployment/deployFullK8s/#configure-ansible-playbooks-for-your-environment","title":"Configure Ansible Playbooks for your environment","text":"<p>At this point, you are ready to modify the ansible playbooks for creating your cluster.</p> <ol> <li> <p>If you have not already done so, clone the <code>pingidentity-devops-getting-started</code> repository to your local <code>${PING_IDENTITY_DEVOPS_HOME}</code> directory.</p> <p>The <code>${PING_IDENTITY_DEVOPS_HOME}</code> environment variable was set by running <code>pingctl config</code>.</p> <pre><code>cd \"${PING_IDENTITY_DEVOPS_HOME}\"\ngit clone \\\n  https://github.com/pingidentity/pingidentity-devops-getting-started.git\n</code></pre> </li> <li> <p>Navigate to the directory with the ansible scripts:</p> </li> </ol> <pre><code>cd \"${PING_IDENTITY_DEVOPS_HOME}\"/pingidentity-devops-getting-started/99-helper-scripts/ansible\n</code></pre> <ol> <li>Modify the <code>inventory.ini</code>, <code>ansible.cfg</code>, <code>install_kubernetes.yaml</code> and <code>install_list.yaml</code> files accordingly to suit your environment:</li> </ol> <p>The inventory.ini will need modification for your IP addresses, private key file, and user if one other than <code>ubuntu</code> was used:    <pre><code>[kubernetes_master]\nk8smaster ansible_host=192.168.163.60\n\n[kubernetes_nodes]\nk8snode01 ansible_host=192.168.163.61\nk8snode02 ansible_host=192.168.163.62\n\n[all:vars]\nansible_user=ubuntu\nansible_ssh_private_key_file=/Users/davidross/.ssh/localvms\nansible_python_interpreter=/usr/bin/python3\n</code></pre></p> <p>The ansible.cfg file should not need any modification:    <pre><code>[defaults]\ninventory = inventory.ini\nhost_key_checking = False\n</code></pre></p> <p>The install_kubernetes.yaml file will need the following changes to lines 11-13 if your IP address differs from this example:</p> <pre><code> k8smaster_ip: \"192.168.163.60\"\n k8snode01_ip: \"192.168.163.61\"\n k8snode02_ip: \"192.168.163.62\"\n</code></pre> <p>Finally, update the install_list.yaml file.  By default, no additional components are installed other than block storage, which is needed for some Ping products.  To install other optional components, set the value to True.  Note that helm is required to install the Ingress controller, and adding K9s, metallb and ingress will provide additional tools for the most production-like implementation:</p> <pre><code>---\nhelm: False\nk9s: False\nmetallb: False\nstorage: True\ningress: False\nistio: False\nistioaddons: False\n...\n</code></pre>"},{"location":"deployment/deployFullK8s/#run-the-first-playbook","title":"Run the first playbook","text":"<p>With the changes above, you are now ready to run the playbooks.  First, run the install_kubernetes.yaml playbook to install Kubernetes.</p> <p><code>ansible-playbook install_kubernetes.yaml -i inventory.ini</code></p> <p>Idempotency</p> <p>The playbook was designed to to be idempotent so you can run it multiple times if needed.  An alternative is to reset to the base-os snapshot, update the <code>sudoers</code> file and run it again.</p> Sample output <pre><code>PLAY [Install Kubernetes on all VMs] *******************************************\n\nTASK [Gathering Facts] *********************************************************\nok: [k8smaster]\nok: [k8snode02]\nok: [k8snode01]\n\nTASK [Update package cache] ****************************************************\nok: [k8snode02]\nok: [k8snode01]\nok: [k8smaster]\n\nTASK [Upgrade all packages] ****************************************************\nchanged: [k8snode01]\nchanged: [k8snode02]\nchanged: [k8smaster]\n\nTASK [Add host file entries] ***************************************************\nchanged: [k8smaster]\nchanged: [k8snode02]\nchanged: [k8snode01]\n\nTASK [Add the br_netfilter kernel module and configure for load at boot] *******\nchanged: [k8smaster]\nchanged: [k8snode02]\nchanged: [k8snode01]\n\nTASK [Add the overlay kernel module and configure for load at boot] ************\nchanged: [k8smaster]\nchanged: [k8snode01]\nchanged: [k8snode02]\n\nTASK [Creating kernel modules file to load at boot] ****************************\nchanged: [k8smaster]\nchanged: [k8snode02]\nchanged: [k8snode01]\n\nTASK [Disable swap in fstab by commenting it out] ******************************\nchanged: [k8smaster]\nchanged: [k8snode01]\nchanged: [k8snode02]\n\nTASK [Disable swap] ************************************************************\nchanged: [k8snode01]\nchanged: [k8snode02]\nchanged: [k8smaster]\n\nTASK [Enable IP forwarding for iptables] ***************************************\nchanged: [k8smaster]\nchanged: [k8snode01]\nchanged: [k8snode02]\n\nTASK [Update sysctl parameters without reboot - bridge (ipv4)] *****************\nchanged: [k8smaster]\nchanged: [k8snode01]\nchanged: [k8snode02]\n\nTASK [Update sysctl parameters without reboot - bridge (ipv6)] *****************\nchanged: [k8smaster]\nchanged: [k8snode01]\nchanged: [k8snode02]\n\nTASK [Update sysctl parameters without reboot - IPforward] *********************\nchanged: [k8smaster]\nchanged: [k8snode01]\nchanged: [k8snode02]\n\nTASK [Install prerequisites] ***************************************************\nchanged: [k8snode02] =&gt; (item=containerd)\nchanged: [k8smaster] =&gt; (item=containerd)\nchanged: [k8snode01] =&gt; (item=containerd)\n\nTASK [Add Kubernetes APT key] **************************************************\nchanged: [k8snode02]\nchanged: [k8smaster]\nchanged: [k8snode01]\n\nTASK [Create directory for containerd configuration file] **********************\nchanged: [k8smaster]\nchanged: [k8snode01]\nchanged: [k8snode02]\n\nTASK [Check if containerd toml configuration file exists] **********************\nok: [k8smaster]\nok: [k8snode01]\nok: [k8snode02]\n\nTASK [Create containerd configuration file if it does not exist] ***************\nchanged: [k8smaster]\nchanged: [k8snode02]\nchanged: [k8snode01]\n\nTASK [Read config.toml file] ***************************************************\nok: [k8smaster]\nok: [k8snode01]\nok: [k8snode02]\n\nTASK [Check if correct containerd.runtimes.runc line exists] *******************\nok: [k8smaster]\nok: [k8snode01]\nok: [k8snode02]\n\nTASK [Error out if the incorrect or missing containerd.runtimes.runc line does not exist] ***\nskipping: [k8smaster]\nskipping: [k8snode01]\nskipping: [k8snode02]\n\nTASK [Set SystemdCgroup line in file to true if it is currently false] *********\nchanged: [k8smaster]\nchanged: [k8snode02]\nchanged: [k8snode01]\n\nTASK [Restart containerd service] **********************************************\nchanged: [k8smaster]\nchanged: [k8snode02]\nchanged: [k8snode01]\n\nTASK [Install prerequisites for Kubernetes] ************************************\nchanged: [k8snode02] =&gt; (item=apt-transport-https)\nchanged: [k8smaster] =&gt; (item=apt-transport-https)\nchanged: [k8snode01] =&gt; (item=apt-transport-https)\nok: [k8snode02] =&gt; (item=ca-certificates)\nok: [k8snode01] =&gt; (item=ca-certificates)\nok: [k8smaster] =&gt; (item=ca-certificates)\nok: [k8snode02] =&gt; (item=curl)\nok: [k8smaster] =&gt; (item=curl)\nok: [k8snode01] =&gt; (item=curl)\nchanged: [k8snode02] =&gt; (item=gnupg2)\nchanged: [k8snode01] =&gt; (item=gnupg2)\nchanged: [k8smaster] =&gt; (item=gnupg2)\nok: [k8snode02] =&gt; (item=software-properties-common)\nok: [k8snode01] =&gt; (item=software-properties-common)\nok: [k8smaster] =&gt; (item=software-properties-common)\nchanged: [k8snode02] =&gt; (item=bzip2)\nchanged: [k8snode01] =&gt; (item=bzip2)\nchanged: [k8smaster] =&gt; (item=bzip2)\nok: [k8snode02] =&gt; (item=tar)\nok: [k8snode01] =&gt; (item=tar)\nok: [k8smaster] =&gt; (item=tar)\nok: [k8snode02] =&gt; (item=vim)\nok: [k8snode01] =&gt; (item=vim)\nok: [k8smaster] =&gt; (item=vim)\nok: [k8snode02] =&gt; (item=git)\nok: [k8snode01] =&gt; (item=git)\nok: [k8smaster] =&gt; (item=git)\nok: [k8snode02] =&gt; (item=wget)\nok: [k8snode01] =&gt; (item=wget)\nok: [k8smaster] =&gt; (item=wget)\nchanged: [k8snode02] =&gt; (item=net-tools)\nchanged: [k8snode01] =&gt; (item=net-tools)\nchanged: [k8smaster] =&gt; (item=net-tools)\nok: [k8snode01] =&gt; (item=lvm2)\nok: [k8snode02] =&gt; (item=lvm2)\nok: [k8smaster] =&gt; (item=lvm2)\n\nTASK [Add Kubernetes APT repository] *******************************************\nchanged: [k8smaster]\nchanged: [k8snode01]\nchanged: [k8snode02]\n\nTASK [Install Kubernetes components] *******************************************\nchanged: [k8snode01] =&gt; (item=kubelet)\nchanged: [k8snode02] =&gt; (item=kubelet)\nchanged: [k8smaster] =&gt; (item=kubelet)\nchanged: [k8snode01] =&gt; (item=kubeadm)\nchanged: [k8snode02] =&gt; (item=kubeadm)\nchanged: [k8smaster] =&gt; (item=kubeadm)\nok: [k8snode02] =&gt; (item=kubectl)\nok: [k8snode01] =&gt; (item=kubectl)\nok: [k8smaster] =&gt; (item=kubectl)\n\nTASK [Hold Kubernetes packages at current version] *****************************\nchanged: [k8smaster]\nchanged: [k8snode01]\nchanged: [k8snode02]\n\nTASK [Run kubeadm reset to ensure fresh start each time.] **********************\nchanged: [k8smaster]\nchanged: [k8snode02]\nchanged: [k8snode01]\n\nTASK [Remove any files from a previous installation attempt] *******************\nskipping: [k8snode01] =&gt; (item=/home/ubuntu/.kube) \nskipping: [k8snode01]\nskipping: [k8snode02] =&gt; (item=/home/ubuntu/.kube) \nskipping: [k8snode02]\nok: [k8smaster] =&gt; (item=/home/ubuntu/.kube)\n\nTASK [Initialize Kubernetes master] ********************************************\nskipping: [k8snode01]\nskipping: [k8snode02]\nchanged: [k8smaster]\n\nTASK [Check if k8s installation file exists] ***********************************\nskipping: [k8snode01]\nskipping: [k8snode02]\nok: [k8smaster]\n\nTASK [Fail if K8s installed file does not exist] *******************************\nskipping: [k8smaster]\nskipping: [k8snode01]\nskipping: [k8snode02]\n\nTASK [Create .kube directory] **************************************************\nskipping: [k8snode01]\nskipping: [k8snode02]\nchanged: [k8smaster]\n\nTASK [Copy kubeconfig to user's home directory] ********************************\nskipping: [k8snode01]\nskipping: [k8snode02]\nchanged: [k8smaster]\n\nTASK [Install Pod network] *****************************************************\nskipping: [k8snode01]\nskipping: [k8snode02]\nchanged: [k8smaster]\n\nTASK [Remove taint from master node] *******************************************\nskipping: [k8snode01]\nskipping: [k8snode02]\nchanged: [k8smaster]\n\nTASK [Retrieve join command from master and run it on the nodes] ***************\nskipping: [k8snode01]\nskipping: [k8snode02]\nchanged: [k8smaster]\n\nTASK [Join worker nodes to the cluster] ****************************************\nskipping: [k8snode01] =&gt; (item=k8snode01) \nskipping: [k8snode01] =&gt; (item=k8snode02) \nskipping: [k8snode01]\nskipping: [k8snode02] =&gt; (item=k8snode01) \nskipping: [k8snode02] =&gt; (item=k8snode02) \nskipping: [k8snode02]\nchanged: [k8smaster -&gt; k8snode01(192.168.163.61)] =&gt; (item=k8snode01)\nchanged: [k8smaster -&gt; k8snode02(192.168.163.62)] =&gt; (item=k8snode02)\n\nTASK [Pause for 5 seconds] *****************************************************\nPausing for 5 seconds\n(ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)\nok: [k8smaster]\n\nTASK [Confirm flannel pods are ready] ******************************************\nskipping: [k8snode01]\nskipping: [k8snode02]\nchanged: [k8smaster]\n\nTASK [Run confirmation command by listing nodes] *******************************\nchanged: [k8snode01 -&gt; k8smaster(192.168.163.60)]\nchanged: [k8smaster]\nchanged: [k8snode02 -&gt; k8smaster(192.168.163.60)]\n\nTASK [Nodes in the cluster] ****************************************************\nok: [k8smaster] =&gt; {\n    \"nodes_command_output.stdout_lines\": [\n        \"NAME        STATUS   ROLES           AGE   VERSION\",\n        \"k8smaster   Ready    control-plane   46s   v1.28.2\",\n        \"k8snode01   Ready    &lt;none&gt;          22s   v1.28.2\",\n        \"k8snode02   Ready    &lt;none&gt;          18s   v1.28.2\"\n    ]\n}\nok: [k8snode01] =&gt; {\n    \"nodes_command_output.stdout_lines\": [\n        \"NAME        STATUS   ROLES           AGE   VERSION\",\n        \"k8smaster   Ready    control-plane   46s   v1.28.2\",\n        \"k8snode01   Ready    &lt;none&gt;          22s   v1.28.2\",\n        \"k8snode02   Ready    &lt;none&gt;          18s   v1.28.2\"\n    ]\n}\nok: [k8snode02] =&gt; {\n    \"nodes_command_output.stdout_lines\": [\n        \"NAME        STATUS   ROLES           AGE   VERSION\",\n        \"k8smaster   Ready    control-plane   46s   v1.28.2\",\n        \"k8snode01   Ready    &lt;none&gt;          22s   v1.28.2\",\n        \"k8snode02   Ready    &lt;none&gt;          18s   v1.28.2\"\n    ]\n}\n\nTASK [Provide cluster connection information] **********************************\nskipping: [k8snode01]\nskipping: [k8snode02]\nchanged: [k8smaster]\n\nTASK [Cluster information for your .kube/config file] **************************\nok: [k8smaster] =&gt; {\n    \"msg\": [\n        \"apiVersion: v1\",\n        \"clusters:\",\n        \"- cluster:\",\n        \"    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJek1EWXlOakUxTkRjeU1Gb1hEVE16TURZeU16RTFORGN5TUZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBT25YCjdiTlRKNi93TGsyM2NSUllFeVI3M2tzRzFKYmlBaVBaYlBETTNXZnJqOFRsYlNhK1QxazNoeTlzNFJBaWdzSzUKaDlnNkE2QSsxWkFxOEw4WWVzZlhYL0ZUb0I2UlN3VWJmd2FVSkpUOWNHQTRvbXZmV2JVVzlrMHp0Qkp2aHREbgo5V2t2dWp4aDhBVEg1Q0JqTVNYWjErYThlS2JkQ3hDNVI5ZWdEaGJGSDIwYmlieG1BS0JvTUhKK2tvUUVTZ212CkRack9GczRDeDlPbWYyZXVFNkRqeGNKVSs2aytxU3hjWHN4ZDJJM0JKSnVxeXFhdDFoMkl1b0ZGMFh4aXVaVWsKWHU4amZhVHZCQ2JJRFZ4SGhaaUswMEs4Q0Naakg2cGxzSnVLT2dXQ0FTcFhGTVJNL3UvRDBubUM0emN3WnhzVQp1d0NYdXQ0QW54eTJqb0hIcDJzQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZJR2w4THREalRUdkJsdGs5a05UV0dWY3dpenZNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBQmpRUy9qSWtadjI0UUp2UkllZgphQ0ZCOVlKMWltTUJSdnprY0hEQjVKSTJpYTJESWxEVS9xMklRb2dUVWIvWmVPK2p1SFJoK2VIbUx3eVZuMTBCClNXb1QzUzlHK3VyK1hDbE8xU2dUMmg2UnlLTWZ2UFpmMFBHNGJIanFWSDJFbk56MGNoTUFBK1RzSGM2WVEyTFEKVnp2OURHb0pFdEkzL0w2M1AwSVVFWEhvalVxcGNHUkZhRFliWnc1NElGUitqdGs5L1lYakhDWUhna0JkMlhjTApwcElZcG40TUpubW9BeUxONGprdVQ4Qi9pTXhjUGxCaWpyZTd0Q3YvbXlnaUp6d0hWcUN1S2FRcG1xUUFuM0MwClNDeTU5ZEJvMmp4Q3ZrNE1xSWNZS1F5eVFsVXhTeEpMc3E4VUlhWi9ZY0ZMT1pJSWJzYzUrbnJuZXNPVkl0OXEKZVhnPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\",\n        \"    server: https://192.168.163.60:6443\",\n        \"  name: kubernetes\",\n        \"contexts:\",\n        \"- context:\",\n        \"    cluster: kubernetes\",\n        \"    user: kubernetes-admin\",\n        \"  name: kubernetes-admin@kubernetes\",\n        \"current-context: kubernetes-admin@kubernetes\",\n        \"kind: Config\",\n        \"preferences: {}\",\n        \"users:\",\n        \"- name: kubernetes-admin\",\n        \"  user:\",\n        \"    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJS2xBRDd6VGJmaFl3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TXpBMk1qWXhOVFEzTWpCYUZ3MHlOREEyTWpVeE5UUTNNakphTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQTRHRmRreEJ0U0RuWS95TFUKUVQ3QnVhMm5tT2kzZ2IvLzdLL3d3UXlqeDBSMFpWWGI3QWw5TU5GVFlUdVJQbm1jVm1CK1FPK215RExsVGlvcwpZQ0dmeHEzS1ZMOU40bXJjUTJwU3dNVTYydVh6bG9MczVZM2VLWlVqL0haZEs0N1ZzSzZBa0duZFFEMGxlRjhHCmY4T3Raek8xOXgwcDZ5WVZIYnJseUxhd1Eybm9FbXdrbWdpUlJpSVFPSDJXQ1JyL1pkeWMyaXE1UmMxMFZTODMKUFdmaEtwZGo2U2VCM3dCdGUyM2dLdzdUSXJIWk0vcWFOOFVJWnB3VE5ibkFlQ2d6VUhmK3kyby9maDNjZ0ZHOQpHUUpRRjdzd1c0MEQ4UGF4eXlZMWFlTWN0OHpBWVhkK0NZamZuNHlLWTdLcnRoTFhnWmVsTUtrcXpYaVljSHVGCjVVQ3N5d0lEQVFBQm8xWXdWREFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFmQmdOVkhTTUVHREFXZ0JTQnBmQzdRNDAwN3daYlpQWkRVMWhsWE1Jcwo3ekFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBbFlqOXlKREIrSjRsYXhkRnZUQ01KUENlQ0lwcWFUS1FHN01LClk3OVZLbWkyazAwQWFsZGRSeHptckF3V3NLa1B5blAyMEdnS0ZST2w1dUJvZk9VUXoxU21oeEN2bmZmbStCN2wKTDQ5aTBDTTNlMStBYTgxcGh4TkROaXk0N1JmcXJTc0svSVduUVFCbzNVd0M2UXpoMm9xTzZMOHlWZUQ0MXJXSwpIeWJ6dHpRS0hzeE0yUVk1VklUazVPQVBvTzJ3ZnJ2SEFCVGxmUDRLU0E2SWVIdmlzVkdreTlTUlVsbU9paXp5CmV5YVV6VG9kZm51M3JIQ3dHWEoxN25rQjI3MVN3V0kxZGpKamJacWdGb1VaNWRWQ0RnNHZ5dVhnUiswSVNRR3IKa0orZWxndEQ3V1RDeUJVL0pHUmExRVB1RnF2T3lEbkw1T0NQSkF6THRyN0I0SnhVWFE9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\",\n        \"    client-key-data: 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\"\n    ]\n}\nskipping: [k8snode01]\nskipping: [k8snode02]\n\nPLAY RECAP *********************************************************************\nk8smaster   : ok=42   changed=32   unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\nk8snode01   : ok=29   changed=23   unreachable=0    failed=0    skipped=14   rescued=0    ignored=0\nk8snode02   : ok=29   changed=23   unreachable=0    failed=0    skipped=14   rescued=0    ignored=0\n</code></pre>"},{"location":"deployment/deployFullK8s/#optional-but-recommended-create-snapshot-k8s-installed","title":"(Optional but recommended) Create snapshot 'k8s-installed'","text":"<p>Halt each VM by running <code>sudo shutdown -h now</code>.</p> <p>Create a snapshot of each VM, naming it k8s-installed.</p> <p>Power up each VM set after taking the snapshot.</p>"},{"location":"deployment/deployFullK8s/#run-the-components-playbook","title":"Run the components playbook","text":"<p>Now that Kubernetes is installed, modify the <code>install_list.yaml</code> file accordingly to enable or disable the components that you want to install.  After making changes, run the install_others.yaml playbook.</p> <p><code>ansible-playbook install_others.yaml -i inventory.ini</code></p> <p>Idempotency</p> <p>The playbook was designed to to be idempotent so you can run it multiple times if needed.  An alternative is to reset to the k8s-installed snapshot and run it again.</p> <p>Monitor progress</p> <p>After K9s is installed on the master, you can launch it in an SSH session and monitor the progress of the pods being instantiated while the playbook is running.</p> Sample output with everything enabled other than Istio and the Istio-addons <pre><code>PLAY [Install Other Components in the Cluster] *********************************\n\nTASK [Gathering Facts] *********************************************************\nok: [k8smaster]\n\nTASK [Get helm installation file] **********************************************\nchanged: [k8smaster]\n\nTASK [Extract helm] ************************************************************\nchanged: [k8smaster]\n\nTASK [Remove helm tarball and extracted folder] ********************************\nchanged: [k8smaster] =&gt; (item=/home/ubuntu/linux-amd64)\nchanged: [k8smaster] =&gt; (item=/home/ubuntu/helm-v3.12.1-linux-amd64.tar.gz)\n\nTASK [Get K9s installation file] ***********************************************\nchanged: [k8smaster]\n\nTASK [Extract k9s] *************************************************************\nchanged: [k8smaster]\n\nTASK [Remove k9s tarball] ******************************************************\nchanged: [k8smaster]\n\nTASK [Get latest MetalLB version] **********************************************\nchanged: [k8smaster]\n\nTASK [Get MetalLB installer] ***************************************************\nchanged: [k8smaster]\n\nTASK [Apply MetalLB file] ******************************************************\nchanged: [k8smaster]\n\nTASK [Pause for 10 seconds] ****************************************************\nPausing for 10 seconds\n(ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)\nok: [k8smaster]\n\nTASK [Wait for MetalLB controller and speaker pods to be ready] ****************\nchanged: [k8smaster] =&gt; (item=app=metallb)\nchanged: [k8smaster] =&gt; (item=component=speaker)\n\nTASK [Creating MetalLB configuration file] *************************************\nchanged: [k8smaster]\n\nTASK [Configure MetalLB] *******************************************************\nchanged: [k8smaster]\n\nTASK [Remove MetalLB installation yaml files] **********************************\nchanged: [k8smaster] =&gt; (item=/home/ubuntu/ipaddress_pool_metal.yaml)\nchanged: [k8smaster] =&gt; (item=/home/ubuntu/metallb-native.yaml)\n\nTASK [Install CertManager prerequisite] ****************************************\nchanged: [k8smaster]\n\nTASK [Check if rook directory exists] ******************************************\nok: [k8smaster]\n\nTASK [Remove directory] ********************************************************\nskipping: [k8smaster]\n\nTASK [Clone Rook repository] ***************************************************\nchanged: [k8smaster]\n\nTASK [Install Rook controller] *************************************************\nchanged: [k8smaster]\n\nTASK [Wait for Rook controller pod to be ready] ********************************\nchanged: [k8smaster]\n\nTASK [Install Rook components] *************************************************\nchanged: [k8smaster]\n\nTASK [Pause for 3 1/2 minutes - wait for Rook components to get started] *******\nPausing for 210 seconds\n(ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)\nok: [k8smaster]\n\nTASK [Confirm Rook cluster pods are ready] *************************************\nchanged: [k8smaster] =&gt; (item=app=csi-cephfsplugin)\nchanged: [k8smaster] =&gt; (item=app=csi-cephfsplugin-provisioner)\nchanged: [k8smaster] =&gt; (item=app=csi-rbdplugin)\nchanged: [k8smaster] =&gt; (item=app=rook-ceph-mgr)\nchanged: [k8smaster] =&gt; (item=app=rook-ceph-mon)\nchanged: [k8smaster] =&gt; (item=app=rook-ceph-crashcollector)\nchanged: [k8smaster] =&gt; (item=app=csi-rbdplugin-provisioner)\nchanged: [k8smaster] =&gt; (item=app=rook-ceph-osd)\n\nTASK [Creating Block Storage class] ********************************************\nchanged: [k8smaster]\n\nTASK [Create block ceph storage class] *****************************************\nchanged: [k8smaster]\n\nTASK [Creating script to patch storage class (globbing and substitution hack)] ***\nchanged: [k8smaster]\n\nTASK [Set storage class as default] ********************************************\nchanged: [k8smaster]\n\nTASK [Remove Rook installation files] ******************************************\nchanged: [k8smaster] =&gt; (item=/home/ubuntu/rook)\nchanged: [k8smaster] =&gt; (item=/home/ubuntu/sc-ceph-block.yaml)\nchanged: [k8smaster] =&gt; (item=/home/ubuntu/patchsc.yaml)\n\nTASK [Install Ingress Nginx] ***************************************************\nchanged: [k8smaster]\n\nTASK [Pause for 5 seconds] *****************************************************\nPausing for 5 seconds\n(ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)\nok: [k8smaster]\n\nTASK [Confirm ingress controller pod is ready] *********************************\nchanged: [k8smaster]\n\nTASK [Get Ingress service components for confirmation] *************************\nchanged: [k8smaster]\n\nTASK [Ingress controller information] ******************************************\nok: [k8smaster] =&gt; {\n    \"msg\": [\n        \"NAME                                 TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)                      AGE\",\n        \"ingress-nginx-controller             LoadBalancer   10.101.36.117    192.168.163.151   80:30865/TCP,443:31410/TCP   31s\",\n        \"ingress-nginx-controller-admission   ClusterIP      10.103.154.247   &lt;none&gt;            443/TCP                      31s\"\n    ]\n}\n\nTASK [Get 'istioctl' installation file] ****************************************\nskipping: [k8smaster]\n\nTASK [Extract istioctl] ********************************************************\nskipping: [k8smaster]\n\nTASK [Install istio] ***********************************************************\nskipping: [k8smaster]\n\nTASK [Pause for 5 seconds] *****************************************************\nskipping: [k8smaster]\n\nTASK [Confirm Istio pods are ready] ********************************************\nskipping: [k8smaster] =&gt; (item=app=istiod) \nskipping: [k8smaster] =&gt; (item=app=istio-ingressgateway) \nskipping: [k8smaster] =&gt; (item=app=istio-egressgateway) \nskipping: [k8smaster]\n\nTASK [Install Istio add-ons] ***************************************************\nskipping: [k8smaster]\n\nTASK [Confirm Istio additional pods are ready] *********************************\nskipping: [k8smaster] =&gt; (item=app=grafana) \nskipping: [k8smaster] =&gt; (item=app=jaeger) \nskipping: [k8smaster] =&gt; (item=app=kiali) \nskipping: [k8smaster] =&gt; (item=app=prometheus) \nskipping: [k8smaster] =&gt; (item=app.kubernetes.io/name=loki) \nskipping: [k8smaster]\n\nTASK [Creating patch file for services] ****************************************\nskipping: [k8smaster]\n\nTASK [Patch istio add-on services to use load balancer] ************************\nskipping: [k8smaster] =&gt; (item=grafana) \nskipping: [k8smaster] =&gt; (item=kiali) \nskipping: [k8smaster] =&gt; (item=tracing) \nskipping: [k8smaster] =&gt; (item=prometheus) \nskipping: [k8smaster]\n\nTASK [Remove istio tarball and extracted folder] *******************************\nskipping: [k8smaster] =&gt; (item=/home/ubuntu/istio-1.18.0) \nskipping: [k8smaster] =&gt; (item=/home/ubuntu/istio-1.18.0-linux-amd64.tar.gz) \nskipping: [k8smaster] =&gt; (item=/home/ubuntu/patch-service.yaml) \nskipping: [k8smaster]\n\nPLAY RECAP *********************************************************************\nk8smaster                  : ok=33   changed=27   unreachable=0    failed=0    skipped=11   rescued=0    ignored=0   \n</code></pre>"},{"location":"deployment/deployFullK8s/#snapshot-k8scomplete","title":"Snapshot 'k8sComplete'","text":"<p>Shutdown the VMs, and snapshot each one.  See the helper script in this repository located at <code>99-helper-scripts/manageCluster.sh</code> that can be used for automating things under VMware.</p> <p>At this time,  your cluster is ready for use.  </p>"},{"location":"deployment/deployFullK8s/#resources-references","title":"Resources &amp; References","text":"<p>This guide was built on the work of others. As with many how-to documents, we have contributed our skills and knowledge to pull everything together and fill in the gaps that were experienced.  However, we want to acknowledge at least some of the many sources where we found inspiration, guidance, fixes for errors, and sanity when a step was missed. Not shown here are dozens of places where we went in exploring different options for pieces we did not use or install in the end. the Ping DevOps Integrations team</p> <ul> <li>How to Deploy MetalLB on Kubernetes - ComputingForGeeks</li> <li>How To: Ubuntu / Debian Linux Regenerate OpenSSH Host Keys - nixCraft</li> <li>Block Storage Overview - Rook Ceph Documentation</li> <li>Toolbox - Rook Ceph Documentation</li> <li>Quickstart - Rook Ceph Documentation</li> <li>How to Install Kubernetes on Ubuntu 22.04 / Ubuntu 20.04 | ITzGeek</li> <li>Kubernetes 1.26 - The electrifying release setup - KubeSimplify</li> <li>Containerd Github</li> <li>etcd-io Github issue #13670</li> </ul>"},{"location":"deployment/deployHelm/","title":"Deploy Ping DevOps Charts using Helm","text":"Helm Charts Repo <p>To use Ping Identity Helm charts for deployment to Kubernetes, go to the Getting Started page for the Helm chart repository to configure your system to run Helm.  Afterward, continue on this page for examples illustrating how to deploy various scenarios from the charts.</p> <p>Notification of new releases</p> <p>If you want to be notified when a new version of the chart is released, see the Orchestration/Helm/Kubernetes section of the FAQ page for instructions on following the GitHub repository for our chart.</p> <p>Ingress with the local K8s cluster</p> <p>If you want to run these examples on the local kind cluster created as outlined on the Deploy a local Kubernetes cluster page with ingresses, see this page for ingress configuration instructions.  Otherwise, you can port-forward to product services.</p>"},{"location":"deployment/deployHelm/#helm-chart-example-configurations","title":"Helm Chart Example Configurations","text":"<p>The following table contains example configurations and instructions to run and configure Ping products using the Ping Devops Helm Chart.</p> Config Description .yaml Notes Everything Example with most products integrated together everything.yaml Ingress Expose an application outside of the cluster ingress.yaml Update line 7 with your domain RBAC Enable RBAC for workloads rbac.yaml Vault Example vault values section vault.yaml Vault Keystores Example vault values for keystores vault-keystores.yaml PingAccess PingAccess Admin Console &amp; Engine pingaccess-cluster.yaml PingAccess &amp; PingFederate Integration PA &amp; PF Admin Console &amp; Engine pingaccess-pingfederate-integration.yaml PingFederate PingFederate Admin Console &amp; Engine pingfederate-cluster.yaml PingFederate Upgrade PingFederate See .yaml files in pingfederate-upgrade PingDirectory PingDirectory Instance pingdirectory.yaml PingDirectory Upgrade PingDirectory Upgrade with partition See .yaml files in pingdirectory-upgrade-partition PingDirectory Backup and Sidecar PingDirectory with periodic backup and sidecar pingdirectory-periodic-backup.yaml PingDirectory Archive Backup to S3 (Demo Only) Archive PingDirectory backup to S3 Sample files in s3-sidecar PingDirectory Scale Down Scale Down a PingDirectory StatefulSet See .yaml files in pingdirectory-scale-down PingAuthorize and PingDirectory PingAuthorize with PAP and PingDirectory pingauthorize-pingdirectory.yaml Entry Balancing PingDirectory and PingDirectoryProxy entry balancing See .yaml files in entry-balancing PingCentral PingCentral pingcentral.yaml PingCentral with MySQL PingCentral with external MySQL deployment pingcentral-external-mysql-db.yaml Simple Sync PingDataSync and PingDirectory simple-sync.yaml PingDataSync Failover PingDataSync and PingDirectory with failover pingdatasync-failover.yaml Cluster Metrics Example values using various open source tools See .yaml files in cluster-metrics PingDataConsole SSO with PingOne Sign into PingDataConsole with PingOne pingdataconsole-pingone-sso.yaml Using CSI Volumes Mount secrets with CSI volumes csi-secrets-volume.yaml Splunk logging sidecar Forward product logs to splunk See files in the splunk folder ImagePullSecrets (individual) Provide secret for private registry authentication image-pull-secrets-individual.yaml Replace stubs with your values ImagePullSecrets (global) Provide global secret for private registry authentication image-pull-secrets-global.yaml Replace stubs with your values"},{"location":"deployment/deployHelm/#to-deploy","title":"To Deploy","text":"<pre><code>helm upgrade --install myping pingidentity/ping-devops \\\n     -f &lt;HTTP link to yaml&gt;\n</code></pre>"},{"location":"deployment/deployHelm/#uninstall","title":"Uninstall","text":"<pre><code>helm uninstall myping\n</code></pre>"},{"location":"deployment/deployHelmLocalIngress/","title":"Local Kind ingress","text":"<p>If you have deployed the local kind cluster as outlined on the create a local cluster page, follow these instructions to use an ingress for accessing your products.</p>"},{"location":"deployment/deployHelmLocalIngress/#prerequisites","title":"Prerequisites","text":"<ul> <li>A kind cluster deployed with ingress enabled.  For this guide, the cluster name <code>ping</code>  is assumed</li> <li>The hostname aliases have been appended to the <code>/etc/hosts</code> file</li> <li>You have created the secret for your DevOps user and key for retrieving licenses</li> </ul>"},{"location":"deployment/deployHelmLocalIngress/#assumptions","title":"Assumptions","text":"<p>With the <code>/etc/hosts</code> file entries created from the page linked above, the release in helm must be <code>myping</code> for the hostnames to work with the configuration here.  Consider the first entry as an example:</p> <pre><code>127.0.0.1 myping-pingaccess-admin.pingdemo.example ...\n</code></pre> <p>When using our charts, the release name provided to helm is prepended - that is what provides the <code>myping-</code> portion of the hostname in the file.  The <code>pingdemo.example</code> domain suffix is provided through the ingress definitions as shown later on this page.  So, the structure is:</p> <pre><code>&lt;helm-release-name&gt;-&lt;ping-product-service&gt;.&lt;domain-name-from-ingress&gt;\n</code></pre> <p>If you use a release name other than <code>myping</code> or a domain other than <code>pingdemo.example</code> you will need to update the aliases in <code>/etc/hosts</code>/ accordingly.</p>"},{"location":"deployment/deployHelmLocalIngress/#instructions","title":"Instructions","text":"<p>There is a file under the <code>30-helm</code> directory of this repository named <code>ingress.yaml</code>.  Modify this file for use with a local cluster:</p> <ul> <li>Replace <code>insert domain name here</code> with your domain name (pingdemo.example in this guide)</li> <li>Edit line 11, removing the <code>-public</code> suffix for the class</li> </ul> <p>The file should look as shown here:</p> <pre><code>global:\n  envs:\n    PING_IDENTITY_ACCEPT_EULA: \"YES\"\n  ingress:\n    enabled: true\n    addReleaseNameToHost: prepend\n    defaultDomain: \"pingdemo.example\"\n    defaultTlsSecret:\n    annotations:\n      nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n      kubernetes.io/ingress.class: \"nginx\"\n</code></pre>"},{"location":"deployment/deployHelmLocalIngress/#create-ingresses","title":"Create ingresses","text":"<p>When deploying using helm and one of the example YAML files in the 30-helm directory, also pass in the ingress.yaml file to include the ingress definitions as part of the overall deployment.</p> <p>For example, to use the <code>everything.yaml</code> and include ingresses, you would run the following command from the 30-helm directory (after updating the ingress.yaml file):</p> <pre><code>helm upgrade --install myping pingidentity/ping-devops -f everything.yaml -f ingress.yaml\n</code></pre> <p>After everything has started, you will see the same pods and services as in the getting started example.  In addition, you will see ingress definitions in the same namespace as the products:</p> <pre><code># List ingress definitions\nkubectl get ingress\n\n# Output\nNAME                         CLASS    HOSTS                                       ADDRESS     PORTS     AGE\nmyping-pingaccess-admin      &lt;none&gt;   myping-pingaccess-admin.pingdemo.example      localhost   80, 443   47m\nmyping-pingaccess-engine     &lt;none&gt;   myping-pingaccess-engine.pingdemo.example     localhost   80, 443   47m\nmyping-pingauthorize         &lt;none&gt;   myping-pingauthorize.pingdemo.example         localhost   80, 443   47m\nmyping-pingdataconsole       &lt;none&gt;   myping-pingdataconsole.pingdemo.example       localhost   80, 443   47m\nmyping-pingdirectory         &lt;none&gt;   myping-pingdirectory.pingdemo.example         localhost   80, 443   47m\nmyping-pingfederate-admin    &lt;none&gt;   myping-pingfederate-admin.pingdemo.example    localhost   80, 443   47m\nmyping-pingfederate-engine   &lt;none&gt;   myping-pingfederate-engine.pingdemo.example   localhost   80, 443   47m\n</code></pre> <p>The HOSTS column reflects the entries added to the <code>/etc/hosts</code> file.</p> <p>To access a given service, enter the HOSTS entry in your browser (you will have to accept the self-signed certificate).  For example, to view the Ping Federate console, you would access https://myping-pingfederate-admin.pingdemo.example/.  For the Ping Data console, https://myping-pingdataconsole.pingdemo.example and so on.</p> <p>Here are the credentials and URLs.  This table is similar to the getting started example but reflects the release name used on this page:</p> Product Connection Details PingFederate <ul> <li>URL: https://myping-pingfederate-admin.pingdemo.example/pingfederate/app</li><li>Username: administrator</li><li>Password: 2FederateM0re</li></ul> PingDirectory <ul><li>URL: https://myping-pingdataconsole.pingdemo.example/console</li><li>Server: ldaps://myping-pingdirectory-cluster:1636</li><li>Username: administrator</li><li>Password: 2FederateM0re</li></ul> PingAccess <ul><li>URL: https://myping-pingaccess-admin.pingdemo.example</li><li>Username: administrator</li><li>Password: 2FederateM0re</li></ul> PingAuthorize <ul><li>URL: https://myping-pingdataconsole.pingdemo.example/console</li><li>Server: ldaps://myping-pingauthorize-cluster:1636</li><li>Username: administrator</li><li>Password: 2FederateM0re</li></ul>"},{"location":"deployment/deployHelmLocalIngress/#cleaning-up","title":"Cleaning up","text":"<p>Since the ingresses are deployed as part of the overall release, deleting the release will also remove the ingress definitions (leaving the ingress controller intact).  </p> <p>The ingress controller will be removed when the cluster is deleted.  If you only want to remove the ingress controller, you can either:</p> <ul> <li>Run <code>kubectl delete -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/1.23/deploy.yaml</code> if you installed the controller from Github</li> </ul> <p>OR</p> <ul> <li>Run <code>kubectl delete -f ./20-kubernetes/kind-nginx.yaml</code> if you used the local copy to install the controller.</li> </ul>"},{"location":"deployment/deployK8s-AKS/","title":"Preparing Azure Kubernetes Service","text":"<p>This directory contains scripts and deployment files to help with the deployment, management, and scaling of Ping Identity DevOps Docker images to Microsoft Azure Kubernetes Service (AKS).</p>"},{"location":"deployment/deployK8s-AKS/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, you must:</p> <ul> <li>Set up your DevOps environment and run a test deployment of the products. For more information, see Get Started.</li> <li>Create a Kubernetes cluster on AKS.</li> <li>Create a Kubernetes secret using your DevOps credentials. See the For Kubernetes topic in Using your DevOps user and key.</li> <li>Download and install the Azure CLI.</li> </ul> <p>We also highly recommend you are familiar with the information in these AKS articles:</p> <ul> <li>Azure Kubernetes Service</li> </ul>"},{"location":"deployment/deployK8s-AKS/#deploying-our-fullstack-example-in-aks","title":"Deploying our fullstack example in AKS","text":"<ol> <li> <p>Create an Azure Resource Group to put all resources into by entering:</p> <pre><code>az group create \\\n   --name ping-devops-rg \\\n   --location westus\n</code></pre> </li> <li> <p>Create a two-node Azure AKS cluster by entering the following.</p> <pre><code>az aks create \\\n   --resource-group ping-devops-rg \\\n   --name ping-devops-cluster \\\n   --node-count 2 \\\n   --enable-addons monitoring \\\n   --ssh-key-value ~/.ssh/id_rsa.pub\n</code></pre> <p>You need a public certificate by default in ~/.ssh/id_rsa.pub.</p> </li> <li> <p>Import the AKS Credentials into <code>.kube/config</code> by entering:</p> <pre><code>az aks get-credentials \\\n   --resource-group ping-devops-rg \\\n   --name ping-devops-cluster\n</code></pre> </li> <li> <p>At this point, the cluster should be ready for helm deployments.</p> </li> <li> <p>To clean up the Azure Resource Group and all associated resources, including the AKS cluster created, enter the following command.</p> </li> </ol> <p>Warning</p> <p>This will remove everything you created that is associated with this resource group.</p> <pre><code>az group delete \\\n  --name ping-devops-rg\n</code></pre>"},{"location":"deployment/deployK8s-AWS/","title":"Preparing AWS EKS for Multi-Region Deployments","text":""},{"location":"deployment/deployK8s-AWS/#overview","title":"Overview","text":"<p>In this guide you will deploy two Kubernetes clusters, each in a different Amazon Web Services (AWS) region. An AWS virtual private cloud (VPC) is assigned and dedicated to each cluster. You will also add communication between these clusters, using a transit gateway. Throughout this document, \"VPC\" is synonymous with \"cluster\".</p>"},{"location":"deployment/deployK8s-AWS/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, you must have</p> <ul> <li>AWS account permissions to create clusters</li> </ul>"},{"location":"deployment/deployK8s-AWS/#create-the-multi-region-clusters","title":"Create the multi-region clusters","text":"<ol> <li> <p>Create VPCs.</p> <ul> <li> <p>Sign on to the AWS console and navigate to the VPC service.</p> </li> <li> <p>Toggle to the <code>eu-west-1</code> region.</p> </li> <li> <p>Select Your VPCs (under Virtual Private Cloud) and click Create VPC</p> </li> <li> <p>Add a name tag, such as <code>demo-vpc-eu-west-1</code></p> </li> <li> <p>Add a IPv4 CIDR, such as <code>10.0.0.0/16</code></p> </li> <li> <p>Click Create VPC.</p> <p>Make note of the <code>VpcId</code> and <code>IPv4 CIDR</code> values for the <code>eu-west-1</code> and <code>us-east-1</code> VPCs for use in subsequent steps.</p> </li> <li> <p>Repeat this step in <code>us-east-1</code> region.</p> </li> </ul> </li> <li> <p>Create the transit gateway for each region on which your deployment is being hosted. Toggle to the <code>eu-west-1</code> region.</p> <ul> <li> <p>Navigate to the Transit gateways section and click Create transit gateway.</p> </li> <li> <p>Add a name tag such as <code>demo-tgw-eu-west-1</code>.</p> </li> <li> <p>Add a unique Amazon side Autonomous System Number for each region (ex. 64512 or 64513).</p> </li> <li> <p>Disable both the <code>Default route table association</code> and <code>Default route table propagation</code>.</p> <p>Note: If you enable the options above, the default association route table and propagation route table will be created. This action may not suit more complex routing needs; see below for details on how to manually set the associations/propagation route tables.</p> </li> <li> <p>Click Create transit gateway.</p> </li> <li> <p>Repeat this step in <code>us-east-1</code> region.</p> </li> </ul> </li> <li> <p>Create the transit gateway peering connection attachment. Toggle to the <code>eu-west-1</code> region.</p> <ul> <li> <p>Navigate to the Transit gateway attachments section and click Create transit gateway attachments.</p> </li> <li> <p>Add a name tag such as <code>demo-peering-attachment-us-east-1</code>.</p> <p>Note: This name refers to the region to which it is peering, not the region in which it is being created.</p> </li> <li> <p>Select the Transit gateway id that you just made in the <code>eu-west-1</code> region.</p> </li> <li> <p>Change Attachment type to <code>Peering Connection</code>.</p> </li> <li> <p>For Region select <code>us-east-1</code>.</p> </li> <li> <p>For Transit gateway (accepter) add the Transit gateway id that you just made in the <code>us-east-1</code> region.</p> </li> <li> <p>Click Create transit gateway attachment .</p> </li> </ul> </li> <li> <p>Accept transit gateway peering attachment.</p> <ul> <li> <p>After the transit gateway peering connection shows <code>pending acceptance</code> as its State, toggle to the <code>us-east-1</code> region and select Transit gateway attachments.</p> </li> <li> <p>You should see the attachment you just made. Select Actions and click accept.</p> </li> <li> <p>Add a name to this attachment such as <code>demo-peering-attachment-eu-west-1</code>.</p> <p>Note: This name refers to the region to which it is peering, not the region in which it is being created.</p> </li> </ul> </li> <li> <p>Attach VPCs to the transit gateways in each region. Toggle to the <code>eu-west-1</code> region.</p> <ul> <li> <p>Navigate to the Transit gateway attachments section and click Create transit gateway attachments.</p> </li> <li> <p>Add a name tag such as <code>demo-vpc-eu-west-1</code>.</p> </li> <li> <p>Select the Transit gateway id that you just made in the <code>eu-west-1</code> region.</p> </li> <li> <p>Select the Vpc Id that you made note of in step 3 for the <code>eu-west-1</code> region.</p> </li> <li> <p>Click Create transit gateway attachment.</p> </li> <li> <p>Repeat this step in <code>us-east-1</code> region.</p> </li> </ul> </li> <li> <p>Accept transit gateway VPC attachments. Toggle to the <code>eu-west-1</code> region.</p> <ul> <li> <p>Navigate to the Transit gateway attachments section and click Create transit gateway attachments.</p> </li> <li> <p>You should see the vpc attachment you just made. Select Actions and click accept.</p> <p>Note: If you are using different accounts to create transit gateways and their attachments, the name tag will not be visible here. In this situation, you should add an attachment name now, such as <code>demo-vpc-eu-west-1</code>.</p> </li> <li> <p>Repeat this step in <code>us-east-1</code> region.</p> </li> </ul> </li> <li> <p>Add routes to vpc route table. Toggle to the <code>eu-west-1</code> region.</p> <ul> <li> <p>Navigate to the Route tables section and select the route table for the Vpc Id you created.</p> </li> <li> <p>Select Routes in the bottom third of the page.</p> </li> <li> <p>Select Edit routes then click Add route.</p> </li> <li> <p>Add a destination that is more broad than the local one that is present. For example if the local destination is <code>10.0.0.0/16</code> add <code>10.0.0.0/8</code>. </p> </li> <li> <p>For the Target select <code>Transit Gateway</code> and add then add the Transit gateway id that you created in this region.</p> </li> <li> <p>Click Save changes.</p> </li> <li> <p>Repeat this step in <code>us-east-1</code> region.</p> </li> </ul> </li> <li> <p>Configure the transit gateway route tables. Toggle to the <code>eu-west-1</code> region.</p> <ul> <li> <p>Navigate to the Transit gateway route tables section and click Create transit gateway route table.</p> </li> <li> <p>Add a name tag such as <code>demo-eu-west-1-route-table</code>.</p> </li> <li> <p>Select the Transit gateway id that you created in this region.</p> </li> <li> <p>Click Create transit gateway route table.</p> </li> <li> <p>Repeat this step in <code>us-east-1</code> region.</p> </li> </ul> </li> <li> <p>Associate the transit gateway. Toggle to the <code>eu-west-1</code> region.</p> <ul> <li> <p>After the transit gateway route table has been successfully created, select that route table and click Associations then Create association.</p> </li> <li> <p>Choose the VPC attachment for this region and click Create association.</p> </li> </ul> </li> <li> <p>Add static routes to the transit gateway. Toggle to the <code>eu-west-1</code> region.</p> <ul> <li> <p>Select that route table that you just created an association for and click Routes then Create static route.</p> </li> <li> <p>Add the<code>IPv4 CIDR</code> for the remote VPC that you made note of in step 3 for the <code>us-east-1</code> region.</p> </li> <li> <p>Select the transit gateway peering connection attachment.</p> </li> <li> <p>Click Create static route.</p> </li> <li> <p>Repeat this step in <code>us-east-1</code> region.</p> </li> </ul> </li> <li> <p>Create a blackout static route to ensure the transit gateway drops any other network traffic. Toggle to the <code>eu-west-1</code> region.</p> <ul> <li> <p>Select Create static route </p> </li> <li> <p>Add 10.0.0.0/8 as the CIDR</p> </li> <li> <p>Select Blackhole</p> </li> <li> <p>Click Create static route.</p> </li> <li> <p>Repeat this step in <code>us-east-1</code> region.</p> </li> </ul> </li> </ol> <p>At this point you should have a system of connected VPCs on the <code>us-east-1</code> <code>eu-west-1</code> regions. You can now deploy EC2 instances to these VPCs and communicate between them.</p>"},{"location":"deployment/deployK8sCloud/","title":"Kubernetes deployments for cloud platforms","text":"<p>We currently have instructions for typical configuration of Kubernetes for use with Ping products on these platforms:</p> <ul> <li>Amazon Web Services (AWS) Elastic Kubernetes Service (EKS)</li> <li>Microsoft Azure Kubernetes Service (AKS)</li> </ul> <p>Each hosting platform supports and manages Kubernetes differently.</p>"},{"location":"deployment/deployK8sCloud/#before-you-begin","title":"Before you begin","text":"<p>You must:</p> <ul> <li>Complete Get Started to set up your DevOps environment and run a test deployment of the products.</li> <li> <p>Create a Kubernetes cluster on one of these platforms:</p> <ul> <li>Amazon EKS</li> <li>Microsoft AKS</li> </ul> </li> <li> <p>Create a Kubernetes secret using your DevOps credentials. For more information, see For Kubernetes  in Using your DevOps user and key.</p> </li> </ul>"},{"location":"deployment/deployK8sCloud/#aws-eks","title":"AWS EKS","text":"<p>See Deploy Peered EKS Clusters.</p>"},{"location":"deployment/deployK8sCloud/#aks","title":"AKS","text":"<p>See Deploy to Azure Kubernetes Service.</p>"},{"location":"deployment/deployK8sClusterMetrics/","title":"Deploy a Kubernetes Cluster Metrics Stack","text":"<p>This document demonstrates the process of deploying and using a sample open-source monitoring stack in a Kubernetes cluster.</p> <p>Not For Production</p> <p>The resulting environment is not production-ready.  It is only intended to show how Ping software can produce metrics for consumption by a popular open-source monitoring system. This example stack is not maintained or directly supported by Ping.</p>"},{"location":"deployment/deployK8sClusterMetrics/#stack-components","title":"Stack Components","text":"<p>Open Source Tools</p> <ul> <li>kube-prometheus-stack, which includes:<ul> <li>Prometheus - Metrics collection and storage</li> <li>Grafana - Metrics visualization in Dashboards</li> <li>telegraf-operator -  Metrics exposure and formatting</li> </ul> </li> </ul> <p>Grafana Dashboard - JSON file to import for dashboard definition</p> <p>Ping-provided values.yaml - Values relevant to exposing metrics for Ping Identity software</p>"},{"location":"deployment/deployK8sClusterMetrics/#prerequisites","title":"Prerequisites","text":"<ul> <li>Familiarity with the prerequisites for the base Helm examples</li> <li>Working knowledge of Prometheus, Grafana, and Telegraf</li> </ul>"},{"location":"deployment/deployK8sClusterMetrics/#deploy-the-stack","title":"Deploy the Stack","text":"<p>In the <code>pingidentity-devops-getting-started/30-helm/cluster-metrics directory</code> of this repository, edit the <code>01-prometheus-values.yaml</code> as needed. This file provides configurations beyond the default kube-prometheus-stack. In this sample deployment, the monitoring stack is granted read access to the entire cluster and is deployed into the <code>metrics</code> namespace.</p> <p>Preparing for production use</p> <p>Altering these settings or making the deployment production-ready is beyond scope of this document. The full set of optional values can be found on the Github repository for the Prometheus chart.</p> <p>There are numerous lines that have <code>##CHANGEME</code>. These lines should be considered for configuration options to meet your needs.</p> <p>After updating the file, deploy the <code>kube-prometheus-stack</code>.  The path to the configuration file assumes you are in the root folder of a local copy of the Getting Started repository:</p> <pre><code>kubectl create namespace metrics\n\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n\nhelm repo update\n\nhelm upgrade --install metrics prometheus-community/kube-prometheus-stack -f 30-helm/cluster-metrics/01-prometheus-values.yaml -n metrics --version 57.2.0\n</code></pre> <p>Deploy <code>telegraf-operator</code>:</p> <pre><code>helm repo add influxdata https://helm.influxdata.com/\n\nhelm upgrade --install telegraf influxdata/telegraf-operator -n metrics --version 1.3.11 -f 30-helm/cluster-metrics/02-telegraf-values.yaml\n</code></pre> <p>Telegraf operator makes it very easy to add monitoring sidecars to your deployments. All you need to do is add annotations, which are shown in <code>30-helm/cluster-metrics/03-ping-with-metrics-values.yaml</code></p> <p>These values can be copied to your ping-devops <code>values.yaml</code> manually, or the file can be referenced at the end of your helm install command. For example:</p> <pre><code>helm upgrade --install ping-metrics pingidentity/ping-devops -f my-values.yaml -f 30-helm/cluster-metrics/03-ping-with-metrics-values.yaml\n</code></pre> <p>After the Ping software is healthy and producing metrics, there should be sidecars on Ping pods.</p> <pre><code>NAME                                                 READY   STATUS\nping-metrics-pingaccess-admin-0                      1/1     Running\nping-metrics-pingaccess-engine-68464d8cc8-mhlsv      2/2     Running\nping-metrics-pingdataconsole-559786c98f-8wsrm        1/1     Running\nping-metrics-pingdirectory-0                         2/2     Running\nping-metrics-pingfederate-admin-64fdb4b975-2xdjl     1/1     Running\nping-metrics-pingfederate-engine-64c5f896c7-fn99v    2/2     Running\n</code></pre> <p>Note the <code>2/2</code> indicator for pods with sidecars.</p>"},{"location":"deployment/deployK8sClusterMetrics/#view-metrics","title":"View Metrics","text":"<p>Browse to Grafana using the Ingress URL or by running a <code>kubectl port-forward</code> command.  For example: <code>kubectl port-forward svc/metrics-grafana --namespace metrics 9000:80</code> In your browser, navigate to <code>http://localhost:9000</code> and log in with the user <code>admin</code> and the password set in <code>01-prometheus-values.yaml</code></p> <p>Finally, import the <code>04-ping-overview-dashboard.json</code> using the New button at the top right of the Dashboard landing page in Grafana.</p> <p>The <code>Ping Identity Overview</code> dashboard will have a dropdown for namespace at the top. Select the namespace running Ping products to see something similar to this example:</p> <p></p> <p>Any of the panels can be edited, or new ones created to fit your needs.</p>"},{"location":"deployment/deployK8sClusterMetrics/#horizontalpodautoscaler","title":"HorizontalPodAutoscaler","text":"<p>If you use the <code>autoscaling/v2</code> API version, you can configure a HorizontalPodAutoscaler to scale based on a custom metric not built in to Kubernetes or any Kubernetes component. If you are using our Helm Charts, you can pass the custom metrics under <code>global.cluster.autoscalingMetricsTemplate</code>. The example code here will scale on a requests-per-second threshold of 10,000:</p> <pre><code>  - type: Pods\n    pods:\n     metric:\n       name: custom-metric\n     target:\n       type: AverageValue\n       averageValue: 10000m\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 50\n  - type: Object\n    object:\n      metric:\n        name: requests-per-second\n      describedObject:\n        apiVersion: networking.k8s.io/v1\n        kind: Ingress\n        name: main-route\n      current:\n        value: 10k\n</code></pre> <p>In addition, you can define the behaviors for scaling up and down under <code>global.cluster.autoscaling.behavior</code>.</p> <pre><code>  scaleDown:\n    stabilizationWindowSeconds: 300\n    policies:\n    - type: Percent\n      value: 100\n      periodSeconds: 15\n  scaleUp:\n    stabilizationWindowSeconds: 0\n    policies:\n    - type: Percent\n      value: 100\n      periodSeconds: 15\n    - type: Pods\n      value: 4\n      periodSeconds: 15\n    selectPolicy: Max\n</code></pre> <p>For more information on custom HPA metrics please visit Kubernetes</p>"},{"location":"deployment/deployK8sUtilitySidecar/","title":"Using a Utility Sidecar","text":""},{"location":"deployment/deployK8sUtilitySidecar/#why-use-a-sidecar","title":"Why Use a Sidecar","text":"<p>When running containerized software, each individual container should represent one process. This model allows containers to be minimal, more easily secured, and to be configured with the proper resource allocations accurately.</p> <p>There are common situations where running commands and tools on a pod running Ping Identity software is useful. These situations include collecting a support archive, exporting data, or running a backup. However, because many of these processes might introduce unexpected contention for CPU and memory resources, executing such commands inside the container running the actual server process can be risky. The container for the product is sized for the server process without consideration for auxiliary processes that might be executed at the same time.</p> <p>To avoid these issues, one practice is to use a utility sidecar for tools that need to run alongside the main server process. This sidecar runs as a separate container on the pod.  However, in the Kubernetes model, all containers in the same pod can share a process namespace (if enabled), and can also be configured to share a persistent volume. This co-location allows any required processes to run in the sidecar without competing with the main server process for the same resources.  </p> <p>The major downside of running a utility sidecar is that it must always be running because new containers cannot be attached to existing pods. The sidecar can be configured with minimal memory and CPU resources, but will continue to run even when it is not actively in use.</p> <p>StatefulSets</p> <p>You cannot remove a sidecar from a running StatefulSet without rolling all the pods.</p>"},{"location":"deployment/deployK8sUtilitySidecar/#how-to-deploy-a-sidecar","title":"How to Deploy a Sidecar","text":"<p>If you are using the Ping Identity Helm charts, you can update your custom values.yaml file to enable a sidecar for any product. For example:</p> <pre><code>pingdirectory:\n  enabled: true\n  workload:\n    # Share process namespace for sidecar to get a view inside the main container\n    shareProcessNamespace: true\n  # Share /tmp so sidecar can see Java processes. Don't keep /tmp around between restarts though.\n  volumes:\n  - name: temp\n    emptyDir: {}\n  volumeMounts:\n  - name: temp\n    mountPath: /tmp\n  # Backups, restores, and other CLI tools should be run from the sidecar to prevent interfering\n  # with the main PingDirectory container process.\n  utilitySidecar:\n    enabled: true\n</code></pre> <p>These values will add a sidecar container using the same image as the main server container, configured with minimal resources and waiting in an endless loop. The generated yaml will look like the following example and could be applied directly outside of Helm:</p> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: pingdirectory\n  name: sidecar-pingdirectory\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: pingdirectory\n  serviceName: sidecar-pingdirectory-cluster\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: pingdirectory\n    spec:\n      containers:\n      - envFrom:\n        - secretRef:\n            name: devops-secret\n            optional: true\n        image: pingidentity/pingdirectory:latest\n        livenessProbe:\n          exec:\n            command:\n            - /opt/liveness.sh\n          failureThreshold: 4\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: pingdirectory\n        ports:\n        - containerPort: 1443\n          name: https\n        - containerPort: 1389\n          name: ldap\n        - containerPort: 1636\n          name: ldaps\n        readinessProbe:\n          exec:\n            command:\n            - /opt/readiness.sh\n          failureThreshold: 4\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2\n            memory: 8Gi\n          requests:\n            cpu: 50m\n            memory: 2Gi\n        startupProbe:\n          exec:\n            command:\n            - /opt/liveness.sh\n          failureThreshold: 180\n          periodSeconds: 10\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /tmp\n          name: temp\n        - mountPath: /opt/out\n          name: out-dir\n      - args:\n        - -f\n        - /dev/null\n        command:\n        - tail\n        image: pingidentity/pingdirectory:latest\n        name: utility-sidecar\n        resources:\n          limits:\n            cpu: \"1\"\n            memory: 2Gi\n          requests:\n            cpu: \"0\"\n            memory: 128Mi\n        volumeMounts:\n        - mountPath: /opt/out\n          name: out-dir\n        - mountPath: /tmp\n          name: temp\n      securityContext:\n        fsGroup: 0\n        runAsGroup: 0\n        runAsUser: 9031\n      shareProcessNamespace: true\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - emptyDir: {}\n        name: temp\n      - name: out-dir\n        persistentVolumeClaim:\n          claimName: out-dir\n  volumeClaimTemplates:\n  - metadata:\n      name: out-dir\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n      storageClassName: null\n</code></pre>"},{"location":"deployment/deployLocalK8sCluster/","title":"Deploy a local Kubernetes Cluster","text":"<p>If you do not have access to a managed Kubernetes cluster you can deploy one on your local machine or or a virtual machine (VM). This document describes deploying a cluster with kind and also for minikube.  Refer to the documentation of each product for additional information.</p> <p>Demo Use Only</p> <p>The instructions in this document are for testing and learning, and not intended for use in production.</p> <p>Why not Docker Desktop?</p> <p>The processes outlined on this page will create either a Kubernetes in Docker (kind) or a minikube cluster.  In both cases, the cluster you get is very similar in functionality to the Docker Desktop implementation of Kubernetes.  However, a distinct advantage of both offerings is portability (not requiring Docker Desktop). As with the Getting Started example, the files provided will enable and deploy an ingress controller for communicating with the services in the cluster from your local environment.</p> <p>Kubernetes in Docker Desktop</p> <p>To use the both examples below, you will need to ensure the Kubernetes feature of Docker Desktop is turned off, as it will conflict.</p> <p>Docker System Resources</p> <p>This note applies only if using Docker as a backing for either solution. kind uses Docker by default, and it is also an option for minikube.  Docker on Linux is typically installed with root privileges and thus has access to the full resources of the machine. Docker Desktop for Mac and Windows provides a way to set the resources allocated to Docker. For this documentation, a Macbook Pro was configured to use 6 CPUs and 12 GB Memory. You can adjust these values as necessary for your needs.</p>"},{"location":"deployment/deployLocalK8sCluster/#kind-cluster","title":"Kind cluster","text":"<p>This section will cover the kind installation process. See the section further down for minikube instructions.</p>"},{"location":"deployment/deployLocalK8sCluster/#prerequisites","title":"Prerequisites","text":"<ul> <li>docker</li> <li>kubectl</li> <li>ports 80 and 443 available on machine</li> </ul> <p>Kubernetes Version</p> <p>For this guide, the kind implementation of Kubernetes 1.29.1 is used. It is deployed using version 0.21.0 of kind.</p> <p>Docker Desktop Version</p> <p>At the time of the writing of this guide, Docker Desktop was version <code>4.27.1 (136059)</code>, which used Docker Engine <code>25.0.2</code>.</p>"},{"location":"deployment/deployLocalK8sCluster/#install-and-confirm-the-cluster","title":"Install and confirm the cluster","text":"<ol> <li> <p>Install kind on your platform.</p> </li> <li> <p>Use the provided sample kind.yaml file to create a kind cluster named <code>ping</code> with ingress support enabled.  From the root of your copy of the repository code, run:</p> <pre><code>kind create cluster --config=./20-kubernetes/kind.yaml\n</code></pre> <p>Output:</p> <p></p> </li> <li> <p>Test cluster health by running the following commands:</p> <pre><code>kubectl cluster-info\n\n# Output - port will vary\nKubernetes control plane is running at https://127.0.0.1:50766\nCoreDNS is running at https://127.0.0.1:50766/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n\n------------------\n\nkubectl version\n\n&lt; output clipped &gt;\nServer Version: v1.29.1\n\n------------------\n\nkubectl get nodes\n\nNAME                 STATUS   ROLES           AGE     VERSION\nping-control-plane   Ready    control-plane   38s     v1.29.1\n</code></pre> </li> </ol>"},{"location":"deployment/deployLocalK8sCluster/#enable-ingress","title":"Enable ingress","text":"<ol> <li>Next, install the nginx-ingress-controller for <code>kind</code> (version 1.9.6 at the time of this writing). In the event the Github file is unavailable, a copy has been made to this repository here.</li> </ol> <p>To use the Github file:     <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.9.6/deploy/static/provider/kind/deploy.yaml\n</code></pre></p> <p>To use the local copy:     <pre><code>kubectl apply -f ./20-kubernetes/kind-nginx.yaml\n</code></pre></p> <p>Output:     <pre><code>namespace/ingress-nginx created\nserviceaccount/ingress-nginx created\nserviceaccount/ingress-nginx-admission created\nrole.rbac.authorization.k8s.io/ingress-nginx created\nrole.rbac.authorization.k8s.io/ingress-nginx-admission created\nclusterrole.rbac.authorization.k8s.io/ingress-nginx created\nclusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created\nrolebinding.rbac.authorization.k8s.io/ingress-nginx created\nrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created\nclusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created\nclusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created\nconfigmap/ingress-nginx-controller created\nservice/ingress-nginx-controller created\nservice/ingress-nginx-controller-admission created\ndeployment.apps/ingress-nginx-controller created\njob.batch/ingress-nginx-admission-create created\njob.batch/ingress-nginx-admission-patch created\ningressclass.networking.k8s.io/nginx created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created\n</code></pre></p> <ol> <li> <p>To wait for the Nginx ingress to reach a healthy state, run the following command.  You can also observe the pod status using k9s or by running <code>kubectl get pods --namespace ingress-nginx</code>. You should see one controller pod running when the ingress controller is ready.  This command should exit after no more than 90 seconds or so, depending on the speed of your computer:</p> <pre><code>kubectl wait --namespace ingress-nginx \\\n  --for=condition=ready pod \\\n  --selector=app.kubernetes.io/component=controller \\\n  --timeout=90s\n</code></pre> </li> <li> <p>Verify nginx-ingress-controller is working:</p> <pre><code>curl localhost\n</code></pre> <p>Output: <pre><code>&lt;html&gt;\n&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> </li> </ol> <p>Our examples will use the Helm release name <code>myping</code> and DNS domain suffix <code>pingdemo.example</code> for accessing applications.  You can add all expected hosts to <code>/etc/hosts</code>:</p> <pre><code>echo '127.0.0.1 myping-pingaccess-admin.pingdemo.example myping-pingaccess-engine.pingdemo.example myping-pingauthorize.pingdemo.example myping-pingauthorizepap.pingdemo.example myping-pingdataconsole.pingdemo.example myping-pingdelegator.pingdemo.example myping-pingdirectory.pingdemo.example myping-pingfederate-admin.pingdemo.example myping-pingfederate-engine.pingdemo.example myping-pingcentral.pingdemo.example' | sudo tee -a /etc/hosts &gt; /dev/null\n</code></pre> <p>Setup is complete.  This local Kubernetes environment should be ready to deploy our Helm examples</p>"},{"location":"deployment/deployLocalK8sCluster/#stop-the-cluster","title":"Stop the cluster","text":"<p>When you are finished, you can remove the cluster by running the following command, which removes the cluster completely.  You will be required to recreate the cluster and reinstall the ingress controller to use <code>kind</code> again.</p> <pre><code>kind delete cluster --name ping\n</code></pre>"},{"location":"deployment/deployLocalK8sCluster/#minikube-cluster","title":"Minikube cluster","text":"<p>In this section, a minikube installation with ingress is created.  Minikube is simpler than kind overall to configure, but ends up needing one step to configured a tunnel to the cluster that must be managed.  For this guide, the Docker driver will be used.  As with <code>kind</code> above, Kubernetes in Docker Desktop must be disabled.</p>"},{"location":"deployment/deployLocalK8sCluster/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Container or virtual machine manager, such as: Docker, QEMU, Hyperkit, Hyper-V, KVM, Parallels, Podman, VirtualBox, or VMware Fusion/Workstation</li> <li>kubectl</li> </ul> <p>Minikube and Kubernetes Version</p> <p>At the time of the writing of this guide, minikube was version <code>1.32.0</code>, which installs Kubernetes version <code>1.28.3</code>.</p>"},{"location":"deployment/deployLocalK8sCluster/#install-and-configure-minikube","title":"Install and configure minikube","text":"<ol> <li> <p>Install minikube for your platform.  See the product Get Started! page for details.</p> </li> <li> <p>Configure the minikube resources and virtualization driver.  For example, the following options were used on an Apple Macbook Pro with Docker as the backing platform:</p> <pre><code>minikube config set cpus 6\nminikube config set driver docker\nminikube config set memory 12g\n</code></pre> <p>Configuration</p> <p>See the documentation for more details on configuring minikube.</p> </li> <li> <p>Start the cluster.  Optionally you can include a profile flag (<code>--profile &lt;name&gt;</code>). Naming the cluster enables you to run multiple minikube clusters simultaneously.  If you use a profile name, you will need to include it on other minikube commands.     <pre><code>minikube start --addons=ingress --kubernetes-version=v1.28.3\n</code></pre></p> <p>Output:</p> <p></p> </li> <li> <p>Test cluster health by running the following commands:</p> <pre><code>kubectl cluster-info\n\n# Output - Port will vary\nKubernetes control plane is running at https://127.0.0.1:62531\nCoreDNS is running at https://127.0.0.1:62531/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n\n------------------\n\nkubectl version\n\n&lt; output clipped &gt;\nServer Version: v1.28.3\n\n------------------\n\nkubectl get nodes\n\nNAME       STATUS   ROLES           AGE    VERSION\nminikube   Ready    control-plane   4m6s   v1.28.3\n</code></pre> </li> </ol>"},{"location":"deployment/deployLocalK8sCluster/#confirm-ingress","title":"Confirm ingress","text":"<ol> <li> <p>Confirm ingress is operational:</p> <pre><code>kubectl get po -n ingress-nginx\n\nNAME                                        READY   STATUS      RESTARTS   AGE\ningress-nginx-admission-create-lr2x2        0/1     Completed   0          174m\ningress-nginx-admission-patch-bjgnn         0/1     Completed   1          174m\ningress-nginx-controller-6cc5ccb977-9n66n   1/1     Running     0          174m\n</code></pre> </li> <li> <p>Deploy a test application</p> <p>Use the following YAML file to create a Pod, Service and Ingress:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: example-web-pod\n  labels:\n    role: webserver\nspec:\n  containers:\n    - name: web\n      image: nginx\n      ports:\n        - name: web\n          containerPort: 80\n          protocol: TCP\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: example-svc\nspec:\n  selector:\n    role: webserver\n  ports:\n    - protocol: TCP\n      port: 80\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  namespace: default\n  annotations:\n    spec.ingressClassName: nginx\nspec:\n  rules:\n    - host: example.k8s.local\n      http:\n        paths:\n          - backend:\n              service:\n                name: example-svc\n                port:\n                  number: 80\n            path: /\n            pathType: Prefix\n</code></pre> <pre><code>kubectl apply -f test.yaml\n\npod/example-web-pod created\nservice/example-svc created\ningress.networking.k8s.io/example-ingress created\n</code></pre> </li> <li> <p>Add an alias to the application to <code>/etc/hosts</code></p> <pre><code>echo '127.0.0.1 example.k8s.local' | sudo tee -a /etc/hosts &gt; /dev/null\n</code></pre> </li> <li> <p>Start a tunnel.  This command will tie up the terminal:</p> <pre><code>minikube tunnel\n\n\u2705  Tunnel successfully started\n\n\ud83d\udccc  NOTE: Please do not close this terminal as this process must stay alive for the tunnel to be accessible ...\n\n\u2757  The service/ingress example-ingress requires privileged ports to be exposed: [80 443]\n\ud83d\udd11  sudo permission will be asked for it.\n\ud83c\udfc3  Starting tunnel for service example-ingress.\n</code></pre> <p>Open a browser to <code>http://example.k8s.local</code>.  You should see the Nginx landing page.</p> </li> <li> <p>Clean up tests</p> <pre><code>kubectl delete -f test.yaml\n</code></pre> </li> </ol> <p>Our examples will use the Helm release name <code>myping</code> and DNS domain suffix <code>pingdemo.example</code> for accessing applications.  You can add all expected hosts to <code>/etc/hosts</code>:</p> <pre><code>echo '127.0.0.1 myping-pingaccess-admin.pingdemo.example myping-pingaccess-engine.pingdemo.example myping-pingauthorize.pingdemo.example myping-pingauthorizepap.pingdemo.example myping-pingdataconsole.pingdemo.example myping-pingdelegator.pingdemo.example myping-pingdirectory.pingdemo.example myping-pingfederate-admin.pingdemo.example myping-pingfederate-engine.pingdemo.example myping-pingcentral.pingdemo.example' | sudo tee -a /etc/hosts &gt; /dev/null\n</code></pre> <p>Setup is complete.  This local Kubernetes environment should be ready to deploy our Helm examples</p>"},{"location":"deployment/deployLocalK8sCluster/#optional-features","title":"Optional features","text":""},{"location":"deployment/deployLocalK8sCluster/#dashboard","title":"Dashboard","text":"<p>Minikube provides other add-ons that enhance your experience when working with your cluster.  One such add-on is the Dashboard, which can also provide metrics as follows:</p> <pre><code>minikube addons enable metrics-server\nminikube dashboard\n</code></pre>"},{"location":"deployment/deployLocalK8sCluster/#multiple-nodes","title":"Multiple nodes","text":"<p>If you have enough system resources, you can create a multi-node cluster.</p> <p>For example, to start a 3-node cluster: <pre><code>minikube start --nodes 3\n</code></pre></p> <p>Resources</p> <p>Keep in mind that each node will receive the RAM/CPU/Disk configured for minikube.  Using the example configuration provided above, a 3-node cluster would need 36GB of RAM and 18 CPUs.</p>"},{"location":"deployment/deployLocalK8sCluster/#stop-the-cluster_1","title":"Stop the cluster","text":"<p>When you are finished, you can stop the cluster by running the following command.  Stopping retains the configuration and state of the cluster (namespaces, deployments, and so on) that will be restored when starting the cluster again.  </p> <pre><code>minikube stop\n</code></pre> <p>You can also pause and unpause the cluster:</p> <pre><code>minikube pause\nminikube unpause\n</code></pre> <p>Alternatively, you can delete the minikube environment, which will recreate everything the next time.</p> <pre><code>minikube delete \n</code></pre>"},{"location":"deployment/deployLocalOpenshift/","title":"Deploy a Local Openshift Cluster","text":"<p>Demo Use Only</p> <p>The instructions in this document are for testing and learning, and not intended for use in production.</p> <p>Licensing</p> <p>Openshift is a licensed product following the open source model where \"upstream\" versions are available under community licensing, but the official release supported by Red Hat requires a subscription in order to access the software for installation.  This guide assumes the user has access to such a license.  Red Hat provides a free Developer account that allows a participant to obtain a 60-day license of the Openshift product at no charge.</p> <p>Video Demonstration</p> <p>A video demonstration of the process outlined on this page is available here.</p> <p>Some customers are using Openshift as their platform for running Ping containerized applications.  If this is the case, access to an Openshift cluster is assumed.  Even in those cases, there are times where a local implementation of Openshift for development and testing is convenient.  </p> <p>Platform</p> <p>For this guide, the Apple MacBook Pro platform is used, and the release of the Red Hat Openshift Local offering is version 2.36.0, which installs Openshift 4.15.12.</p> <p>The Openshift Local offering is used in this guide. This page was derived from the documentation provided by Red Hat.</p>"},{"location":"deployment/deployLocalOpenshift/#prerequisites","title":"Prerequisites","text":"<ul> <li>Entitlement for Openshift code.  If you have registered for the Red Hat developer program, you can obtain your entitlement for the free trial from the portal after logging in.</li> <li>kubectl</li> <li>Openshift client (oc)</li> <li>ports 80, 443 and 6443 available on machine. If you have Docker Desktop installed, you must either disable Kubernetes or stop Docker in order for the installation to work.</li> <li>sudo privileges on your hosting environment</li> </ul>"},{"location":"deployment/deployLocalOpenshift/#configuration-and-setup","title":"Configuration and Setup","text":"<ol> <li> <p>Install the Red Hat Openshift Local binary for your platform. When you download the installer, also download the pull secret.</p> </li> <li> <p>With the <code>crc</code> utility installed, configure settings.  Provide as much RAM and CPU as you can, depending on your system.  In this example, 16 GB of RAM, 7 vCPUs, and a disk size of 80GB are set.  The consent-telemetry is optional, depending on whether you consent to usage data metrics being sent to Red Hat.</p> <pre><code># Version information\ncrc version\n\n# Output\nCRC version: 2.36.0+27c493\nOpenShift version: 4.15.12\nPodman version: 4.4.4\n\n# Set configuration\ncrc config set memory 16384\ncrc config set cpus 7\ncrc config set consent-telemetry no\ncrc config set disk-size 80\n\n# Confirm\ncrc config view\n\n- consent-telemetry                     : no\n- cpus                                  : 7\n- disk-size                             : 80\n- memory                                : 16384\n</code></pre> </li> <li> <p>Set up your local machine for running Red Hat Openshift Local by running crc setup:</p> <pre><code>crc setup\n\n# Output\nINFO Using bundle path /Users/userjoe/.crc/cache/crc_vfkit_4.15.12_amd64.crcbundle\nINFO Checking if running macOS version &gt;= 13.x\nINFO Checking if running as non-root\nINFO Checking if crc-admin-helper executable is cached\nINFO Checking if running on a supported CPU architecture\nINFO Checking if crc executable symlink exists\nINFO Checking minimum RAM requirements\nINFO Check if Podman binary exists in: /Users/userjoe/.crc/bin/oc\nINFO Removing Podman binary from: /Users/userjoe/.crc/bin/oc\nINFO Checking if running emulated on Apple silicon\nINFO Checking if vfkit is installed\nINFO Checking if CRC bundle is extracted in '$HOME/.crc'\nINFO Checking if /Users/userjoe/.crc/cache/crc_vfkit_4.15.12_amd64.crcbundle exists\nINFO Getting bundle for the CRC executable\nINFO Downloading bundle: /Users/userjoe/.crc/cache/crc_vfkit_4.15.12_amd64.crcbundle...\n4.98 GiB / 4.98 GiB [--------------------------------------------------------------------------------] 100.00% 15.22 MiB/s\nINFO Uncompressing /Users/userjoe/.crc/cache/crc_vfkit_4.15.12_amd64.crcbundle\ncrc.img:  31.00 GiB / 31.00 GiB [--------------------------------------------------------------------------------] 100.00%\noc:  118.78 MiB / 118.78 MiB [-----------------------------------------------------------------------------------] 100.00%\nINFO Checking if old launchd config for tray and/or daemon exists\nINFO Checking if crc daemon plist file is present and loaded\nINFO Adding crc daemon plist file and loading it\nINFO Checking SSH port availability\nYour system is correctly setup for using CRC. Use 'crc start' to start the instance\n</code></pre> </li> <li> <p>Start the Red Hat Openshift Local instance:</p> <pre><code>crc start\n\n# Output\nINFO Using bundle path /Users/userjoe/.crc/cache/crc_vfkit_4.15.12_amd64.crcbundle\nINFO Checking if running macOS version &gt;= 13.x\nINFO Checking if running as non-root\nINFO Checking if crc-admin-helper executable is cached\nINFO Checking if running on a supported CPU architecture\nINFO Checking if crc executable symlink exists\nINFO Checking minimum RAM requirements\nINFO Check if Podman binary exists in: /Users/userjoe/.crc/bin/oc\nINFO Checking if running emulated on Apple silicon\nINFO Checking if vfkit is installed\nINFO Checking if old launchd config for tray and/or daemon exists\nINFO Checking if crc daemon plist file is present and loaded\nINFO Checking SSH port availability\nINFO Loading bundle: crc_vfkit_4.15.12_amd64...\nINFO Creating CRC VM for OpenShift 4.15.12...\nINFO Generating new SSH key pair...\nINFO Generating new password for the kubeadmin user\nINFO Starting CRC VM for openshift 4.15.12...\nINFO CRC instance is running with IP 127.0.0.1\nINFO CRC VM is running\nINFO Updating authorized keys...\nINFO Resizing /dev/vda4 filesystem\nINFO Configuring shared directories\nINFO Check internal and public DNS query...\nINFO Check DNS query from host...\nINFO Verifying validity of the kubelet certificates...\nINFO Starting kubelet service\nINFO Waiting for kube-apiserver availability... [takes around 2min]\nINFO Adding user's pull secret to the cluster...\nINFO Updating SSH key to machine config resource...\nINFO Waiting until the user's pull secret is written to the instance disk...\nINFO Changing the password for the kubeadmin user\nINFO Updating cluster ID...\nINFO Updating root CA cert to admin-kubeconfig-client-ca configmap...\nINFO Starting openshift instance... [waiting for the cluster to stabilize]\nE0531 14:12:45.233915   69684 request.go:1116] Unexpected error when reading response body: net/http: request canceled (Client.Timeout or context cancellation while reading body)\nINFO 2 operators are progressing: console, network\nINFO All operators are available. Ensuring stability...\nINFO Operators are stable (2/3)...\nINFO Operators are stable (3/3)...\nINFO Adding crc-admin and crc-developer contexts to kubeconfig...\nStarted the OpenShift cluster.\n\nThe server is accessible via web console at:\n  https://console-openshift-console.apps-crc.testing\n\nLog in as administrator:\n  Username: kubeadmin\n  Password: &lt;password&gt;\n\nLog in as user:\n  Username: developer\n  Password: &lt;password&gt;\n\nUse the 'oc' command line interface:\n  $ eval $(crc oc-env)\n  $ oc login -u developer https://api.crc.testing:6443\n</code></pre> <p>Depending on the speed of your system, this will take 8 to 15 minutes.  There is a 10 minute timeout on checking the stability of operators deployed by Openshift.  It might be the case that the tool reports these have not reached full stability in that window.  In the writing of this guide, no issues were found using Openshift deployed in this manner, even if the error occurs.  Each time the steps in this guide were tested, everything eventually reached a healthy status, even if not in the window expected on some occasions.</p> </li> </ol> <p>Setup is complete.  This local environment should be ready to deploy our Helm examples</p>"},{"location":"deployment/deployLocalOpenshift/#stop-the-red-hat-openshift-local-instance","title":"Stop the Red Hat Openshift Local instance","text":"<p>When not working with the environment, you can stop the instance by running the following command.  All settings, projects and objects created will be retained and available when it is started again.</p> <pre><code>crc stop\n</code></pre> <p>Run <code>crc start</code> again to launch the instance.</p>"},{"location":"deployment/deployLocalOpenshift/#delete-the-red-hat-openshift-local-instance","title":"Delete the Red Hat Openshift Local instance","text":"<p>You can also remove the instance by running the following command.  If you do this, the embedded VM instance, all objects and projects created will be lost.  A new instance will be deployed the next time you run <code>crc start</code>.</p> <pre><code>crc delete\n</code></pre> <p>Remove all configuration</p> <p>Deleting the instance does not delete the configuration settings for Red Hat Openshift Local (RAM, CPU, disk, and so on, created or modified when running the <code>crc config set</code> command).  If you want to completely remove all configuration, you can delete the $HOME/.crc folder and its contents.  Also, you will need to edit <code>/etc/hosts</code> and remove the following aliases to the 127.0.0.1 IP: <code>api.crc.testing canary-openshift-ingress-canary.apps-crc.testing console-openshift-console.apps-crc.testing default-route-openshift-image-registry.apps-crc.testing downloads-openshift-console.apps-crc.testing oauth-openshift.apps-crc.testing</code></p>"},{"location":"deployment/deployPACluster/","title":"Deploy a PingAccess Cluster with PingIdentity Helm Charts Without a Server Profile","text":"<p>Demo Use Only</p> <p>The instructions in this document are for testing and learning and are not intended for use in production.</p>"},{"location":"deployment/deployPACluster/#purpose","title":"Purpose","text":"<p>Create and deploy a PingAccess Cluster using PingIdentity Helm Charts, without having to create a custom server profile. This process will allow you to quickly bring up the PingAccess UI and conduct any tests you need.</p>"},{"location":"deployment/deployPACluster/#prerequisites","title":"Prerequisites","text":"<ul> <li>kubectl</li> <li>Access to a Kubernetes cluster</li> </ul>"},{"location":"deployment/deployPACluster/#steps","title":"Steps","text":"<ol> <li> <p>Confirm that your kuberenetes context and namespace are set correctly</p> <pre><code># Display kuberenetes context\nkubectx\n\n# Display namespace\nkubens -c\n</code></pre> <p>If these values are not set or are incorrect, you can set them with the following commands. If you do not yet have a namespace, or do not have access to a kubernetes cluster, refer to Deploy Example Stack.</p> <pre><code># Display kuberenetes context\nkubectx &lt;context&gt;\n\n# Display namespace\nkubens &lt;namespace&gt;\n</code></pre> </li> <li> <p>Confirm that there are no conflicting persistent volumes. </p> <p><pre><code>#List any persistent volumes\nkubectl get pvc\n</code></pre> If you see a persistent volume with a name that resembles <code>out-dir-demo-pingaccess-admin-0</code>, then delete it before deploying you cluster. <pre><code>#Delete name_of_pvc persistent volume\nkubectl delete pvc out-dir-demo-pingaccess-admin-0\n</code></pre></p> </li> </ol> <p>Implemetned for Sprint 2211 and onwards</p> <p>This functionality has only been implemented for Sprint tags of 2211 or later. Therefore, it will not work for all earlier tags.</p> <ol> <li> <p>Create a YAML file similar to the one shown here. Make sure to replace <code>insert domain name here</code> with your domain name.</p> <pre><code>global:\nenvs:\n    PING_IDENTITY_ACCEPT_EULA: \"YES\"\ningress:\n    enabled: true\n    addReleaseNameToHost: prepend\n    defaultDomain: \"insert domain name here\"\n    defaultTlsSecret:\n    annotations:\n        nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n        kubernetes.io/ingress.class: \"nginx-public\"\n\n#############################################################\n# pingaccess-admin values\n#############################################################\npingaccess-admin:\nenabled: true\nprivateCert:\n    generate: true\nenvs: \n    PING_IDENTITY_PASSWORD: \"2FederateM0re!\"\n\n#############################################################\n# pingaccess-engine values\n#############################################################\npingaccess-engine:\nenabled: true\ncontainer:\n    replicaCount: 1\nenvs: \n    PING_IDENTITY_PASSWORD: \"2FederateM0re!\"\n</code></pre> </li> <li> <p>Create the default PingAccess cluster. Make sure that you fill in the \"PATH\" to your new values.yaml file. This deployment may take a few minutes to become healthy.</p> <pre><code>helm upgrade --install demo pingidentity/ping-devops -f &lt;path-to-yaml&gt;/values.yaml\n</code></pre> </li> <li> <p>To display the status of the deployed components, you can use k9s or issue the corresponding commands shown here:</p> <ul> <li> <p>Display the services (endpoints for connecting) by running <code>kubectl get service --selector=app.kubernetes.io/instance=demo</code></p> <pre><code>NAME                            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE\ndemo-pingaccess-admin           ClusterIP   172.20.221.233   &lt;none&gt;        9090/TCP,9000/TCP   37s\ndemo-pingaccess-admin-cluster   ClusterIP   None             &lt;none&gt;        &lt;none&gt;              37s\ndemo-pingaccess-engine          ClusterIP   172.20.126.86    &lt;none&gt;        3000/TCP            37s\n</code></pre> </li> <li> <p>To view the pods, run <code>kubectl get pods --selector=app.kubernetes.io/instance=demo</code> - you will need to run this at intervals until all pods have started (** Running ** status):</p> <pre><code>NAME                                      READY   STATUS            RESTARTS   AGE\ndemo-pingaccess-admin-0                   1/1     Running   0          28m\ndemo-pingaccess-engine-6b977b9498-298jw   1/1     Running   0          28m\n</code></pre> </li> <li> <p>To see the ingresses you will use to access the product, run <code>kubectl get ingress</code>. If the ingress controller is configured properly, the URL you will see under demo-pingaccess-admin HOST (<code>demo-pingaccess-admin.&lt;domain-name&gt;</code>) will be the URL you use to access the PingAccess management console.</p> <pre><code>NAME                     CLASS    HOSTS                                    ADDRESS                                                                         PORTS     AGE\ndemo-pingaccess-admin    &lt;none&gt;   demo-pingaccess-admin.&lt;domain-name&gt;      adab69408130011eab1cd028479a4fe3-532fea1b3272797d.elb.us-east-2.amazonaws.com   80, 443   2m1s\ndemo-pingaccess-engine   &lt;none&gt;   demo-pingaccess-engine.&lt;domain-name&gt;     adab69408130011eab1cd028479a4fe3-532fea1b3272797d.elb.us-east-2.amazonaws.com   80, 443   2m1s\n</code></pre> </li> <li> <p>To see everything tied to the helm release run <code>kubectl get all --selector=app.kubernetes.io/instance=demo</code>:</p> <pre><code>NAME                                          READY   STATUS    RESTARTS   AGE\npod/demo-pingaccess-admin-0                   1/1     Running   0          29m\npod/demo-pingaccess-engine-6b977b9498-298jw   1/1     Running   0          29m\n\nNAME                                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE\nservice/demo-pingaccess-admin           ClusterIP   172.20.221.233   &lt;none&gt;        9090/TCP,9000/TCP   29m\nservice/demo-pingaccess-admin-cluster   ClusterIP   None             &lt;none&gt;        &lt;none&gt;              29m\nservice/demo-pingaccess-engine          ClusterIP   172.20.126.86    &lt;none&gt;        3000/TCP            29m\n\nNAME                                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/demo-pingaccess-engine   1/1     1            1           29m\n\nNAME                                                DESIRED   CURRENT   READY   AGE\nreplicaset.apps/demo-pingaccess-engine-6b977b9498   1         1         1       29m\n\nNAME                                     READY   AGE\nstatefulset.apps/demo-pingaccess-admin   1/1     29m\n</code></pre> </li> <li> <p>To view logs, look at the logs for the deployment of the product in question.  For example:</p> <pre><code>#Admin pod logs\nkubectl logs demo-pingaccess-admin-0\n\n#Engine pod logs\nkubectl logs demo-pingaccess-engine-6b977b9498\n</code></pre> </li> </ul> </li> <li> <p>Below are the credentials and URL to sign on to the PingAccess management console after the cluster is up and healthy.</p> <p>Certificates</p> <p>This example uses self-signed certificates that will have to be accepted in your browser or added to your keystore.</p> <p>With the ingress in place, you can access the product at the URL seen below, using the domain-name you set in you values.yaml file.</p> Product Connection Details PingAccess <ul><li>URL: https://demo-pingaccess-admin.(domain-name)</li><li>Username: Administrator</li><li>Password: 2FederateM0re!</li></ul> </li> <li> <p>When you are finished, you can remove the demonstration components by running the uninstall command for helm:</p> <pre><code>helm uninstall demo\n</code></pre> </li> <li> <p>Finally make sure to prune the persistent volume created in the deployment of your PingAccess cluster, by running the delete pvc command for kubectl:</p> <pre><code>#Delete name_of_pvc persistent volume\nkubectl delete pvc out-dir-demo-pingaccess-admin-0\n</code></pre> </li> </ol>"},{"location":"deployment/deployPDMultiRegion/","title":"Deploy PingDirectory Across Multiple Kubernetes Clusters","text":"<p>This example shows how to deploy PingDirectory containers that replicate across multiple Kubernetes clusters.</p> <p></p>"},{"location":"deployment/deployPDMultiRegion/#overview","title":"Overview","text":"<p>Implementing a replicated PingDirectory topology across multiple Kubernetes clusters is desired for highly available active/active deployments as well as active/partial-active scenarios where a hot backup is expected.</p> <p>PingDirectory Docker images abstract much of the complexity of replication initialization scripts, even across clusters. With this simplification, the focus shifts to providing accessible DNS hostnames across clusters and environment variables to build ordinal hostnames for each PingDirectory instance.</p>"},{"location":"deployment/deployPDMultiRegion/#what-you-will-do","title":"What You Will Do","text":"<ol> <li>PingDirectory Host Naming - Set the variables needed to create your hostnames</li> <li>Cluster Startup Walkthrough - A description of what happens when a PingDirectory cluster starts</li> <li>Deploy the Helm Example - Deploy an example set of servers across multiple Kubernetes clusters</li> </ol> <p>Details within each Kubernetes cluster are hidden from outside the cluster, which means that external access to each pod in the cluster is required. The PingDirectory images will set up access to each of the pods using load balancers from an external host to allow each pod to communicate over the LDAP and replication protocols.</p>"},{"location":"deployment/deployPDMultiRegion/#pingdirectory-host-naming","title":"PingDirectory Host Naming","text":"<p>The most important aspect of a successful PingDirectory cross-cluster deployment is assigning accessible and logical DNS hostnames. The rules for this setup include:</p> <ol> <li>Each PingDirectory pod needs its own hostname available in DNS</li> <li>Hostnames need to include the ordinal representing the instance in the statefulset</li> <li>All hostnames must be accessible to all directory instances</li> </ol> <p>These rules still leave plenty of room for flexibility, particularly when accounting for the cluster-native DNS names Kubernetes creates.</p>"},{"location":"deployment/deployPDMultiRegion/#single-cluster-example-with-multiple-namespaces","title":"Single Cluster Example with Multiple Namespaces","text":"<p>For example, you can simulate a multi-cluster environment in a single Kubernetes cluster by using separate namespaces and creating a separate ClusterIP service for each directory environment. You would end up with something similar to this example:</p>"},{"location":"deployment/deployPDMultiRegion/#primary-cluster","title":"Primary Cluster","text":"Pod Service Name Namespace Hostname pingdirectory-0 pingdirectory-0 primary pingdirectory-0.primary pingdirectory-1 pingdirectory-1 primary pingdirectory-1.primary pingdirectory-2 pingdirectory-2 primary pingdirectory-2.primary"},{"location":"deployment/deployPDMultiRegion/#secondary-cluster","title":"Secondary Cluster","text":"Pod Service Name Namespace Hostname pingdirectory-0 pingdirectory-0 secondary pingdirectory-0.secondary pingdirectory-1 pingdirectory-1 secondary pingdirectory-1.secondary pingdirectory-2 pingdirectory-2 secondary pingdirectory-2.secondary"},{"location":"deployment/deployPDMultiRegion/#production-example-with-external-dns-names","title":"Production Example with External DNS Names","text":"<p>An example from a production environment with external hostnames might appear more like this example:</p>"},{"location":"deployment/deployPDMultiRegion/#us-west-cluster","title":"us-west cluster","text":"Pod Service Name DNS / Hostname pingdirectory-0 pingdirectory-0 pingdirectory-0-us-west.ping-devops.com pingdirectory-1 pingdirectory-1 pingdirectory-1-us-west.ping-devops.com pingdirectory-2 pingdirectory-2 pingdirectory-2-us-west.ping-devops.com"},{"location":"deployment/deployPDMultiRegion/#us-east-cluster","title":"us-east cluster","text":"Pod Service Name DNS / Hostname pingdirectory-0 pingdirectory-0 pingdirectory-0-us-east.ping-devops.com pingdirectory-1 pingdirectory-1 pingdirectory-1-us-east.ping-devops.com pingdirectory-2 pingdirectory-2 pingdirectory-2-us-east.ping-devops.com"},{"location":"deployment/deployPDMultiRegion/#variables-to-create-hostnames","title":"Variables to Create Hostnames","text":"<p>To provide flexibility on how each PingDirectory instance will find other instances, a full DNS hostname is broken into multiple variables.</p> Variable Description <code>K8S_POD_HOSTNAME_PREFIX</code> The string used as the prefix for all host names.  Defaults to the name of the <code>StatefulSet</code>. <code>K8S_POD_HOSTNAME_SUFFIX</code> The string used as the suffix for all pod host names.  Defaults to <code>K8S_CLUSTER</code>. <code>K8S_SEED_HOSTNAME_SUFFIX</code> The string used as the suffix for all seed host names.  Defaults to <code>K8S_SEED_CLUSTER</code> (discussed later). <p>With these variables, a full hostname is created in this manner:</p> <pre><code>${K8S_POD_HOSTNAME_PREFIX}&lt;instance-ordinal&gt;${K8S_SEED_HOSTNAME_SUFFIX}\n</code></pre>"},{"location":"deployment/deployPDMultiRegion/#previous-hostname-example-breakdown","title":"Previous Hostname Example Breakdown","text":"hostname K8S_POD_HOSTNAME_PREFIX K8S_POD_HOSTNAME_SUFFIX K8S_SEED_HOSTNAME_SUFFIX pingdirectory-0.primary <code>pingdirectory-</code> <code>.primary</code> <code>.primary</code> pingdirectory-2-us-west.ping-devops.com <code>pingdirectory-</code> <code>-us-west.ping-devops.com</code> <code>-us-west.ping-devops.com</code>"},{"location":"deployment/deployPDMultiRegion/#environment-variables","title":"Environment Variables","text":"Variable Required Description <code>K8S_CLUSTERS</code> *** The total list of Kubernetes clusters to which the StatefulSet will replicate. <code>K8S_CLUSTER</code> *** The Kubernetes cluster to which the StatefulSet will be deployed. <code>K8S_SEED_CLUSTER</code> *** The Kubernetes cluster to which the seed server is deployed. <code>K8S_NUM_REPLICAS</code> The number of replicas that make up the StatefulSet. <code>K8S_POD_HOSTNAME_PREFIX</code> The string used as the prefix for all host names.  Defaults to <code>StatefulSet</code>. <code>K8S_POD_HOSTNAME_SUFFIX</code> The string used as the suffix for all pod host names.  Defaults to <code>K8S_CLUSTER</code>. <code>K8S_SEED_HOSTNAME_SUFFIX</code> The string used as the suffix for all seed host names.  Defaults to <code>K8S_SEED_CLUSTER</code>. <code>K8S_INCREMENT_PORTS</code> <code>true</code> or <code>false</code>.  If <code>true</code>, the port for each pod will be incremented by 1. <p>An example set of the YAML configuration for these environment variables is as follows:</p>"},{"location":"deployment/deployPDMultiRegion/#primary","title":"Primary","text":"<pre><code>K8S_STATEFUL_SET_NAME=pingdirectory\nK8S_STATEFUL_SET_SERVICE_NAME=pingdirectory\n\nK8S_CLUSTERS=us-east-2 eu-west-1\nK8S_CLUSTER=us-east-2\nK8S_SEED_CLUSTER=us-east-2\nK8S_NUM_REPLICAS=3\n\nK8S_POD_HOSTNAME_PREFIX=pd-\nK8S_POD_HOSTNAME_SUFFIX=.us-cluster.ping-devops.com\nK8S_SEED_HOSTNAME_SUFFIX=.us-cluster.ping-devops.com\n\nK8S_INCREMENT_PORTS=true\nLDAPS_PORT=8600\nREPLICATION_PORT=8700\n</code></pre> <p>These environment variable settings map out like this:</p> Seed Instance Host name LDAP REPL CLUSTER: us-east-2 *** pingdirectory-0.us-east-2 pd-0.us-cluster.ping-devops.com 8600 8700 pingdirectory-1.us-east-2 pd-1.us-cluster.ping-devops.com 8601 8701 pingdirectory-2.us-east-2 pd-2.us-cluster.ping-devops.com 8602 8702"},{"location":"deployment/deployPDMultiRegion/#secondary","title":"Secondary","text":"<pre><code>K8S_STATEFUL_SET_NAME=pingdirectory\nK8S_STATEFUL_SET_SERVICE_NAME=pingdirectory\n\nK8S_CLUSTERS=us-east-2 eu-west-1\nK8S_CLUSTER=eu-west-1\nK8S_SEED_CLUSTER=us-east-2\nK8S_NUM_REPLICAS=3\n\nK8S_POD_HOSTNAME_PREFIX=pd-\nK8S_POD_HOSTNAME_SUFFIX=.eu-cluster.ping-devops.com\nK8S_SEED_HOSTNAME_SUFFIX=.us-cluster.ping-devops.com\n\nK8S_INCREMENT_PORTS=true\nLDAPS_PORT=8600\nREPLICATION_PORT=8700\n</code></pre> Seed Instance Host name LDAP REPL CLUSTER: eu-west-1 pingdirectory-0.eu-west-1 pd-0.eu-cluster.ping-devops.com 8600 8700 pingdirectory-1.eu-west-1 pd-1.eu-cluster.ping-devops.com 8601 8701 pingdirectory-2.eu-west-1 pd-2.eu-cluster.ping-devops.com 8602 8702"},{"location":"deployment/deployPDMultiRegion/#cluster-startup-walkthrough","title":"Cluster Startup Walkthrough","text":"<p>By now you can see that there are many variables that have been described. These variables exist to provide flexibility to accommodate various infrastructure constraints. For example, in some environments you cannot use the same port for each instance, so we must accommodate incrementing ports.</p> <p>Continuing, it is helpful to know what happens when a cluster starts in order to understand why the initial creation of a cluster must be very prescriptive.</p> <ol> <li> <p>The first pod must start on its own and become healthy. This startup is critical to prevent replication islands. The very first time the very first pod starts, we call it \"GENESIS\". All other pods are dependent on this <code>SEED_POD</code> in the <code>SEED_CLUSTER</code> starting correctly by itself. The entire purpose of defining <code>SEED_POD</code> and <code>SEED_CLUSTER</code> variables is avoid multiple genesis scenarios.</p> </li> <li> <p>After the first pod is healthy, it begins querying DNS for combinations of hostnames at their LDAPS port to find another PingDirectory instance.</p> </li> </ol> <p>In our first cluster, this would be the hostname of pingdirectory-1, but it could also be pingdirectory-0 of another cluster. After the query returns successful, creation of the replication topology automatically begins. From this point onward, the order in which instances start is less important.</p>"},{"location":"deployment/deployPDMultiRegion/#deploying-across-multiple-regions-with-multiple-load-balancers","title":"Deploying Across Multiple Regions with Multiple Load Balancers","text":"<p>If infrastructure constraints prevent you from using Peered Clusters, an alternate option is to deploy with a separate LoadBalancer service for each PingDirectory pod.</p> <p>The following diagram shows how you can use muliple load balancers.</p> <p></p> <p>Advantages:</p> <ul> <li>Use the same well-known port, such as 1636/8989</li> <li>Separate IP addresses per instance</li> </ul> <p>Disadvantages:</p> <ul> <li>DNS management<ul> <li>Separate hostname required per pod</li> </ul> </li> </ul> <p>This method is supported in our Helm charts with the <code>pingdirectory.services.loadBalancerServicePerPod</code> field.</p>"},{"location":"deployment/deployPDMultiRegion/#deploy-the-helm-example","title":"Deploy the Helm Example","text":"<p>Clone this <code>getting-started</code> repository to get the Helm values .yaml files for the exercise. There are two multi-region examples. For peered clusters, the example files are under the folder <code>30-helm/multi-region/pingdirectory</code>. For deploying with multiple load balancers, the example files are under the folder <code>30-helm/multi-region/pingdirectory-loadbalancer-per-pod</code>. After cloning:</p> <ol> <li> <p>Modify any external hostnames in the sample files as necessary - see the lines under <code>## CHANGEME</code> comments.</p> </li> <li> <p>Deploy the first set of pods (the example here uses kubectx to set the kubectl context).</p> <pre><code>kubectx west\nhelm upgrade --install example pingidentity/ping-devops -f 01-west.yaml\n</code></pre> </li> <li> <p>Wait for the example-pingdirectory pods to be running and ready.</p> </li> <li> <p>Deploy the second set of pods.</p> <pre><code>kubectx east\nhelm upgrade --install example pingidentity/ping-devops -f 02-east.yaml\n</code></pre> </li> <li> <p>Wait for all example-pingdirectory pods to be running and ready.</p> </li> <li> <p>Verify that pods are replicating.</p> <pre><code>kubectx west\nkubectl exec example-pingdirectory-0 -- dsreplication status --showAll\n</code></pre> </li> </ol>"},{"location":"deployment/deployPDMultiRegion/#cleanup","title":"Cleanup","text":"<pre><code>kubectx west\nhelm uninstall example\nkubectl delete pvc --selector=app.kubernetes.io/instance=example\nkubectx east\nhelm uninstall example\nkubectl delete pvc --selector=app.kubernetes.io/instance=example\n</code></pre>"},{"location":"deployment/deployPDProxyBackendDiscovery/","title":"Deploy PingDirectoryProxy and PingDirectory with automatic backend discovery","text":"<p>Since version 8.3 of PingDirectoryProxy, proxy servers can use automatic server discovery to determine the backend PingDirectory servers, rather than adding those servers individually to the configuration. This page describes how to use this feature with the PingDirectory and PingDirectoryProxy Docker images and the ping-devops Helm chart</p> <p>The directory and proxy Docker images added support for this feature as of the 2310 release, and the ping-devops Helm chart added support in release <code>0.9.20</code>.</p>"},{"location":"deployment/deployPDProxyBackendDiscovery/#configuring-the-proxy-instance-to-join-the-directory-topology","title":"Configuring the proxy instance to join the directory topology","text":"<p>The first step of enabling automatic server discovery is to have the proxy server(s) join the topology of replicating directory servers. To enable this, the proxy Docker image supports the following variables:</p> <ul> <li><code>JOIN_PD_TOPOLOGY</code>: Set to <code>true</code> to add the proxy instance to a directory topology</li> <li><code>PINGDIRECTORY_HOSTNAME</code>: The hostname of the directory server to connect with when joining the topology</li> <li><code>PINGDIRECTORY_LDAPS_PORT</code>: The LDAPS port of the directory server to connect with when joining the topology</li> </ul> <p>If all three of these variables are set, the proxy server will join the designated topology after the server starts up.</p>"},{"location":"deployment/deployPDProxyBackendDiscovery/#waiting-for-the-directory-topology-to-be-ready-before-starting","title":"Waiting for the directory topology to be ready before starting","text":"<p>The designated directory server must be running for the proxy server to join the topology. To ensure directory is running before proxy attempts to join, a <code>wait-for</code> can be used.</p> <p>For example, using the ping-devops Helm chart, the following values yaml instructs proxy to wait until the second <code>pingdirectory</code> pod is running before starting and attempting to join the topology. \"releasename\" can be replaced with the Helm release name.</p> <pre><code>initContainers:\n  wait-for-pd:\n    name: wait-for-pd\n    image: pingidentity/pingtoolkit:2309\n    command: ['sh', '-c', 'echo \"Waiting for PingDirectory...\" &amp;&amp; wait-for releasename-pingdirectory-1.releasename-pingdirectory-cluster:1636 -t 300 -- echo \"PingDirectory running\"']\n\npingdirectory:\n  container:\n    replicaCount: 2\n  enabled: true\n  envs:\n    SERVER_PROFILE_URL: https://github.com/pingidentity/pingidentity-server-profiles.git\n    SERVER_PROFILE_PATH: baseline/pingdirectory\n    LOAD_BALANCING_ALGORITHM_NAMES:dc_example_dc_com-fewest-operations;dc_example_dc_com-failover\n\npingdirectoryproxy:\n  includeInitContainers:\n  - wait-for-pd\n  container:\n    replicaCount: 1\n  enabled: true\n  envs:\n    SERVER_PROFILE_URL: https://github.com/pingidentity/pingidentity-server-profiles.git\n    SERVER_PROFILE_PATH: pingdirectoryproxy-automatic-server-discovery\n    JOIN_PD_TOPOLOGY: \"true\"\n    PINGDIRECTORY_HOSTNAME: releasename-pingdirectory-0.releasename-pingdirectory-cluster\n    PINGDIRECTORY_LDAPS_PORT: \"1636\"\n</code></pre>"},{"location":"deployment/deployPDProxyBackendDiscovery/#configuring-automatic-server-discovery-on-proxy-using-a-server-profile","title":"Configuring automatic server discovery on proxy using a server profile","text":"<p>The proxy server must also be configured via <code>dsconfig</code> to enable automatic server discovery. For an example, see the automatic server discovery server profile.</p>"},{"location":"deployment/deployPDProxyBackendDiscovery/#setting-load-balancing-algorithm-names-on-the-directory-instances","title":"Setting load balancing algorithm names on the directory instances","text":"<p>To associate directory servers with the load balancing algorithms configured on the proxy server, the <code>load-balancing-algorithm-name</code> property must be set. This can be done with the <code>LOAD_BALANCING_ALGORITHM_NAMES</code> environment variable in the directory Docker image. When using multiple algorithm names, separate them with a <code>;</code>. See the above yaml snippet for an example.</p>"},{"location":"deployment/deployPDProxyBackendDiscovery/#removing-the-proxy-server-from-the-topology-on-pod-shutdown","title":"Removing the proxy server from the topology on pod shutdown","text":"<p>By default the proxy server will rejoin the topology automatically on restarts. In the <code>ping-devops</code> Helm chart, proxy does not use a persistent volume, so it will fully restart and rejoin the topology during each startup.</p> <p>Another option, which allows for scaling down the number of proxy servers, is adding a <code>preStop</code> hook to remove the proxy server from the topology. In general this can cause slowness because it will run whenever a pod stops, but it ensures that scaling down the number of proxies does not leave outdated servers in the topology registry. For example:</p> <pre><code>pingdirectoryproxy:\n  container:\n    # Add the preStop hook to run the remove-defunct-server tool\n    lifecycle:\n      preStop:\n        exec:\n          command:\n          - /opt/staging/hooks/90-shutdown-sequence.sh\n</code></pre>"},{"location":"deployment/deployPDProxyBackendDiscovery/#automatic-server-discovery-when-directory-and-proxy-pods-are-split-across-multiple-clusters","title":"Automatic server discovery when directory and proxy pods are split across multiple clusters","text":"<p>When deploying directory pods across multiple Kubernetes clusters, some additional configuration needs to be added to allow proxy to join the directory topology and enable automatic server discovery.</p> <p>Essentially, the proxy workload will need to have similar variables and network access as the directory workload (see the directory multi-cluster doc linked above). In addition, proxy will need the right variables set to join the topology and the right wait-for logic to wait for the other servers to be ready before starting and joining the topology.</p> <p>See here for a complete Helm example.</p>"},{"location":"deployment/deployPFMultiRegion/","title":"Deploy PingFederate Across Multiple Kubernetes Clusters","text":"<p>This section will discuss deploying a single PingFederate cluster that spans across multiple Kubernetes clusters.</p> <p>Deploying PingFederate in multiple regions should not imply that spanning a single PingFederate cluster across multiple Kubernetes clusters is necessary or optimal.  This scenario makes sense when you have:</p> <ul> <li>Traffic that can cross between regions at any time. For example, west and east and users may be routed to either location.</li> <li>Configuration that needs to be the same in multiple regions and there is no reliable automation to ensure this is the case</li> </ul> <p>If all configuration changes are delivered via a pipeline, and traffic will not cross regions, having separate PingFederate clusters can work.</p> <p>Note</p> <p>The set of pre-requisites required for AWS Kubernetes multi-clustering to be successful is found here.</p> <p>Static engine lists, which may be used to extend traditional, on-premise PingFederate clusters is out of scope in this document.</p>"},{"location":"deployment/deployPFMultiRegion/#prerequisites","title":"Prerequisites","text":"<ul> <li>Two Kubernetes clusters created with the following requirements:<ul> <li>VPC IPs selected from RFC1918 CIDR blocks</li> <li>The two cluster VPCs peered together</li> <li>All appropriate routing tables modified in both clusters to send cross cluster traffic to the VPC peer connection</li> <li>Security groups on both clusters to allow traffic for ports 7600 and 7700 in both directions</li> <li>Verification that a pod in one cluster can connect to a pod in the second cluster on ports 7600 and 7700 (directly to the back-end IP assigned to the pod, not through an exposed service)</li> <li>externalDNS enabled <p>See example \"AWS configuration\" instructions here</p> </li> </ul> </li> <li>Helm client installed</li> </ul>"},{"location":"deployment/deployPFMultiRegion/#overview","title":"Overview","text":"<p>The PingFederate Docker image default <code>instance/server/default/conf/tcp.xml</code> file points to DNS_PING. After you have two peered Kubernetes clusters, spanning a PingFederate cluster across the two becomes easy. A single PingFederate cluster uses DNS_PING to query a local headless service. In this example we use externalDNS to give an externalName to the headless service. The <code>externalDNS</code> feature from the Kubernetes special interest group (SIG) creates a corresponding record on AWS Route53 and constantly updates it with container IP addresses of the backend PF engines.</p> <p>External DNS</p> <p>If you are unable to use externalDNS, another way to expose the headless service across clusters is needed. HAProxy may be a viable option to explore and is beyond the scope of this document.</p>"},{"location":"deployment/deployPFMultiRegion/#what-you-will-do","title":"What You Will Do","text":"<ul> <li>Clone the example files from the <code>getting-started</code> Repository</li> <li>Edit the externalName of the pingfederate-cluster service and the DNS_QUERY_LOCATION variable as needed <p>Search the files for <code># CHANGEME</code> comments to find where these changes need to be made.</p> </li> <li>Deploy the clusters</li> <li>Cleanup</li> </ul>"},{"location":"deployment/deployPFMultiRegion/#example-deployment","title":"Example deployment","text":"<p>Clone this <code>getting-started</code> repository to get the Helm values yaml for the exercise. The files are located under the folder <code>30-helm/multi-region/pingfederate</code>.  </p> <p>After cloning:</p> <ol> <li> <p>Update the first uncommented line under any <code>## CHANGEME</code> comment in the files. The changes will indicate the Kubernetes namespace and the externalName of the pingfederate-cluster service.</p> </li> <li> <p>Deploy the first cluster (the example here uses kubectx to set the kubectl context))</p> <pre><code>kubectx west\nhelm upgrade --install example pingidentity/ping-devops -f base.yaml -f 01-layer-west.yaml\n</code></pre> </li> <li> <p>Deploy the second cluster</p> <pre><code>kubectx east\nhelm upgrade --install example pingidentity/ping-devops -f base.yaml -f 01-layer-east.yaml\n</code></pre> </li> <li> <p>Switch back to the first cluster, and simulate a regional failure by removing the PingFederate cluster entirely:</p> <pre><code>kubectx west\nhelm uninstall example\n</code></pre> </li> <li> <p>Switch back to the second cluster and switch failover to active</p> <pre><code>kubectx east\nhelm upgrade --install example pingidentity/ping-devops -f base.yaml -f 02-layer-east.yaml\n</code></pre> </li> </ol>"},{"location":"deployment/deployPFMultiRegion/#cleanup","title":"Cleanup","text":"<pre><code>kubectx east\nhelm uninstall example\nkubectx west\nhelm uninstall example\n</code></pre>"},{"location":"deployment/deploymentPatterns/","title":"Operating Patterns","text":"<p>This page discusses how to have a successful first day and beyond.</p> <p>After you are comfortable with the deployment examples in the getting-started repository, you can shift your focus to managing ongoing operations of the products that are relevant to you. Since it is not feasible to cover every operating scenario, this section will focus on guidance to identify an operating pattern suitable for your organization.</p> <p>The PingFederate application is used as an example of performing this assessment with some example patterns.</p>"},{"location":"deployment/deploymentPatterns/#pingfederate-configuration-management","title":"PingFederate Configuration Management","text":"<p>PingFederate has a variety of operating patterns. These patterns typically involve a trade-off between ease of implementation and mitigation of deployment risks.</p> <p>To simplify the moving parts, PingFederate configuration can be categorized into three patterns:</p>"},{"location":"deployment/deploymentPatterns/#1-infrastructure-configuration","title":"1) Infrastructure Configuration","text":""},{"location":"deployment/deploymentPatterns/#examples-of-managed-components","title":"Examples of managed components:","text":"<ul> <li>Resource allocation (CPU/Memory/Storage)</li> <li>Client Ingress (Access and Hostnames)</li> <li>Image Version</li> <li>Exposed Ports</li> <li>Environment Variable Definitions</li> <li>Secrets Definitions</li> </ul>"},{"location":"deployment/deploymentPatterns/#orchestration","title":"Orchestration","text":"<ul> <li>These items are defined in the release values.yaml file and any changes here triggers an update.</li> </ul>"},{"location":"deployment/deploymentPatterns/#2-server-configuration","title":"2) Server Configuration","text":"<p>This pattern can be oversimplified to everything outside of the <code>/instance/server/default/data</code> folder or <code>/instance/bulk-config/data.json</code>.</p>"},{"location":"deployment/deploymentPatterns/#examples-of-managed-components_1","title":"Examples of managed components:","text":"<ul> <li><code>*.properties</code> files</li> <li>Integration Kits</li> <li>HTML templates</li> <li>log formatting (log4j2.xml)</li> </ul>"},{"location":"deployment/deploymentPatterns/#orchestration_1","title":"Orchestration","text":"<p>These items are stored in the Server Profile and any change should trigger an update. It is up to the implementer to ensure that happens. Triggering an update can be done by adding a non-functional variable in <code>values.yaml</code> to track the current profile \"version\". Example: <code>SERVER_PROFILE_VERSION: v1.1</code></p>"},{"location":"deployment/deploymentPatterns/#3-application-configuration-app-config","title":"3) Application Configuration (App Config)","text":"<p>This pattern can be oversimplified to the <code>/instance/server/default/data</code> folder or <code>/instance/bulk-config/data.json</code>.</p>"},{"location":"deployment/deploymentPatterns/#managed-components","title":"Managed components","text":"<p>This category is the core PingFederate configuration. This pattern incorporates changes that are typically made through the UI or Admin APIs.</p>"},{"location":"deployment/deploymentPatterns/#orchestration_2","title":"Orchestration","text":"<p>Depending on your operating pattern, changes here may be delivered through a rolling update or by configuration replication.</p>"},{"location":"deployment/deploymentPatterns/#pingfederate-data-mount","title":"PingFederate Data Mount","text":"<p>In the most common pattern, a user would attach a persistent volume (PV) to <code>/opt/out/instance/server/default/data</code> only on the PingFederate Admin Console.</p> <p>This model is intended to be used when PingFederate Administrators need to deliver configuration through the UI in each environment, including production. Another reason for this use case may be if SP connections are allowed to be created by app developers using the Admin API. In both of these scenarios, the defining factor is that there are mutations in the production Admin console that are not being tracked in any other way, such as through source control, and therefore must be persisted.</p> <p>Attributes of this pattern:</p> <ul> <li>App Config is persisted in each SDLC environment (e.g. Dev, QA, Prod)</li> <li>App Config promotion is done manually or via the Admin API</li> <li>App Config is replicated from Admin Console to Engines</li> <li>Server Config is maintained and delivered via the server profile</li> <li>Server profile does not include App Config</li> <li>Server profile must not have <code>instance/bulk-config/data.json</code> or <code>/instance/server/default/data</code></li> <li>Backups are taken regularly to provide recovery in case of PV loss or corruption</li> </ul>"},{"location":"deployment/deploymentPatterns/#data-mount-helm-example","title":"Data Mount Helm Example","text":"<p>Helm values relevant to this configuration may look like:</p> <pre><code>pingfederate-admin:\n  enabled: true\n  container:\n    replicaCount: 1\n  envs:\n    SERVER_PROFILE_URL: &lt;insert your server profile URL here&gt;\n    SERVER_PROFILE_PATH: &lt;insert your server profile path here&gt;\n    SERVER_PROFILE_VERSION: &lt;server profile version&gt;\n  workload:\n    type: StatefulSet\n    statefulSet:\n      persistentvolume:\n        enabled: true\n        volumes:\n          out-dir:\n            ## NOTE THIS PVC DEFINITION ##\n            mountPath: /opt/out/instance/server/default/data\n            persistentVolumeClaim:\n              accessModes:\n              - ReadWriteOnce\n              storageClassName:\n              resources:\n                requests:\n                  storage: 8Gi\n\npingfederate-engine:\n  enabled: true\n  envs:\n    SERVER_PROFILE_URL: &lt;insert your server profile URL here&gt;\n    SERVER_PROFILE_PATH: &lt;insert your server profile path here&gt;\n    SERVER_PROFILE_VERSION: &lt;server profile version&gt;\n  container:\n    replicaCount: 3\n  workload:\n    type: Deployment\n    deployment:\n      strategy:\n        type: RollingUpdate\n        rollingUpdate:\n          maxSurge: 1\n          maxUnavailable: 0\n</code></pre> <p>The key aspect here is <code>pingfederate-admin.workload.statefulset.persistentvolume.volumes.out-dir.mountPath=/opt/out/instance/server/default/data</code>. This location is where all UI configuration (App Config) is stored as files. As this location is the <code>mountPath</code>, PingFederate administrators have the freedom to deliver any files not used in <code>/opt/out/instance/server/default/data</code> via a Server Profile.</p> <p>For example, adding a new IDP adapter requires a restart of the service in order for the adapter to be identified and available to App Config. The steps in this case would be:</p> <ol> <li>Add the adapter at <code>&lt;server profile URL&gt;/&lt;server profile path&gt;/pingfederate/instance/server/default/deploy/idp-adapter-name-1.jar</code></li> <li>Update <code>SERVER_PROFILE_VERSION: &lt;current version&gt;</code> -&gt; <code>SERVER_PROFILE_VERSION: &lt;new version&gt;</code> on both the admin and engine deployments (for example, v1.1 -&gt; v1.2)</li> <li>Run <code>helm upgrade --install myping pingidentity/ping-devops -f /path/to/values.yaml</code></li> </ol> <p>If the release already exists, the variable change signifies that the definition has mutated, and therefore must be redeployed. The admin pod will be deleted and recreated while the engines will surge and roll one by one.</p> <p>Reference links:</p> <ul> <li>K8s - Performing a Rolling Update</li> <li>K8s - Update a deployment</li> </ul>"},{"location":"deployment/deploymentPatterns/#data-mount-pros-and-cons","title":"Data Mount Pros and Cons","text":""},{"location":"deployment/deploymentPatterns/#values-with-this-approach","title":"Values with this approach","text":"<ul> <li>Managing App Config is more familiar to PingFederate administrators with traditional experience</li> <li>Fewer parts to consider when building a CI/CD pipeline because there is no configuration export and templating needed</li> <li>Ability to have configurations different in each environment</li> </ul>"},{"location":"deployment/deploymentPatterns/#cautions-with-this-approach","title":"Cautions with this approach","text":"<ul> <li>There is more room for user configuration error and possible outages because configurations are not promoted with automated testing</li> </ul>"},{"location":"deployment/introduction/","title":"Deployment Examples","text":"<p>This section assumes you have already deployed the full-stack server profile in Get Started.</p> <p>In this section, you will find examples for using Docker Compose for running standalone product containers and Helm/Kubernetes to deploy Ping products in typical combinations. </p>"},{"location":"deployment/k8sClusterSizing/","title":"Sizing Kubernetes clusters","text":"<p>When creating your Kubernetes cluster, size the nodes appropriately.</p>"},{"location":"deployment/k8sClusterSizing/#kubernetes-cluster-capacity","title":"Kubernetes cluster capacity","text":"<p>When determining the capacity of your cluster, there are many ways to approach sizing the nodes. For example, if you calculated a cluster sizing of 16 CPU and 64GB RAM, you could break down the node sizing into these options:</p> <ol> <li>2 nodes: 8 CPU / 32 GB RAM</li> <li>4 nodes: 4 CPU / 16 GB RAM</li> <li>8 nodes: 2 CPU / 8 GB RAM</li> <li>16 nodes: 1 CPU / 4 GB RAM</li> </ol> <p>To understand which sizing option to select, consider the associated pros and cons.</p> <p>Instance pricing</p> <p>You can generally assume that instance cost per CPU/RAM is linear among the major cloud platforms.</p>"},{"location":"deployment/k8sClusterSizing/#option-1-fewer-larger-nodes","title":"Option 1: fewer, larger nodes","text":""},{"location":"deployment/k8sClusterSizing/#pros","title":"Pros","text":"<ul> <li>If you have applications that are CPU or RAM intensive, having larger nodes can ensure your application has sufficient resources.</li> </ul>"},{"location":"deployment/k8sClusterSizing/#cons","title":"Cons","text":"<ul> <li>High availability is difficult to achieve with a minimal set of nodes. If your application has 50 pods across two nodes (25 pods per node) and a node goes down, you lose 50% of your service.</li> <li>Scaling: When autoscaling the cluster, the increment size becomes larger which could result in provisioning more hardware than needed.</li> </ul>"},{"location":"deployment/k8sClusterSizing/#option-2-more-smaller-nodes","title":"Option 2: more, smaller nodes","text":""},{"location":"deployment/k8sClusterSizing/#pros_1","title":"Pros","text":"<ul> <li>High availability is easier to maintain. If you have 50 instances with two pods per node (25 nodes) and one node goes down, you only reduce your service capacity by 4%.</li> </ul>"},{"location":"deployment/k8sClusterSizing/#cons_1","title":"Cons","text":"<ul> <li>More system overhead to manage all of the nodes.</li> <li>Possible under-utilization as the nodes might be too small to add additional services.</li> </ul>"},{"location":"deployment/k8sClusterSizing/#guidance","title":"Guidance","text":"<p>For production deployments where high availability is paramount, creating a cluster with more nodes running fewer pods per node is preferable to ensure the health of your deployed service.</p> <p>For some applications, you can decide to size one pod per node.</p> <p>To determine the physical instance type needed, multiply the desired resources for each service by the number of pods per node, plus additional capacity for system overhead. Follow product guidelines to determine system requirements.</p>"},{"location":"deployment/k8sClusterSizing/#example-service-using-3-pods-per-node","title":"Example service using 3 pods per node","text":"<ul> <li>Each pod is typically deployed with 2 CPU and 4GB RAM which when multiplied by 3 yields:<ul> <li>Minimum node requirement: 6 CPU 12 GB RAM</li> </ul> </li> <li>Add 10% for system overhead</li> </ul> <p>For these requirements in Amazon Web Services (AWS), a <code>c5.2xlarge</code> type (8 CPU / 16 GB RAM) might be the instance type selected.</p> <p>To determine the base number of nodes required, divide the number of pods by 3 to determine your minimum cluster size.  Further, you must ensure that you add definitions for cluster horizontal auto-scaling so the cluster scales in or out as needed.</p>"},{"location":"deployment/pingDataEnvironmentConsiderations/","title":"Environment considerations","text":""},{"location":"deployment/pingDataEnvironmentConsiderations/#network-file-system-nfs-constraints","title":"Network File System (NFS) constraints","text":"<p>All PingData products use the <code>manage-extension</code> tool for installing extensions. Due to how the tool operates, it can lead to issues when the deployment involves NFS.</p> <p>If your deployment uses NFS, rather than using the <code>manage-extensions</code> tool, unzip the extension manually and add it to the appropriate directory.</p> <p>The following example script, called <code>181-install-extensions.sh.post</code>, loops through the extensions to unzip and then removes them from the server profile.</p> <pre><code>#!/usr/bin/env sh\n# Loop through extensions to unzip, then remove them from the server profile\nPROFILE_EXTENSIONS_DIR=\"${PD_PROFILE}/server-sdk-extensions\"\nif test -d \"${PROFILE_EXTENSIONS_DIR}\"; then\n  find \"${PROFILE_EXTENSIONS_DIR}\" -type f -name '*.zip' -print &gt; /tmp/_extensionList\n  while IFS= read -r _extensionFile; do\n      echo \"Installing extension: ${_extensionFile}\"\n      unzip -q \"${_extensionFile}\" -d /opt/out/instance/extensions/\n      rm \"${_extensionFile}\"\n  done &lt; /tmp/_extensionList\n  rm -f /tmp/_extensionList\nfi\n</code></pre>"},{"location":"deployment/pingDataEnvironmentConsiderations/#pingdirectory-inotify-watch-limit-requirement","title":"PingDirectory inotify watch limit requirement","text":"<p>When using inotify with PingDirectory, you must set a watch limit on the host system. This value cannot be set from a docker container, and the value read within a docker container is always the host value.</p> <p>For more information, see Set file system event monitoring (inotify) in the PingDirectory documentation.</p>"},{"location":"deployment/restorePDMultiRegionSeedFailure/","title":"Restoring a Multi Region PingDirectory Deployment After Seed Cluster Failure","text":"<p>The PingDirectory hook scripts rely on the seed server of the seed cluster being available when running in a multi-region environment. This dependency leads to the question of what can be done if the seed region's servers, along with their persistent volumes, are lost. This page describes manual steps that can be taken to restore the PingDirectory topology in this case.</p> <p>Note: these steps are only needed if the seed region is lost including its persistent volumes.</p> <p>In this document, the non-seed region is referred to as the \"surviving\" region.</p>"},{"location":"deployment/restorePDMultiRegionSeedFailure/#starting-point","title":"Starting point","text":"<p>Assume the seed region and all of its persistent volumes have been lost. The surviving region will still replicate among itself, but it will not be able to reach the seed region servers even after they are restarted, due to changes to the server certificates caused by the restart.</p> <p>Running the <code>status</code> command on one of the surviving pods, you will see in the output that there are configured servers in the topology that are not reachable:</p> <pre><code>          --- Mirrored Subtrees ---\nBase DN               : Write Master                   : Configured      : Outbound        : Inbound         : Failed Write\n                      :                                : Peers           : Connections     : Connections     : Operations\n----------------------:--------------------------------:-----------------:-----------------:-----------------:----------------\ncn=Cluster,cn=config  : N/A (single instance topology) : 0               : 0               : 0               : 0\ncn=Topology,cn=config : No Master (data read-only)     : 3               : 1               : 1               : 0\n</code></pre> <p>You will also see administrative alerts in the output indicating that the mirrored subtree manager cannot establish a connection with the servers in the seed region.</p>"},{"location":"deployment/restorePDMultiRegionSeedFailure/#overview","title":"Overview","text":"<p>The key steps to restore the topology in this case are:</p> <ol> <li>Force a server in the surviving region to act as master of the topology</li> <li>Remove the unreachable servers from the topology</li> <li>Undo forcing a server to act as master of the topology</li> <li>Ensure any seed region pods are in single-server topologies</li> <li>Use <code>dsreplication enable</code> to add the servers from the refreshed seed region to the topology of the surviving region</li> <li>Use <code>dsreplication initialize-all</code> from a server of the surviving region to update the data across the regions</li> </ol>"},{"location":"deployment/restorePDMultiRegionSeedFailure/#force-a-server-to-act-as-master-of-the-topology","title":"Force a server to act as master of the topology","text":"<p>The PingDirectory topology will see that it cannot connect with half the servers and will switch to read-only mode. To allow the changes we need to make to the topology to fix this, exec into one of the pods in the surviving region.</p> <pre><code>kubectl exec -ti example-pingdirectory-0 sh\n</code></pre> <p>Run the following command to force this pod as master:</p> <pre><code>dsconfig set-global-configuration-prop --set force-as-master-for-mirrored-data:true --no-prompt\n</code></pre>"},{"location":"deployment/restorePDMultiRegionSeedFailure/#remove-the-unreachable-servers-from-the-topology","title":"Remove the unreachable servers from the topology","text":"<p>Now we must tell the surviving pods that the original seed region pods no longer exist, and that they must be removed from the topology. These commands may take a long time to run, as the <code>remove-defunct-server</code> tool will keep trying to connect for up to ten minutes depending on the state of the seed region.</p> <pre><code>remove-defunct-server --ignoreOnline --serverInstanceName example-pingdirectory-1.west --bindDN [bind dn] --bindPassword [bind password]\n</code></pre> <p>In the above command, replace the <code>--serverInstanceName</code> argument with the instance name of one of the seed region pods. Repeat the command for each seed region pod's instance name.</p> <p>This step may differ depending on the state of the seed region. If the seed region is wiped out and is still not available, then you may be prompted during the <code>remove-defunct-server</code> process whether you want to retry connecting to a server that was from the failed seed region. Enter \"no\" and continue if prompted.</p> <p>If the seed region has been restored and the servers are up by the time you are running this command, then you will likely see the ten minute timeout described above. This situation occurs because the servers are available on the same hostnames as before, but their inter-server certificates have changed during the restart. The new certificates mean SSL connections will not be possible, leading to the connection timeout.</p>"},{"location":"deployment/restorePDMultiRegionSeedFailure/#undo-forcing-a-server-to-act-as-master-of-the-topology","title":"Undo forcing a server to act as master of the topology","text":"<p>At this point the pods in the surviving region should now be the only pods in that region's topology - they should no longer be attempting to contact any pod from the failed seed region.</p> <pre><code>          --- Mirrored Subtrees ---\nBase DN               : Write Master                   : Configured      : Outbound        : Inbound         : Failed Write\n                      :                                : Peers           : Connections     : Connections     : Operations\n----------------------:--------------------------------:-----------------:-----------------:-----------------:----------------\ncn=Cluster,cn=config  : N/A (single instance topology) : 0               : 0               : 0               : 0\ncn=Topology,cn=config : example-pingdirectory-0.east   : 1               : 1               : 1               : 0\n</code></pre> <p>Exec into the pod that was forced as master in the first step. Run this command to undo the previous change:</p> <pre><code>dsconfig set-global-configuration-prop --set force-as-master-for-mirrored-data:false --no-prompt\n</code></pre>"},{"location":"deployment/restorePDMultiRegionSeedFailure/#remove-the-seed-servers-from-their-own-topology-after-restart-if-necessary","title":"Remove the seed servers from their own topology after restart if necessary","text":"<p>If the seed region was completely wiped out and unavailable during the earlier <code>remove-defunct-server</code> step, this step will be necessary. When the seed region comes up again, it will join its servers together in a new topology containing only the seed pods, as it is unaware of the other region.</p> <p>It is not possible to merge two existing topologies containing more than one server each. We need to split up the restarted seed region pods into individual single-server topologies so that we can add them to the topology of the surviving region.</p> <p>Exec into one of the seed region pods:</p> <pre><code>kubectl exec -ti example-pingdirectory-0 sh\n</code></pre> <p>Use <code>remove-defunct-server</code> to split up each server, starting with the highest pod ordinal and working down until ordinal <code>1</code>. After this is done, all seed region pods will be in separate single-server topologies, and we can then add them to the existing topology of the surviving region.</p> <pre><code>remove-defunct-server --ignoreOnline --serverInstanceName example-pingdirectory-1.west --bindDN [bind dn] --bindPassword [bind password]\n</code></pre>"},{"location":"deployment/restorePDMultiRegionSeedFailure/#add-the-servers-from-the-refreshed-seed-region-to-the-topology-of-the-surviving-region","title":"Add the servers from the refreshed seed region to the topology of the surviving region","text":"<p>At this point the servers in the seed region should be in their own single-server topologies, and the servers in the surviving region should be in a topology containing only the pods in that region.</p> <p>Now we can re-enable replication between the regions. Run the following command once for each pod in the seed region, updating the <code>--host1</code> or <code>--host2</code> argument each time to point to the server being enabled in that run. The command can be run from a shell on any pod.</p> <pre><code>dsreplication enable \\\n    --trustAll \\\n    --host1 example-pingdirectory-0.example.east.example.com \\\n    --port1 \"${LDAPS_PORT}\" \\\n    --useSSL1 \\\n    --replicationPort1 \"${REPLICATION_PORT}\" \\\n    --bindDN1 \"${ROOT_USER_DN}\" \\\n    --bindPasswordFile1 \"${ROOT_USER_PASSWORD_FILE}\" \\\n    --host2 example-pingdirectory-0.example.west.example.com \\\n    --port2 \"${LDAPS_PORT}\" \\\n    --useSSL2 \\\n    --replicationPort2 \"${REPLICATION_PORT}\" \\\n    --bindDN2 \"${ROOT_USER_DN}\" \\\n    --bindPasswordFile2 \"${ROOT_USER_PASSWORD_FILE}\" \\\n    --adminUID \"${ADMIN_USER_NAME}\" \\\n    --adminPasswordFile \"${ADMIN_USER_PASSWORD_FILE}\" \\\n    --no-prompt --ignoreWarnings \\\n    --baseDN dc=example,dc=com \\\n    --noSchemaReplication\n</code></pre>"},{"location":"deployment/restorePDMultiRegionSeedFailure/#run-dsreplication-initialize-all-from-a-server-of-the-surviving-region","title":"Run dsreplication initialize-all from a server of the surviving region","text":"<p>All of the pods are again in a topology together. Now we need to initialize the seed region with the data from the surviving region. Run the following command, targeting a server in the surviving region with the <code>--hostname</code> argument (this indicates which server the data is coming from, so we want to use a server in the surviving region):</p> <pre><code>dsreplication initialize-all \\\n    --hostname example-pingdirectory-0.example.east.example.com \\\n    --port 7700 --useSSL \\\n    --baseDN dc=example,dc=com --adminUID admin \\\n    --adminPasswordFile /tmp/pw --no-prompt\n</code></pre> <p>Now we can see from <code>dsreplication status --showAll</code> that all the pods are replicating and have matching generation IDs:</p> <pre><code>          --- Replication Status for dc=example,dc=com: Enabled ---\nServer                                                                               : Location : Entries : Conflict Entries : Backlog (1) : Rate (2) : A.O.B.C. (3) : Generation ID : Server ID : Replica ID\n-------------------------------------------------------------------------------------:----------:---------:------------------:-------------:----------:--------------:---------------:-----------:-----------\nexample-pingdirectory-0.east (example-pingdirectory-0.example.east.example.com:7700) : east     : 2038    : 0                : 0           : 0        : 0 seconds    : 4105471824    : 19064     : 32073\nexample-pingdirectory-1.east (example-pingdirectory-1.example.east.example.com:7700) : east     : 2038    : 0                : 0           : 0        : 0 seconds    : 4105471824    : 4444      : 18281\nexample-pingdirectory-0.west (example-pingdirectory-0.example.west.example.com:7700) : west     : 2038    : 0                : 0           : 0        : 0 seconds    : 4105471824    : 28554     : 13185\nexample-pingdirectory-1.west (example-pingdirectory-1.example.west.example.com:7700) : west     : 2038    : 0                : 0           : 0        : 0 seconds    : 4105471824    : 2590      : 4761\n</code></pre> <p>And from <code>status</code> we can see all the inbound and outbound connections are functioning as expected:</p> <pre><code>          --- Mirrored Subtrees ---\nBase DN               : Write Master                   : Configured      : Outbound        : Inbound         : Failed Write\n                      :                                : Peers           : Connections     : Connections     : Operations\n----------------------:--------------------------------:-----------------:-----------------:-----------------:----------------\ncn=Cluster,cn=config  : N/A (single instance topology) : 0               : 0               : 0               : 0\ncn=Topology,cn=config : example-pingdirectory-0.east   : 3               : 3               : 3               : 0\n</code></pre>"},{"location":"docker-builds/","title":"Ping docker-builds Repository Summary","text":"<p>Using the repository</p> <p>This page only describes the repository structure for context and understanding. For instructions on using the repository to build an image, visit this page.</p> <p>In the docker-builds Github repository, we share our Dockerfiles and automation scripts used for building Ping product container images.  You can use these scripts and supporting pieces found in the repository to build an image of your own locally.  For example, you can use this process to build an image for an older version we no longer provide on Docker Hub.</p> <p>The image build process at Ping uses the Docker multi-stage build process.  In summary, we build out image layers that are common to most or all of our products, and from those we add layers to refine and create the image for each product.  In doing so, we ensure that all of our product container images have a similar structure.  For example, all Ping products written in Java use the same Java runtime version in our images, since each product container image has the same JVM base image.</p> <p>As you explore the repository, you will find directories for building each product and supporting image, each with a Dockerfile and supporting scripts.  As these are layered, they build out a complete product image for use in Kubernetes or another container orchestration environment.  For the most part, each directory represents the source for an image, whether a final product image or an intermediate layer.  </p>"},{"location":"docker-builds/#filesystem-layout","title":"Filesystem layout","text":"<p>The repository includes the following directories. This list is not comprehensive; it is focused on those folders of relevance to users:</p> <p>Highlighting</p> <p>The template used for this portal renders a highlight when you hover on a row in a table.  This action is expected, but the rows are not linked in any fashion.</p>"},{"location":"docker-builds/#non-image-related","title":"Non-image related","text":"Repository Directory Description ci_scripts Files and scripts used for building container images, both in an automated build pipeline and local use"},{"location":"docker-builds/#image-directories","title":"Image directories","text":"Repository Directory Description ldap-sdk-tools Files and scripts for an image with LDAP SDK tools available for use with Ping Directory pingbase Base OS, default environment variables, volumes, healthcheck and entrypoint command definitions.  This image provides a base to all Ping Identity container images pingcommon Files and scripts used with all Ping Identity container images pingdatacommon Files and scripts used with all PingData container images (i.e. PingDirectory, PingDataSync, etc.) pingjvm Files and scripts for creating the JVM image that is the foundation for most Ping product container images pingaccess Product-specific files and scripts for PingAccess container image pingauthorize Product-specific files and scripts for PingAuthorize container image pingauthorizepap Product-specific files and scripts for PingAuthorize Policy Editor container image pingcentral Product-specific files and scripts for PingCentral container image pingdataconsole Product-specific files and scripts for Ping Data Console container image pingdatasync Product-specific files and scripts for PingDataSync container image pingdelegator Product-specific files and scripts for PingDelegator container image pingdirectory Product-specific files and scripts for PingDirectory container image pingdirectoryproxy Product-specific files and scripts for PingDirectory Proxy container image pingfederate Product-specific files and scripts for PingFederate container image pingintelligence Product-specific files and scripts for PingIntelligence container image pingtoolkit Files and scripts for a utility image, typically used for an init or task-related container"},{"location":"docker-builds/DOCKER_BUILDS_HOOKS/","title":"Docker Builds - Hooks","text":"<p>Audience - Operators of DevOps Cloud environments. Not intended for Developers and admins of the Ping Identity products.</p> <p>Description - This document describes the many number of scripts that are called in during the lifecycle of a Ping Identity docker image from the initial <code>entrypoint.sh</code> script.</p> <p>Included with the base docker images, there is an example/stub provided for all possible hooks. It is very important that these names be used if a developer wishes to make subtle changes to their server-profile.</p> <p>The full ordered list of scripts that are called depending on what type of image (i.e. pingdirectory or pingdatasync) are:</p> <p></p>"},{"location":"docker-builds/DOCKER_BUILDS_HOOKS/#hooks-details","title":"Hooks Details","text":"<p>Details on hooks can be found within the code of each hook in the Docker-Builds Repo as well in <code>pingidentity-devops-getting-started/docs/docker-images/&lt;image_name&gt;/hooks</code> for each of the products images.</p>"},{"location":"docker-images/","title":"Index","text":"<p>See Using release tags for more information.</p>"},{"location":"docker-images/dockerImageSecurity/","title":"Evaluation of Docker Base Image Security","text":"<p>In the Center for Internet Security (CIS) Docker Benchmark v1.2.0, one of the recommendations says, \"4.3 Ensure that unnecessary packages are not installed in the container.\"</p> <p>It further states, \"You should consider using a minimal base image rather than the standard Red Hat/CentOS/Debian images if you can. Some of the options available include BusyBox and Alpine.\"</p> <p>The following sections present security aspects of different Linux distributions compared to Alpine Docker image. This doesn't necessarily mean that one is the best for Docker base images. Other factors, such as usability and compatibility, should also be considered when choosing the most suitable Docker image for an organization.</p>"},{"location":"docker-images/dockerImageSecurity/#evaluation-overview","title":"Evaluation overview","text":"<p>To evaluate Alpine\u2019s security, we compared it with the following popular Linux distributions: Ubuntu, CentOS, and Red Hat Enterprise Linux 7.</p> <p>For this comparison, we used the latest version, as of March 12, 2020, of each distribution\u2019s Docker image and compared them in four different categories:</p> <ul> <li>Image size</li> <li>Number of packages installed by default</li> <li>Number of historical vulnerabilities reported on cvedetails.com</li> <li>Number of vulnerabilities reported by the Clair scan</li> </ul> <p>The following table summarizes the numbers for each distribution.</p> Alpine Ubuntu CentOS RHEL7 Image Version alpine:3.11.3 ubuntu:18.04 centos:centos8.1.1911 rhel7:7.7-481 Image Size 5.59MB 64.2MB 237MB 205MB Number of Packages Installed 14 89 173 162 Number of Historical CVE*s 2 2007 2 662 Number of Vulnerabilities Reported by Clair 0 32 7 0 <p>*CVE - Common Vulnerabilities and Exposures</p>"},{"location":"docker-images/dockerImageSecurity/#image-size","title":"Image size","text":"<p>Alpine has an advantage in image size. Although smaller size doesn\u2019t directly translate into better security, the smaller size does mean less code packed into the image, which means smaller attack surface.</p>"},{"location":"docker-images/dockerImageSecurity/#number-of-packages-installed","title":"Number of packages installed","text":"<p>Because of Alpine's smaller size, Alpine has the fewest packages out of box. Fewer packages means lesser chance of having vulnerabilities in the dependencies, which is a plus for security.</p>"},{"location":"docker-images/dockerImageSecurity/#number-of-historical-cves","title":"Number of historical CVEs","text":"<p>Alpine and CentOS both rank highest in number of historical CVEs even though CentOS has a close relationship with RHEL7, and RHEL7 has 600+ reported vulnerabilities.</p>"},{"location":"docker-images/dockerImageSecurity/#number-of-vulnerabilities-reported-by-clair","title":"Number of vulnerabilities reported by Clair","text":"<p>Some vulnerabilities reported by Clair might not be real issues, but their presence does mean extra overhead for developers or security teams to triage these findings. This overhead can be avoided if unnecessary dependencies are excluded from the image in the first place.</p>"},{"location":"docker-images/dockerImageSecurity/#final-evaluation-results","title":"Final evaluation results","text":"<p>Although none of the four categories is perfect on its own for evaluating the security of a Linux distribution, in combination, Alpine presents greater advantages for use, which is why we selected it as the disribution for all of our Docker images.</p>"},{"location":"docker-images/dockerImageSecurity/#references","title":"References","text":"<ul> <li>CIS Docker Benchmarks</li> <li>Alpine CVEs</li> <li>Ubuntu CVEs</li> <li>CentOS CVEs</li> <li>Redhat Enterprise Linux CVEs</li> </ul>"},{"location":"docker-images/dockerImageSecurity/#ping-identitys-docker-image-hardening-guide","title":"Ping Identity's Docker Image Hardening Guide","text":"<p>For best practices for securing your product Docker image, see Ping Identity's Hardening Guide.</p>"},{"location":"docker-images/dockerImagesRef/","title":"Image Information","text":"<p>These documents for Ping Identity Docker images include information on:</p> <ul> <li>Related Docker images</li> <li>Ports exposed for the containers</li> <li>Environment variables for the image</li> <li>Associated deployment information</li> <li>Tagging methodology and support policy</li> </ul> <p>The image-specific documentation is generated from each new build ensuring these documents align with any changes over time.</p> <p>Storage Considerations on AWS</p> <p>When deploying Ping products in containers, please consider the information found here on storage options.</p> <p>Older images based on product versions that are no longer supported under our policy are removed from Docker Hub.  See the support policy page for details.</p> <p>As with many organizations, Ping Identity uses floating Docker image tags. This practice means, for example, that the <code>edge</code> tag does not refer to the same image over time as product updates occur. The release tags page has information on the <code>edge</code> and other tags, how often they are updated, and how to ensure the use of a particular version and release of a product image.</p> <p>Notification of new image tags</p> <p>If you want to be notified when new versions of product Docker images are available, see the Docker Images section of the FAQ page for instructions on following the docker-builds GitHub repository.</p>"},{"location":"docker-images/imageSupport/","title":"Ping Identity Docker image support policy","text":"<p>Ping Identity DevOps Support Policy \u00b6</p> <p>The support policy for other Ping DevOps offerings is found at Ping Identity DevOps Support Policy.</p>"},{"location":"docker-images/imageSupport/#overview","title":"Overview","text":"<p>Unlike software delivered as an archive, Docker images include:</p> <ul> <li>Product artifacts</li> <li>OS shim</li> <li>An optimized Java virtual machine (JVM) build</li> <li>Miscellaneous tools/libraries (Git, SSH, SSL) to run the software and automation scripts</li> </ul> <p>Because of the number of dependency updates and to ensure all patches are kept up to date, Ping Identity actively maintains product images semi-weekly (edge), releasing a stable build each month (sprint and latest).</p> <p>The build process retrieves the latest versions of:</p> <ul> <li>Operating System Shim (Alpine)</li> <li>Optimized JVM</li> <li>Product files</li> <li>Supporting tools/libraries</li> </ul>"},{"location":"docker-images/imageSupport/#actively-maintained-product-versions","title":"Actively Maintained Product Versions","text":"<p>The DevOps program actively maintains Docker images for:</p> <ul> <li>The two most recent feature releases (major and minor) of each product</li> <li>The latest patch release for each minor version</li> </ul> <p>Examples:</p> <ul> <li>If we currently maintain images for PingFederate 10.0 and 10.1, when PingFederate 10.2 is released, Docker images with PingFederate 10.0 will no longer be actively maintained.</li> <li>If a patch is released for 10.1, it supersedes the previous patch. In other words, if we currently maintain an image for PingFederate 10.1.2, when PingFederate 10.1.3 is released, it replaces 10.1.2.</li> </ul> <p>Active Build Product Versions</p> <p>To view products and versions actively being built, see the most recent Release Notes.</p>"},{"location":"docker-images/imageSupport/#docker-hub-image-removal","title":"Docker Hub image removal","text":"<p>Security vulnerabilities that arise over time and the continued evolution of our products create a situation in which older product images should be replaced with newer ones in your environment.  Images that have fallen out of Ping's active maintenance window are removed from Docker Hub 1 year after they were last built. If you need to keep images longer than this period, you will need to store them in a private repository.  The Docker documentation has instructions on this process.</p>"},{"location":"docker-images/imageSupport/#supported-os-shim","title":"Supported OS shim","text":"<p>The DevOps program uses Alpine as its base OS shim. For the rationale, see Evaluation of Docker Base Image Security.</p> <p>In rare scenarios where the consumer absolutely cannot run an Alpine based image, you can customize the base image. For more information, see Build a Docker Product Image Locally.</p> <p>Custom Built Images</p> <p>Using other Linux distributions should not cause an issue, but it cannot be guaranteed that the products will function as expected because these are not verified for compatibility. Ping Identity Support on custom images might be challenging and experience longer delays.</p>"},{"location":"docker-images/productVersionMatrix/","title":"Product Version, Image Release Matrix","text":"<p>It is recommended that you use the most recent Docker release tag available for the product version you want to run.</p> <p>The tag used to pull the image is in the format <code>{RELEASE}-{PRODUCT VERSION}</code></p> <p>Examples:</p> <ul> <li>PingFederate 10.2.5<ul> <li>pingidentity/pingfederate:<code>2108-10.2.5</code></li> </ul> </li> <li>PingDirectory 8.2.0.1<ul> <li>pingidentity/pingdirectory:<code>2101-8.2.0.1</code></li> </ul> </li> </ul> <p>This file shows the matrix of Ping Identity product software versions and the Ping Docker release tag in which they are available.  In accordance with our image support policy, only images from the past 12 months are supported:</p>"},{"location":"docker-images/releaseTags/","title":"Using Release Tags","text":"<p>Ping Identity uses multiple tags for each released image. On our Docker Hub site, you can view the available tags for each image.</p> <p>Multi-product deployment</p> <p>All product containers in a deployment should use the same release tag.</p>"},{"location":"docker-images/releaseTags/#store-images-privately","title":"Store images privately","text":"<p>Before discussing tags, it is important to know more about Ping Identity's use of Docker Hub for images.  While Docker Hub is very reliable and you can always find the latest images of Ping Identity products hosted there, do not rely on Ping to maintain Docker images in Docker Hub over time. See the image support policy for details.</p> <p>To ensure continued access to any image, pull the image in question and maintain it in your own image registry. Common Docker registry providers include: JFrog, AWS ECR, Google GCR and Azure ACR.</p>"},{"location":"docker-images/releaseTags/#tagging-format","title":"Tagging Format","text":"<p>To specify a release tag for deployments, use the following format:</p> <pre><code>image: pingidentity/&lt;ping-product&gt;:${PING_IDENTITY_DEVOPS_TAG}\n</code></pre> <p>In the example above, <code>&lt;ping-product&gt;</code> is the name of the product and <code>${PING_IDENTITY_DEVOPS_TAG}</code> is the assigned release tag value. The file containing the setting for <code>${PING_IDENTITY_DEVOPS_TAG}</code> is <code>~/.pingidentity/config</code> by default. This file is created by running the <code>pinctl config</code> command, documented here. You can also specify the release tag explicitly in your deployments. The release tag must be the same for each container in the deployment. For example:</p> <pre><code>image: pingidentity/&lt;ping-product&gt;:edge\n</code></pre>"},{"location":"docker-images/releaseTags/#determine-which-tag-to-use","title":"Determine Which Tag To Use","text":"<p>The tag to use depends on the purpose of the deployment in question.  Along with using a tag, any image on Docker Hub can be referenced using the SHA256 digest to ensure immutability in your environments.  The digest for a given image never changes regardless of any tag or tags with which it is associated.</p>"},{"location":"docker-images/releaseTags/#production-stability","title":"Production Stability","text":"<p>For customers in production environments, stability is often the highest priority. To ensure a deployment with the fewest dependencies and highest product stability:</p> <ul> <li> <p>Use the digest of a full sprint tag that includes the sprint version and product version.  For example, consider the image tag <code>pingidentity/pingfederate:2206-11.1.0</code>. To pull this image using the corresponding digest:</p> <pre><code>docker pull pingidentity/pingfederate@sha256:8eb88fc3345d8d71dafd83bcdcc38827ddb09768c6571c930b4d217ea177debf\n</code></pre> </li> </ul>"},{"location":"docker-images/releaseTags/#latest-image-features","title":"Latest Image Features","text":"<p>For demonstrations and testing latest features, use an <code>edge</code> based image. In these situations, it is a good practice to use a full tag variation similar to <code>pingfederate:11.1.0-edge</code>, rather than simply <code>pingfederate:edge</code>. Doing so avoids dependency conflicts that might occur in server profiles between product versions (for example, 10.x versus 11.x).</p>"},{"location":"docker-images/releaseTags/#evergreen-bleeding-edge","title":"Evergreen Bleeding Edge","text":"<p>The <code>edge</code> is the absolute latest product version and image features, with zero guarantees for stability. Typically, this tag is only of interest to Ping employees and partners.</p>"},{"location":"docker-images/releaseTags/#base-release-tags","title":"Base Release Tags","text":"<p>The base release tags for a product image build are:</p> <ul> <li>edge</li> <li>latest</li> <li>sprint</li> </ul>"},{"location":"docker-images/releaseTags/#edge","title":"edge","text":"<p>The <code>edge</code> release tag refers to \"bleeding edge\", indicating a build similar to an alpha release. This sliding tag includes the latest hooks and scripts, and is considered highly unstable. The <code>edge</code> release is characterized by:</p> <ul> <li>Latest product version</li> <li>Latest build image enhancements and fixes from our current sprint in progress</li> <li>Linux Alpine as the container base OS</li> </ul> <p>Example: <code>pingidentity/pingfederate:edge</code>, <code>pingidentity/pingfederate:11.1.0-edge</code></p>"},{"location":"docker-images/releaseTags/#latest","title":"latest","text":"<p><code>edge</code>is tagged as <code>latest</code> at the beginning of each month. The release tag indicates the latest stable release. This tag is also a sliding tag that marks the stable release for the latest sprint. The <code>latest</code> release is characterized by:</p> <ul> <li>Latest product version</li> <li>All completed and qualified enhacements and fixes from the prior monthly sprint</li> <li>Linux Alpine as the container base OS</li> </ul> <p>Example: <code>pingfederate:latest</code>, <code>pingfederate:11.1.0-latest</code></p>"},{"location":"docker-images/releaseTags/#sprint","title":"sprint","text":"<p>In addition to becoming <code>latest</code>, <code>edge</code> also is tagged as a stable <code>sprint</code> each month.  The <code>sprint</code> release tag is a build number indicating a stable build that will not change over time. The <code>sprint</code> number uses the YYMM format. For example, 2208 = August 2022.  The <code>sprint</code> release is characterized by:</p> <ul> <li>Latest product version at the time the sprint ended.</li> <li>All completed and qualified enhancements and fixes from the specified monthly sprint. The Docker images are generated at the end of the sprint.</li> <li>Linux Alpine as the container base OS</li> </ul> <p>Example: <code>pingfederate:2206</code>, <code>pingidentity/pingfederate:2206-11.1.0</code></p>"},{"location":"docker-images/releaseTags/#sprint-point-release","title":"sprint (point release)","text":"<p>Occasionally, a bug might be found on a stable release, whether in the product itself or something from the team building the image. In these situations, to avoid changing a <code>sprint</code> tag which is promised to be immutable, a point release would be created to move <code>latest</code> forward.  </p> <p>Example only</p> <p>These example tags do not exist, they are used here only for illustration purposes.</p> <p>Example: <code>pingfederate:2206.1</code>, <code>pingidentity/pingfederate:2206.1-11.1.0</code></p>"},{"location":"docker-images/releaseTags/#determine-image-version","title":"Determine Image Version","text":"<p>If you are unsure of the exact version of the image used for a given product container, shell into the container and examine the $IMAGE_VERSION environment variable. For example, if you are running a container locally under Docker, you would run the following commands:</p> <pre><code>docker container exec -it &lt;container id&gt; sh\necho $IMAGE_VERSION\n</code></pre> <p>The IMAGE_VERSION variable returns the version in this format:</p> <pre><code>[product]-[container OS]-[jdk]-[product version]-[build date]-[git revision]\n</code></pre> <p>For example:</p> <pre><code>IMAGE_VERSION=pingdirectory-alpine_3.16.0-al11-9.1.0.0-220725-c917\n</code></pre> <p>Where:</p> Key Value Product pingdirectory Container OS alpine_3.16.0 JDK al11 Product Version 9.1.0.0 Build Date 220725 Git Revision c917 <p>If the container is running under Kubernetes, use the kubectl exec command to access the container in order to obtain this information.</p> <p>Date Format</p> <p>In the $IMAGE_VERSION variable, Date is in YYMMDD format</p>"},{"location":"docker-images/apache-jmeter/","title":"Ping Identity DevOps Docker Image - `apache-jmeter`","text":""},{"location":"docker-images/apache-jmeter/#environment-variables","title":"Environment Variables","text":"<p>The following environment <code>ENV</code> variables can be used with this image.</p> ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} DATE ${DATE} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT Apache-JMeter Ping product name JAVA_RAM_PERCENTAGE 90.0 Percentage of the container memory to allocate to PingFederate JVM DO NOT set to 100% or your JVM will exit with OutOfMemory errors and the container will terminate STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh The command that the entrypoint will execute in the foreground to instantiate the container"},{"location":"docker-images/apache-jmeter/#docker-container-hook-scripts","title":"Docker Container Hook Scripts","text":"<p>Please go here for details on all apache-jmeter hook scripts</p> <p>This document is auto-generated from apache-jmeter/Dockerfile</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/apache-jmeter/hooks/","title":"Ping Identity DevOps <code>apache-jmeter</code> Hooks","text":"<p>List of available hooks: * 04-check-variables.sh * 17-check-license.sh</p> <p>These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon)</p> <p>This document is auto-generated from apache-jmeter/opt/staging/hooks</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/apache-jmeter/hooks/04-check-variables.sh/","title":"Ping Identity DevOps <code>apache-jmeter</code> Hook - <code>04-check-variables.sh</code>","text":"<p>This document is auto-generated from apache-jmeter/opt/staging/hooks/04-check-variables.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/apache-jmeter/hooks/17-check-license.sh/","title":"Ping Identity DevOps <code>apache-jmeter</code> Hook - <code>17-check-license.sh</code>","text":"<p>This document is auto-generated from apache-jmeter/opt/staging/hooks/17-check-license.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/ldap-sdk-tools/","title":"Ping Identity Docker Image - <code>ldap-sdk-tools</code>","text":"<p>This docker image provides an alpine image with the LDAP Client SDK tools to be used against other PingDirectory instances.</p>"},{"location":"docker-images/ldap-sdk-tools/#related-docker-images","title":"Related Docker Images","text":"<ul> <li><code>openjdk:8-jre8-alpine</code> - Alpine server to run LDAP SDK Tools from</li> </ul>"},{"location":"docker-images/ldap-sdk-tools/#environment-variables","title":"Environment Variables","text":"<p>The following environment <code>ENV</code> variables can be used with this image.</p> ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} DATE ${DATE} PING_PRODUCT_VERSION ${VERSION} PATH /opt/tools:${PATH} PING_PRODUCT ldap-sdk-tools Ping product name"},{"location":"docker-images/ldap-sdk-tools/#list-all-available-tools","title":"List all available tools","text":"<p><code>docker run -it --rm pingidentity/ldap-sdk-tools:edge ls</code></p>"},{"location":"docker-images/ldap-sdk-tools/#use-ldapsearch","title":"Use LDAPSearch","text":""},{"location":"docker-images/ldap-sdk-tools/#get-some-help","title":"Get some help","text":"<p><code>docker run -it --rm pingidentity/ldap-sdk-tools:edge ldapsearch --help</code></p>"},{"location":"docker-images/ldap-sdk-tools/#simple-search","title":"Simple search","text":"<pre><code>docker run -it --rm pingidentity/ldap-sdk-tools:edge \\\n    ldapsearch \\\n        -b dc=example,dc=com \\\n        -p 1389 \"(objectClass=*)\"\n</code></pre>"},{"location":"docker-images/ldap-sdk-tools/#save-output-to-host-file","title":"Save output to host file","text":"<pre><code>docker run -it --rm \\\n    -v /tmp:/opt/out \\\n    pingidentity/ldap-sdk-tools:edge \\\n    ldapsearch \\\n        --baseDN dc=example,dc=com \\\n        --port 1389 \\\n        --outputFormat json \"(objectClass=*)\" &gt;/tmp/search-result.json\n</code></pre>"},{"location":"docker-images/ldap-sdk-tools/#use-manage-certificates","title":"Use manage-certificates","text":""},{"location":"docker-images/ldap-sdk-tools/#trusting-certificates","title":"trusting certificates","text":"<pre><code>PWD=2FederateM0re\nmkdir -p /tmp/hibp\ndocker run -it --rm \\\n  -v /tmp/hibp:/opt/out \\\n  pingidentity/ldap-sdk-tools:edge \\\n  manage-certificates trust-server-certificate \\\n    --hostname haveibeenpwned.com \\\n    --port 1443 \\\n    --keystore /opt/out/hibp-2019.jks \\\n    --keystore-password ${PWD}\nls -all /tmp/hibp\nkeytool -list \\\n  -keystore /tmp/hibp/hibp-2019.jks \\\n  -storepass ${PWD}\n</code></pre>"},{"location":"docker-images/ldap-sdk-tools/#docker-container-hook-scripts","title":"Docker Container Hook Scripts","text":"<p>Please go here for details on all ldap-sdk-tools hook scripts</p> <p>This document is auto-generated from ldap-sdk-tools/Dockerfile</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/ldap-sdk-tools/hooks/","title":"Ping Identity DevOps <code>ldap-sdk-tools</code> Hooks","text":"<p>There are no default hooks defined for the <code>ldap-sdk-tools</code> image.</p> <p>Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image.</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingaccess/","title":"Ping Identity DevOps Docker Image - <code>pingaccess</code>","text":"<p>This docker image includes the Ping Identity PingAccess product binaries and associated hook scripts to create and run both PingAccess Admin and Engine nodes.</p>"},{"location":"docker-images/pingaccess/#related-docker-images","title":"Related Docker Images","text":"<ul> <li><code>pingidentity/pingbase</code> - Parent Image <p>This image inherits, and can use, Environment Variables from pingidentity/pingbase</p> </li> <li><code>pingidentity/pingcommon</code> - Common Ping files (i.e. hook scripts)</li> </ul>"},{"location":"docker-images/pingaccess/#environment-variables","title":"Environment Variables","text":"<p>In addition to environment variables inherited from pingidentity/pingbase, the following environment <code>ENV</code> variables can be used with this image.</p> ENV Variable Default Description BASE ${BASE:-/opt} Location of the top level directory where everything is located in image/container ROOT_USER administrator the default administrative user for PingData JAVA_HOME /opt/java STAGING_DIR ${BASE}/staging Path to the staging area where the remote and local server profiles can be merged OUT_DIR ${BASE}/out Path to the runtime volume SERVER_ROOT_DIR ${OUT_DIR}/instance Path from which the runtime executes IN_DIR ${BASE}/in Location of a local server-profile volume SERVER_BITS_DIR ${BASE}/server Path to the server bits BAK_DIR ${BASE}/backup Path to a volume generically used to export or backup data LOGS_DIR ${BASE}/logs Path to a volume generically used for logging PING_IDENTITY_ACCEPT_EULA NO Must be set to 'YES' for the container to start PING_IDENTITY_DEVOPS_FILE devops-secret File name for devops-creds passed as a Docker secret STAGING_MANIFEST ${BASE}/staging-manifest.txt Path to a manifest of files expected in the staging dir on first image startup CLEAN_STAGING_DIR false Whether to clean the staging dir when the image starts SECRETS_DIR /run/secrets Default path to the secrets TOPOLOGY_FILE ${STAGING_DIR}/topology.json Path to the topology file HOOKS_DIR ${STAGING_DIR}/hooks Path where all the hooks scripts are stored CONTAINER_ENV ${STAGING_DIR}/.env Environment Property file use to share variables between scripts in container SERVER_PROFILE_DIR /tmp/server-profile Path where the remote server profile is checked out or cloned before being staged prior to being applied on the runtime SERVER_PROFILE_URL A valid git HTTPS URL (not ssh) SERVER_PROFILE_URL_REDACT true When set to \"true\", the server profile git URL will not be printed to container output. SERVER_PROFILE_BRANCH A valid git branch (optional) SERVER_PROFILE_PATH The subdirectory in the git repo SERVER_PROFILE_UPDATE false Whether to update the server profile upon container restart SECURITY_CHECKS_STRICT false Requires strict checks on security SECURITY_CHECKS_FILENAME .jwk .pin Perform a check for filenames that may violate security (i.e. secret material) UNSAFE_CONTINUE_ON_ERROR If this is set to true, then the container will provide a hard warning and continue. LICENSE_DIR ${SERVER_ROOT_DIR} License directory PD_LICENSE_DIR ${STAGING_DIR}/pd.profile/server-root/pre-setup PD License directory. Separating from above LICENSE_DIR to differentiate for different products STARTUP_COMMAND The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container PING_IDENTITY_DEVOPS_KEY_REDACT true TAIL_LOG_FILES A whitespace separated list of log files to tail to the container standard output - DO NOT USE WILDCARDS like /path/to/logs/*.log COLORIZE_LOGS true If 'true', the output logs will be colorized with GREENs and REDs, otherwise, no colorization will be done.  This is good for tools that monitor logs and colorization gets in the way. LOCATION Docker Location default value If PingDirectory is deployed in multi cluster mode, that is, K8S_CLUSTER, K8S_CLUSTERS and K8S_SEED_CLUSTER are defined, LOCATION is ignored and K8S_CLUSTER is used as the location LOCATION_VALIDATION true Any string denoting a logical/physical location MAX_HEAP_SIZE 384m Heap size (for java products) JVM_TUNING AGGRESSIVE JAVA_RAM_PERCENTAGE 75.0 Percentage of the container memory to allocate to PingFederate JVM DO NOT set to 100% or your JVM will exit with OutOfMemory errors and the container will terminate VERBOSE false Triggers verbose messages in scripts using the set -x option. PING_DEBUG false Set the server in debug mode, with increased output PING_PRODUCT The name of Ping product, i.e. PingFederate, PingDirectory - must be a valid Ping product type. This variable should be overridden by child images. PING_PRODUCT_VALIDATION true i.e. PingFederate,PingDirectory ADDITIONAL_SETUP_ARGS List of setup arguments passed to Ping Data setup-arguments.txt file LDAP_PORT 1389 Port over which to communicate for LDAP LDAPS_PORT 1636 Port over which to communicate for LDAPS HTTPS_PORT 1443 Port over which to communicate for HTTPS JMX_PORT 1689 Port for monitoring over JMX protocol ORCHESTRATION_TYPE The type of orchestration tool used to run the container, normally set in the deployment (.yaml) file.  Expected values include: - compose - swarm - kubernetes Defaults to blank (i.e. No type is set) USER_BASE_DN dc=example,dc=com Base DN for user data DOLLAR '$' Variable with a literal value of '$', to avoid unwanted variable substitution PD_ENGINE_PUBLIC_HOSTNAME localhost PD (PingDirectory) public hostname that may be used in redirects PD_ENGINE_PRIVATE_HOSTNAME pingdirectory PD (PingDirectory) private hostname PDP_ENGINE_PUBLIC_HOSTNAME localhost PDP (PingDirectoryProxy) public hostname that may be used in redirects PDP_ENGINE_PRIVATE_HOSTNAME pingdirectoryproxy PDP (PingDirectoryProxy) private hostname PDS_ENGINE_PUBLIC_HOSTNAME localhost PDS (PingDataSync) public hostname that may be used in redirects PDS_ENGINE_PRIVATE_HOSTNAME pingdatasync PDS (PingDataSync) private hostname PAZ_ENGINE_PUBLIC_HOSTNAME localhost PAZ (PingAuthorize) public hostname that may be used in redirects PAZ_ENGINE_PRIVATE_HOSTNAME pingauthorize PAZ (PingAuthorize) private hostname PAZP_ENGINE_PUBLIC_HOSTNAME localhost PAZP (PingAuthorize-PAP) public hostname that may be used in redirects PAZP_ENGINE_PRIVATE_HOSTNAME pingauthorizepap PAZP (PingAuthorize-PAP) private hostname PF_ENGINE_PUBLIC_HOSTNAME localhost PF (PingFederate) engine public hostname that may be used in redirects PF_ENGINE_PRIVATE_HOSTNAME pingfederate PF (PingFederate) engine private hostname PF_ADMIN_PUBLIC_BASEURL https://localhost:9999 PF (PingFederate) admin public baseurl that may be used in redirects PF_ADMIN_PUBLIC_HOSTNAME localhost PF (PingFederate) admin public hostname that may be used in redirects PF_ADMIN_PRIVATE_HOSTNAME pingfederate-admin PF (PingFederate) admin private hostname PA_ENGINE_PUBLIC_HOSTNAME localhost PA (PingAccess) engine public hostname that may be used in redirects PA_ENGINE_PRIVATE_HOSTNAME pingaccess PA (PingAccess) engine private hostname PA_ADMIN_PUBLIC_HOSTNAME localhost PA (PingAccess) admin public hostname that may be used in redirects PA_ADMIN_PRIVATE_HOSTNAME pingaccess-admin PA (PingAccess) admin private hostname ROOT_USER_DN cn=${ROOT_USER} DN of the server root user ENV ${BASE}/.profile MOTD_URL https://raw.githubusercontent.com/pingidentity/pingidentity-devops-getting-started/master/motd/motd.json Instructs the image to pull the MOTD json from the following URL. If this MOTD_URL variable is empty, then no motd will be downloaded. The format of this MOTD file must match the example provided in the url: https://raw.githubusercontent.com/pingidentity/pingidentity-devops-getting-started/master/motd/motd.json PS1 \\${PING_PRODUCT}:\\h:\\w\\n&gt; Default shell prompt (i.e. productName:hostname:workingDir) PATH ${JAVA_HOME}/bin:${BASE}:${SERVER_ROOT_DIR}/bin:${PATH} PATH used by the container SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} DATE ${DATE} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingAccess Ping product name LICENSE_DIR ${SERVER_ROOT_DIR}/conf License directory LICENSE_FILE_NAME pingaccess.lic Name of license file LICENSE_SHORT_NAME PA Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server OPERATIONAL_MODE STANDALONE PA_RUN_PA_OPERATIONAL_MODE will override this value for PingAccess 7.3 and later. PA_ADMIN_PASSWORD_INITIAL 2Access PING_IDENTITY_PASSWORD 2FederateM0re Specify a password for administrator user for interaction with admin API STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh The command that the entrypoint will execute in the foreground to instantiate the container TAIL_LOG_FILES ${SERVER_ROOT_DIR}/log/pingaccess.log Files tailed once container has started PA_ADMIN_PORT 9000 Default port for PA Admin API and console Ignored when using PingIdentity Helm charts PA_ADMIN_CLUSTER_PORT 9090 Default port when clustering PA primary administrative node Ignored when using PingIdentity Helm charts JAVA_RAM_PERCENTAGE 60.0 Percentage of the container memory to allocate to PingAccess JVM DO NOT set to 100% or your JVM will exit with OutOfMemory errors and the container will terminate FIPS_MODE_ON false Turns on FIPS mode (currently with the Bouncy Castle FIPS provider) set to exactly \"true\" lowercase to turn on set to anything else to turn off PA_FIPS_MODE_PA_FIPS_MODE will override this for PingAccess 7.3 and later. SHOW_LIBS_VER true Defines a variable to allow showing library versions in the output at startup default to true SHOW_LIBS_VER_PRE_PATCH false Defines a variable to allow showing library version prior to patches being applied default to false This is helpful to ensure that the patch process updates all libraries affected PA_ENGINE_PORT 3000 ADMIN_WAITFOR_TIMEOUT 300 wait-for timeout for 80-post-start.sh hook script How long to wait for the PA Admin console to be available"},{"location":"docker-images/pingaccess/#ports-exposed","title":"Ports Exposed","text":"<p>The following ports are exposed from the container.  If a variable is used, then it may come from a parent container</p> <ul> <li>${PA_ADMIN_PORT}</li> <li>${PA_ENGINE_PORT}</li> <li>${HTTPS_PORT}</li> </ul>"},{"location":"docker-images/pingaccess/#running-a-pingaccess-container","title":"Running a PingAccess container","text":"<p>To run a PingAccess container:</p> <pre><code>  docker run \\\n           --name pingaccess \\\n           --publish 9000:9000 \\\n           --publish 443:1443 \\\n           --detach \\\n           --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\\n           --env SERVER_PROFILE_PATH=getting-started/pingaccess \\\n           --env PING_IDENTITY_ACCEPT_EULA=YES \\\n           --env PING_IDENTITY_DEVOPS_USER \\\n           --env PING_IDENTITY_DEVOPS_KEY \\\n           --tmpfs /run/secrets \\\n           pingidentity/pingaccess:edge\n</code></pre> <p>Follow Docker logs with:</p> <pre><code>docker logs -f pingaccess\n</code></pre> <p>If using the command above with the embedded server profile, log in with:</p> <ul> <li>https://localhost:9000</li> <li>Username: Administrator</li> <li>Password: 2FederateM0re</li> </ul>"},{"location":"docker-images/pingaccess/#docker-container-hook-scripts","title":"Docker Container Hook Scripts","text":"<p>Please go here for details on all pingaccess hook scripts</p> <p>This document is auto-generated from pingaccess/Dockerfile</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingaccess/hooks/","title":"Ping Identity DevOps <code>pingaccess</code> Hooks","text":"<p>List of available hooks: * 04-check-variables.sh.pre * 20-restart-sequence.sh.pre * 50-before-post-start.sh * 51-add-engine.sh * 80-post-start.sh * 81-after-start-process.sh * 83-change-password.sh * 85-import-configuration.sh</p> <p>These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon)</p> <p>This document is auto-generated from pingaccess/opt/staging/hooks</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingaccess/hooks/04-check-variables.sh.pre/","title":"Ping Identity DevOps <code>pingaccess</code> Hook - <code>04-check-variables.sh.pre</code>","text":"<p>This document is auto-generated from pingaccess/opt/staging/hooks/04-check-variables.sh.pre</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingaccess/hooks/20-restart-sequence.sh.pre/","title":"Ping Identity DevOps <code>pingaccess</code> Hook - <code>20-restart-sequence.sh.pre</code>","text":"<p>This document is auto-generated from pingaccess/opt/staging/hooks/20-restart-sequence.sh.pre</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingaccess/hooks/50-before-post-start.sh/","title":"Ping Identity DevOps <code>pingaccess</code> Hook - <code>50-before-post-start.sh</code>","text":"<p>This is called after the start or restart sequence has finished and before  the server within the container starts</p> <p>This document is auto-generated from pingaccess/opt/staging/hooks/50-before-post-start.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingaccess/hooks/51-add-engine.sh/","title":"Ping Identity DevOps <code>pingaccess</code> Hook - <code>51-add-engine.sh</code>","text":"<p>This script is started in the background immediately before  the server within the container is started  This is useful to implement any logic that needs to occur after the  server is up and running  For example, enabling replication in PingDirectory, initializing Sync  Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess</p> <p>This document is auto-generated from pingaccess/opt/staging/hooks/51-add-engine.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingaccess/hooks/80-post-start.sh/","title":"Ping Identity DevOps <code>pingaccess</code> Hook - <code>80-post-start.sh</code>","text":"<p>This script is used to import any configurations that are  needed after PingAccess starts</p> <p>This document is auto-generated from pingaccess/opt/staging/hooks/80-post-start.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingaccess/hooks/81-after-start-process.sh/","title":"Ping Identity DevOps <code>pingaccess</code> Hook - <code>81-after-start-process.sh</code>","text":"<p>This document is auto-generated from pingaccess/opt/staging/hooks/81-after-start-process.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingaccess/hooks/83-change-password.sh/","title":"Ping Identity DevOps <code>pingaccess</code> Hook - <code>83-change-password.sh</code>","text":"<p>This document is auto-generated from pingaccess/opt/staging/hooks/83-change-password.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingaccess/hooks/85-import-configuration.sh/","title":"Ping Identity DevOps <code>pingaccess</code> Hook - <code>85-import-configuration.sh</code>","text":"<p>This script is started in the background immediately before  the server within the container is started  This is useful to implement any logic that needs to occur after the  server is up and running  For example, enabling replication in PingDirectory, initializing Sync  Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess</p> <p>This document is auto-generated from pingaccess/opt/staging/hooks/85-import-configuration.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingauthorize/","title":"Ping Identity DevOps Docker Image - <code>pingauthorize</code>","text":"<p>This docker image includes the Ping Identity PingAuthorize product binaries and associated hook scripts to create and run a PingAuthorize instance or instances.</p>"},{"location":"docker-images/pingauthorize/#related-docker-images","title":"Related Docker Images","text":"<ul> <li><code>pingidentity/pingbase</code> - Parent Image <p>This image inherits, and can use, Environment Variables from pingidentity/pingbase</p> </li> <li><code>pingidentity/pingdatacommon</code> - Common Ping files (i.e. hook scripts)</li> </ul>"},{"location":"docker-images/pingauthorize/#environment-variables","title":"Environment Variables","text":"<p>In addition to environment variables inherited from pingidentity/pingbase, the following environment <code>ENV</code> variables can be used with this image.</p> ENV Variable Default Description SHIM ${SHIM} --shm-size 256m \\ IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} DATE ${DATE} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingAuthorize Ping product name LICENSE_DIR ${PD_LICENSE_DIR} PD License directory. This value is set from the pingbase dockerfile LICENSE_FILE_NAME PingAuthorize.lic Name of license file LICENSE_SHORT_NAME PingAuthorize Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server MAX_HEAP_SIZE 1g Minimal Heap size required for PingAuthorize STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to /SECRETS_DIR/root-user-password ENCRYPTION_PASSWORD_FILE Location of file with the passphrase for setting up encryption Defaults to /SECRETS_DIR/encryption-password KEYSTORE_FILE Location of the keystore file containing the server certificate. If left undefined, the SECRETS_DIR will be checked for a keystore. If that keystore does not exist, the server will generate a self-signed certificate. KEYSTORE_PIN_FILE Location of the pin file for the keystore defined in KEYSTORE_FILE. You must specify a KEYSTORE_PIN_FILE when a KEYSTORE_FILE is present. This value does not need to be defined when allowing the server to generate a self-signed certificate. KEYSTORE_TYPE Format of the keystore defined in KEYSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the keystore format will be inferred based on the file extension of the KEYSTORE_FILE, defaulting to \"jks\". TRUSTSTORE_FILE Location of the truststore file for the server. If left undefined, the SECRETS_DIR will be checked for a truststore. If that truststore does not exist, the server will generate a truststore, containing its own certificate. TRUSTSTORE_PIN_FILE Location of the pin file for the truststore defined in TRUSTSTORE_FILE. You must specify a TRUSTSTORE_PIN_FILE when a TRUSTSTORE_FILE is present. This value does not need to be defined when allowing the server to generate a truststore. TRUSTSTORE_TYPE Format of the truststore defined in TRUSTSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the truststore format will be inferred based on the file extension of the TRUSTSTORE_FILE, defaulting to \"jks\". TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/trace ${SERVER_ROOT_DIR}/logs/policy-decision ${SERVER_ROOT_DIR}/logs/ldap-access Files tailed once container has started PD_PROFILE ${STAGING_DIR}/pd.profile Directory for the profile used by the PingData manage-profile tool UNBOUNDID_SKIP_START_PRECHECK_NODETACH true Setting this variable to true speeds up server startup time by skipping an unnecessary JVM check. CERTIFICATE_NICKNAME There is an additional certificate-based variable used to identity the certificate alias used within the <code>KEYSTORE_FILE</code>. That variable is called <code>CERTIFICATE_NICKNAME</code>, which identifies the certificate to use by the server in the <code>KEYSTORE_FILE</code>. If a value is not provided, the container will look at the list certs found in the <code>KEYSTORE_FILE</code> and if one - and only one - certificate is found of type <code>PrivateKeyEntry</code>, that alias will be used. COLUMNS 120 Sets the number of columns in PingAuthorize command-line tool output"},{"location":"docker-images/pingauthorize/#ports-exposed","title":"Ports Exposed","text":"<p>The following ports are exposed from the container.  If a variable is used, then it may come from a parent container</p> <ul> <li>${LDAP_PORT}</li> <li>${LDAPS_PORT}</li> <li>${HTTPS_PORT}</li> <li>${JMX_PORT}</li> </ul>"},{"location":"docker-images/pingauthorize/#running-a-pingauthorize-container","title":"Running a PingAuthorize container","text":"<p>The easiest way to test a simple standalone image of PingAuthorize is to cut/paste the following command into a terminal on a machine with docker.</p> <pre><code>  docker run \\\n           --name pingauthorize \\\n           --publish 1389:1389 \\\n           --publish 8443:1443 \\\n           --detach \\\n           --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\\n           --env SERVER_PROFILE_PATH=getting-started/pingauthorize \\\n           --env PING_IDENTITY_ACCEPT_EULA=YES \\\n           --env PING_IDENTITY_DEVOPS_USER \\\n           --env PING_IDENTITY_DEVOPS_KEY \\\n           --tmpfs /run/secrets \\\n          pingidentity/pingauthorize:edge\n</code></pre> <p>You can view the Docker logs with the command:</p> <pre><code>  docker logs -f pingauthorize\n</code></pre> <p>You should see the ouptut from a PingAuthorize install and configuration, ending with a message the the PingAuthorize has started.  After it starts, you will see some typical access logs.  Simply <code>Ctrl-C</code> after to stop tailing the logs.</p>"},{"location":"docker-images/pingauthorize/#stoppingremoving-the-container","title":"Stopping/Removing the container","text":"<p>To stop the container:</p> <pre><code>  docker container stop pingauthorize\n</code></pre> <p>To remove the container:</p> <pre><code>  docker container rm -f pingauthorize\n</code></pre>"},{"location":"docker-images/pingauthorize/#docker-container-hook-scripts","title":"Docker Container Hook Scripts","text":"<p>Please go here for details on all pingauthorize hook scripts</p> <p>This document is auto-generated from pingauthorize/Dockerfile</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingauthorize/hooks/","title":"Ping Identity DevOps <code>pingauthorize</code> Hooks","text":"<p>There are no default hooks defined for the <code>pingauthorize</code> image.</p> <p>Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image.</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingauthorizepap/","title":"Ping Identity DevOps Docker Image - <code>pingauthorizepap</code>","text":"<p>This docker image includes the Ping Identity PingAuthorize Policy Editor product binaries and associated hook scripts to create and run a PingAuthorize Policy Editor instance.</p>"},{"location":"docker-images/pingauthorizepap/#related-docker-images","title":"Related Docker Images","text":"<ul> <li><code>pingidentity/pingbase</code> - Parent Image <p>This image inherits, and can use, Environment Variables from pingidentity/pingbase</p> </li> <li><code>pingidentity/pingdatacommon</code> - Common Ping files (i.e. hook scripts)</li> </ul>"},{"location":"docker-images/pingauthorizepap/#environment-variables","title":"Environment Variables","text":"<p>In addition to environment variables inherited from pingidentity/pingbase, the following environment <code>ENV</code> variables can be used with this image.</p> ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} DATE ${DATE} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingAuthorize-PAP Ping product name LICENSE_DIR ${PD_LICENSE_DIR} PD License directory. This value is set from the pingbase dockerfile LICENSE_FILE_NAME PingAuthorize.lic Name of license File LICENSE_SHORT_NAME PingAuthorize Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server MAX_HEAP_SIZE 384m Minimal Heap size required for PingAuthorize Policy Editor STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint executes in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container KEYSTORE_FILE Location of the keystore file containing the server certificate. If left undefined, the SECRETS_DIR will be checked for a keystore. If that keystore does not exist, the server will generate a self-signed certificate. KEYSTORE_PIN_FILE Location of the pin file for the keystore defined in KEYSTORE_FILE. You must specify a KEYSTORE_PIN_FILE when a KEYSTORE_FILE is present. This value does not need to be defined when allowing the server to generate a self-signed certificate. KEYSTORE_TYPE Format of the keystore defined in KEYSTORE_FILE. One of \"jks\" or \"pkcs12\". If not defined, the keystore format will be inferred based on the file extension of the KEYSTORE_FILE, defaulting to \"jks\". TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/authorize-pe.log ${SERVER_ROOT_DIR}/logs/management-audit.log ${SERVER_ROOT_DIR}/logs/policy-db.log ${SERVER_ROOT_DIR}/logs/setup.log ${SERVER_ROOT_DIR}/logs/start-server.log ${SERVER_ROOT_DIR}/logs/stop-server.log Files tailed once container has started REST_API_HOSTNAME localhost Hostname used for the REST API (deprecated, use <code>PING_EXTERNAL_BASE_URL</code> instead) DECISION_POINT_SHARED_SECRET 2FederateM0re Defines the shared secret between PAZ and the Policy Editor PING_ENABLE_API_HTTP_CACHE true When set to <code>false</code>, disables the default HTTP API caching in the Policy Manager, Trust Framework, and Test Suite PING_POLICY_DB_SYNC When set to <code>true</code>, the container uses the provided policy database admin credentials to either create a PostgreSQL policy database or upgrade it if it already exists. You must also provide the <code>PING_DB_CONNECTION_STRING</code>, <code>PING_DB_ADMIN_USERNAME</code>, <code>PING_DB_ADMIN_PASSWORD</code> <code>PING_DB_APP_USERNAME</code>, and <code>PING_DB_APP_PASSWORD</code> variables for this to work. PING_DB_CONNECTION_STRING jdbc:h2:file:./Symphonic;ALIAS_COLUMN_NAME=TRUE The JDBC connection string used to connect to the policy database. Use the format <code>jdbc:postgresql://&lt;host&gt;:&lt;port&gt;/&lt;database&gt;</code>. Only H2 embedded files and PostgreSQL are supported. PING_DB_ADMIN_USERNAME sa The database administration username to use when creating or upgrading a policy database. PING_DB_ADMIN_PASSWORD Passw0rd The database administration password to use when creating or upgrading a policy database. PING_DB_APP_USERNAME pap_user The username that the Policy Editor should use when accessing the policy database during server runtime. PING_DB_APP_PASSWORD Symphonic2014! The password that the Policy Editor should use when accessing the policy database during server runtime."},{"location":"docker-images/pingauthorizepap/#ports-exposed","title":"Ports Exposed","text":"<p>The following ports are exposed from the container.  If a variable is used, then it may come from a parent container</p> <ul> <li>${HTTPS_PORT}</li> </ul>"},{"location":"docker-images/pingauthorizepap/#running-a-pingauthorize-policy-editor-container","title":"Running a PingAuthorize Policy Editor container","text":"<p>A PingAuthorize Policy Editor may be set up in one of two modes:</p> <ul> <li>Demo mode: Uses insecure username/password authentication.</li> <li>OIDC mode: Uses an OpenID Connect provider for authentication.</li> </ul> <p>To run a PingAuthorize Policy Editor container in demo mode:</p> <pre><code>  docker run \\\n           --name pingauthorizepap \\\n           --env PING_EXTERNAL_BASE_URL=my-pap-hostname:8443 \\\n           --publish 8443:1443 \\\n           --detach \\\n           --env PING_IDENTITY_ACCEPT_EULA=YES \\\n           --env PING_IDENTITY_DEVOPS_USER \\\n           --env PING_IDENTITY_DEVOPS_KEY \\\n           --tmpfs /run/secrets \\\n           pingidentity/pingauthorizepap:edge\n</code></pre> <p>Log in with:</p> <ul> <li>https://my-pap-hostname:8443/<ul> <li>Username: admin</li> <li>Password: password123</li> </ul> </li> </ul> <p>To run a PingAuthorize Policy Editor container in OpenID Connect mode, specify the <code>PING_OIDC_CONFIGURATION_ENDPOINT</code> and <code>PING_CLIENT_ID</code> environment variables. To provide scopes other than the default (<code>oidc email profile</code>), specify the <code>PING_SCOPE</code> environment variable:</p> <pre><code>  docker run \\\n           --name pingauthorizepap \\\n           --env PING_EXTERNAL_BASE_URL=my-pe-hostname:8443 \\\n           --env PING_OIDC_CONFIGURATION_ENDPOINT=https://my-oidc-provider/.well-known/openid-configuration \\\n           --env PING_CLIENT_ID=b1929abc-e108-4b4f-83d467059fa1 \\\n           --env PING_SCOPE=\"oidc email profile phone\" \\\n           --publish 8443:1443 \\\n           --detach \\\n           --env PING_IDENTITY_ACCEPT_EULA=YES \\\n           --env PING_IDENTITY_DEVOPS_USER \\\n           --env PING_IDENTITY_DEVOPS_KEY \\\n           --tmpfs /run/secrets \\\n           pingidentity/pingauthorizepap:edge\n</code></pre> <p>Note: If both <code>PING_OIDC_CONFIGURATION_ENDPOINT</code> and <code>PING_CLIENT_ID</code> are not specified, then Docker sets up the PingAuthorize Policy Editor container in demo mode.</p> <p>Log in with:</p> <ul> <li>https://my-pap-hostname:8443/<ul> <li>Provide credentials as prompted by the OIDC provider</li> </ul> </li> </ul> <p>Follow Docker logs with:</p> <pre><code>docker logs -f pingauthorizepap\n</code></pre>"},{"location":"docker-images/pingauthorizepap/#specifying-the-external-hostname-and-port","title":"Specifying the external hostname and port","text":"<p>The Policy Editor consists of a client-side application that runs in the user's web browser and a backend REST API service that runs within the container. So that the client-side application can successfully make API calls to the backend, the Policy Editor must be configured with an externally accessible hostname:port. If the Policy Editor is configured in OIDC mode, then the external hostname:port pair is also needed so that the Policy Editor can correctly generate its OIDC redirect URI.</p> <p>Use the <code>PING_EXTERNAL_BASE_URL</code> environment variable to specify the Policy Editor's external hostname and port using the form <code>hostname[:port]</code>, where <code>hostname</code> is the hostname of the Docker host and <code>port</code> is the Policy Editor container's published port. If the published port is 443, then it should be omitted.</p> <p>For example:</p> <pre><code>  docker run \\\n           --name pingauthorizepap \\\n           --env PING_EXTERNAL_BASE_URL=my-pap-hostname:8443 \\\n           --publish 8443:1443 \\\n           --detach \\\n           --env PING_IDENTITY_ACCEPT_EULA=YES \\\n           --env PING_IDENTITY_DEVOPS_USER \\\n           --env PING_IDENTITY_DEVOPS_KEY \\\n           --tmpfs /run/secrets \\\n           pingidentity/pingauthorizepap:edge\n</code></pre>"},{"location":"docker-images/pingauthorizepap/#changing-the-default-periodic-database-backup-schedule-and-location","title":"Changing the default periodic database backup schedule and location","text":"<p>The PAP performs periodic backups of the policy database. The results are placed in the <code>policy-backup</code> directory underneath the instance root.</p> <p>Use the <code>PING_BACKUP_SCHEDULE</code> environment variable to specify the PAP's periodic database backup schedule in the form of a cron expression. The cron expression evaluates against the container timezone, UTC. Use the <code>PING_H2_BACKUP_DIR</code> environment variable to change the backup output directory.</p> <p>For example, to perform backups daily at UTC noon and place backups in <code>/opt/out/backup</code>:</p> <pre><code>  docker run \\\n           --name pingauthorizepap \\\n           --env PING_EXTERNAL_BASE_URL=my-pap-hostname:8443 \\\n           --env PING_BACKUP_SCHEDULE=\"0 0 12 * * ?\" \\\n           --env PING_H2_BACKUP_DIR=/opt/out/backup \\\n           --publish 8443:1443 \\\n           --detach \\\n           --env PING_IDENTITY_ACCEPT_EULA=YES \\\n           --env PING_IDENTITY_DEVOPS_USER \\\n           --env PING_IDENTITY_DEVOPS_KEY \\\n           --tmpfs /run/secrets \\\n           pingidentity/pingauthorizepap:edge\n</code></pre>"},{"location":"docker-images/pingauthorizepap/#creating-and-upgrading-a-postgresql-policy-database","title":"Creating and upgrading a PostgreSQL policy database","text":"<p>Although the Policy Editor uses an embedded H2 file for its policy database implementation by default, it also has the capability to use a PostgreSQL database. However, this database must first be initialized with database objects.</p> <p>Set the <code>PING_POLICY_DB_SYNC</code> environment variable to <code>true</code>, provide the PostgreSQL JDBC connection string in <code>PING_DB_CONNECTION_STRING</code>, the database administration credentials through <code>PING_DB_ADMIN_USERNAME</code> and <code>PING_DB_ADMIN_PASSWORD</code>, and the server runtime credentials through <code>PING_DB_APP_USERNAME</code> and <code>PING_DB_APP_PASSWORD</code> to indicate that pingauthorizepap should create the necessary policy database objects.</p> <p>Similarly, use the same environment variables with the same values when a new version of the application is released. pingauthorizepap will use the database administration user to perform any necessary upgrades to the database objects.</p> <p>In both scenarios, the database administration user and the server runtime user must exist and be able to sign into the PostgreSQL server. In the create scenario, the database administration user must be able to create databases. In the upgrade scenario, the database administration user must own the database objects. Therefore, it is advisable to continually provide the same administration credentials during creation and upgrades to prevent permissions issues.</p> <p>For example, assume that a system administrator has created a PostgreSQL username <code>pap_admin</code> that can sign into a PostgreSQL server hosted at example.com and listening on port 5432. They have also created the runtime user <code>pap_user</code>.</p> <p>To create the database objects under the database named <code>my_pap_db</code> and start the Policy Editor, use the following command:</p> <pre><code>  docker run \\\n           --name pingauthorizepap \\\n           --env PING_EXTERNAL_BASE_URL=my-pap-hostname:8443 \\\n           --env PING_POLICY_DB_SYNC=true \\\n           --env PING_DB_CONNECTION_STRING=jdbc:postgresql://example.com:5432/my_pap_db \\\n           --env PING_DB_ADMIN_USERNAME=pap_admin \\\n           --env PING_DB_ADMIN_PASSWORD=2FederateM0re \\\n           --env PING_DB_APP_USERNAME=pap_user \\\n           --env PING_DB_APP_PASSWORD=2FederateM0re \\\n           --publish 8443:1443 \\\n           --detach \\\n           --env PING_IDENTITY_ACCEPT_EULA=YES \\\n           --env PING_IDENTITY_DEVOPS_USER \\\n           --env PING_IDENTITY_DEVOPS_KEY \\\n           --tmpfs /run/secrets \\\n           pingidentity/pingauthorizepap:edge\n</code></pre> <p>Use the same command when a new pingauthorizepap release requires an upgrade to the policy database schema.</p> <p>Note that <code>PING_DB_ADMIN_PASSWORD</code> and <code>PING_DB_APP_PASSWORD</code> are only provided on the command line for illustrative purposes and can instead be provided through a Vault or through a <code>/run/secrets</code> <code>.env</code> file.</p>"},{"location":"docker-images/pingauthorizepap/#docker-container-hook-scripts","title":"Docker Container Hook Scripts","text":"<p>Please go here for details on all pingauthorizepap hook scripts</p> <p>This document is auto-generated from pingauthorizepap/Dockerfile</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingauthorizepap/hooks/","title":"Ping Identity DevOps <code>pingauthorizepap</code> Hooks","text":"<p>List of available hooks: * 18-setup-sequence.sh * 183-run-setup.sh * 184-run-policy-db.sh * 80-post-start.sh * 81-install-policies.sh * pingauthorizepap.lib.sh</p> <p>These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon)</p> <p>This document is auto-generated from pingauthorizepap/opt/staging/hooks</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingauthorizepap/hooks/18-setup-sequence.sh/","title":"Ping Identity DevOps <code>pingauthorizepap</code> Hook - <code>18-setup-sequence.sh</code>","text":"<p>Quarterbacks all the scripts associated with the setup of a  PingData product</p> <p>This document is auto-generated from pingauthorizepap/opt/staging/hooks/18-setup-sequence.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingauthorizepap/hooks/183-run-setup.sh/","title":"Ping Identity DevOps <code>pingauthorizepap</code> Hook - <code>183-run-setup.sh</code>","text":"<p>This document is auto-generated from pingauthorizepap/opt/staging/hooks/183-run-setup.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingauthorizepap/hooks/184-run-policy-db.sh/","title":"Ping Identity DevOps <code>pingauthorizepap</code> Hook - <code>184-run-policy-db.sh</code>","text":"<p>This document is auto-generated from pingauthorizepap/opt/staging/hooks/184-run-policy-db.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingauthorizepap/hooks/80-post-start.sh/","title":"Ping Identity DevOps <code>pingauthorizepap</code> Hook - <code>80-post-start.sh</code>","text":"<p>This script is used to import any configurations that are  needed after PingAuthorize Policy Editor starts</p> <p>This document is auto-generated from pingauthorizepap/opt/staging/hooks/80-post-start.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingauthorizepap/hooks/81-install-policies.sh/","title":"Ping Identity DevOps <code>pingauthorizepap</code> Hook - <code>81-install-policies.sh</code>","text":"<p>This document is auto-generated from pingauthorizepap/opt/staging/hooks/81-install-policies.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingauthorizepap/hooks/pingauthorizepap.lib.sh/","title":"Ping Identity DevOps <code>pingauthorizepap</code> Hook - <code>pingauthorizepap.lib.sh</code>","text":"<p>This document is auto-generated from pingauthorizepap/opt/staging/hooks/pingauthorizepap.lib.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingbase/","title":"Ping Identity Docker Image - <code>pingbase</code>","text":"<p>This docker image provides a base image for all Ping Identity DevOps product images.</p>"},{"location":"docker-images/pingbase/#environment-variables","title":"Environment Variables","text":"<p>The following environment <code>ENV</code> variables can be used with this image.</p> ENV Variable Default Description BASE ${BASE:-/opt} Location of the top level directory where everything is located in image/container ROOT_USER administrator the default administrative user for PingData JAVA_HOME /opt/java STAGING_DIR ${BASE}/staging Path to the staging area where the remote and local server profiles can be merged OUT_DIR ${BASE}/out Path to the runtime volume SERVER_ROOT_DIR ${OUT_DIR}/instance Path from which the runtime executes IN_DIR ${BASE}/in Location of a local server-profile volume SERVER_BITS_DIR ${BASE}/server Path to the server bits BAK_DIR ${BASE}/backup Path to a volume generically used to export or backup data LOGS_DIR ${BASE}/logs Path to a volume generically used for logging PING_IDENTITY_ACCEPT_EULA NO Must be set to 'YES' for the container to start PING_IDENTITY_DEVOPS_FILE devops-secret File name for devops-creds passed as a Docker secret STAGING_MANIFEST ${BASE}/staging-manifest.txt Path to a manifest of files expected in the staging dir on first image startup CLEAN_STAGING_DIR false Whether to clean the staging dir when the image starts SECRETS_DIR /run/secrets Default path to the secrets TOPOLOGY_FILE ${STAGING_DIR}/topology.json Path to the topology file HOOKS_DIR ${STAGING_DIR}/hooks Path where all the hooks scripts are stored CONTAINER_ENV ${STAGING_DIR}/.env Environment Property file use to share variables between scripts in container SERVER_PROFILE_DIR /tmp/server-profile Path where the remote server profile is checked out or cloned before being staged prior to being applied on the runtime SERVER_PROFILE_URL A valid git HTTPS URL (not ssh) SERVER_PROFILE_URL_REDACT true When set to \"true\", the server profile git URL will not be printed to container output. SERVER_PROFILE_BRANCH A valid git branch (optional) SERVER_PROFILE_PATH The subdirectory in the git repo SERVER_PROFILE_UPDATE false Whether to update the server profile upon container restart SECURITY_CHECKS_STRICT false Requires strict checks on security SECURITY_CHECKS_FILENAME .jwk .pin Perform a check for filenames that may violate security (i.e. secret material) UNSAFE_CONTINUE_ON_ERROR If this is set to true, then the container will provide a hard warning and continue. LICENSE_DIR ${SERVER_ROOT_DIR} License directory PD_LICENSE_DIR ${STAGING_DIR}/pd.profile/server-root/pre-setup PD License directory. Separating from above LICENSE_DIR to differentiate for different products STARTUP_COMMAND The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container PING_IDENTITY_DEVOPS_KEY_REDACT true TAIL_LOG_FILES A whitespace separated list of log files to tail to the container standard output - DO NOT USE WILDCARDS like /path/to/logs/*.log COLORIZE_LOGS true If 'true', the output logs will be colorized with GREENs and REDs, otherwise, no colorization will be done.  This is good for tools that monitor logs and colorization gets in the way. LOCATION Docker Location default value If PingDirectory is deployed in multi cluster mode, that is, K8S_CLUSTER, K8S_CLUSTERS and K8S_SEED_CLUSTER are defined, LOCATION is ignored and K8S_CLUSTER is used as the location LOCATION_VALIDATION true Any string denoting a logical/physical location MAX_HEAP_SIZE 384m Heap size (for java products) JVM_TUNING AGGRESSIVE JAVA_RAM_PERCENTAGE 75.0 Percentage of the container memory to allocate to PingFederate JVM DO NOT set to 100% or your JVM will exit with OutOfMemory errors and the container will terminate VERBOSE false Triggers verbose messages in scripts using the set -x option. PING_DEBUG false Set the server in debug mode, with increased output PING_PRODUCT The name of Ping product, i.e. PingFederate, PingDirectory - must be a valid Ping product type. This variable should be overridden by child images. PING_PRODUCT_VALIDATION true i.e. PingFederate,PingDirectory ADDITIONAL_SETUP_ARGS List of setup arguments passed to Ping Data setup-arguments.txt file LDAP_PORT 1389 Port over which to communicate for LDAP LDAPS_PORT 1636 Port over which to communicate for LDAPS HTTPS_PORT 1443 Port over which to communicate for HTTPS JMX_PORT 1689 Port for monitoring over JMX protocol ORCHESTRATION_TYPE The type of orchestration tool used to run the container, normally set in the deployment (.yaml) file.  Expected values include: - compose - swarm - kubernetes Defaults to blank (i.e. No type is set) USER_BASE_DN dc=example,dc=com Base DN for user data DOLLAR '$' Variable with a literal value of '$', to avoid unwanted variable substitution PD_ENGINE_PUBLIC_HOSTNAME localhost PD (PingDirectory) public hostname that may be used in redirects PD_ENGINE_PRIVATE_HOSTNAME pingdirectory PD (PingDirectory) private hostname PDP_ENGINE_PUBLIC_HOSTNAME localhost PDP (PingDirectoryProxy) public hostname that may be used in redirects PDP_ENGINE_PRIVATE_HOSTNAME pingdirectoryproxy PDP (PingDirectoryProxy) private hostname PDS_ENGINE_PUBLIC_HOSTNAME localhost PDS (PingDataSync) public hostname that may be used in redirects PDS_ENGINE_PRIVATE_HOSTNAME pingdatasync PDS (PingDataSync) private hostname PAZ_ENGINE_PUBLIC_HOSTNAME localhost PAZ (PingAuthorize) public hostname that may be used in redirects PAZ_ENGINE_PRIVATE_HOSTNAME pingauthorize PAZ (PingAuthorize) private hostname PAZP_ENGINE_PUBLIC_HOSTNAME localhost PAZP (PingAuthorize-PAP) public hostname that may be used in redirects PAZP_ENGINE_PRIVATE_HOSTNAME pingauthorizepap PAZP (PingAuthorize-PAP) private hostname PF_ENGINE_PUBLIC_HOSTNAME localhost PF (PingFederate) engine public hostname that may be used in redirects PF_ENGINE_PRIVATE_HOSTNAME pingfederate PF (PingFederate) engine private hostname PF_ADMIN_PUBLIC_BASEURL https://localhost:9999 PF (PingFederate) admin public baseurl that may be used in redirects PF_ADMIN_PUBLIC_HOSTNAME localhost PF (PingFederate) admin public hostname that may be used in redirects PF_ADMIN_PRIVATE_HOSTNAME pingfederate-admin PF (PingFederate) admin private hostname PA_ENGINE_PUBLIC_HOSTNAME localhost PA (PingAccess) engine public hostname that may be used in redirects PA_ENGINE_PRIVATE_HOSTNAME pingaccess PA (PingAccess) engine private hostname PA_ADMIN_PUBLIC_HOSTNAME localhost PA (PingAccess) admin public hostname that may be used in redirects PA_ADMIN_PRIVATE_HOSTNAME pingaccess-admin PA (PingAccess) admin private hostname ROOT_USER_DN cn=${ROOT_USER} DN of the server root user ENV ${BASE}/.profile MOTD_URL https://raw.githubusercontent.com/pingidentity/pingidentity-devops-getting-started/master/motd/motd.json Instructs the image to pull the MOTD json from the following URL. If this MOTD_URL variable is empty, then no motd will be downloaded. The format of this MOTD file must match the example provided in the url: https://raw.githubusercontent.com/pingidentity/pingidentity-devops-getting-started/master/motd/motd.json PS1 \\${PING_PRODUCT}:\\h:\\w\\n&gt; Default shell prompt (i.e. productName:hostname:workingDir) PATH ${JAVA_HOME}/bin:${BASE}:${SERVER_ROOT_DIR}/bin:${PATH} PATH used by the container"},{"location":"docker-images/pingbase/#docker-container-hook-scripts","title":"Docker Container Hook Scripts","text":"<p>Please go here for details on all pingbase hook scripts</p> <p>This document is auto-generated from pingbase/Dockerfile</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingbase/hooks/","title":"Ping Identity DevOps <code>pingbase</code> Hooks","text":"<p>There are no default hooks defined for the <code>pingbase</code> image.</p> <p>Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image.</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcentral/","title":"Ping Identity DevOps Docker Image - <code>pingcentral</code>","text":"<p>This docker image includes the Ping Identity PingCentral product binaries and associated hook scripts to create and run PingCentral in a container.</p>"},{"location":"docker-images/pingcentral/#related-docker-images","title":"Related Docker Images","text":"<ul> <li><code>pingidentity/pingbase</code> - Parent Image <p>This image inherits, and can use, Environment Variables from pingidentity/pingbase</p> </li> <li><code>pingidentity/pingcommon</code> - Common Ping files (i.e. hook scripts)</li> </ul>"},{"location":"docker-images/pingcentral/#environment-variables","title":"Environment Variables","text":"<p>In addition to environment variables inherited from pingidentity/pingbase, the following environment <code>ENV</code> variables can be used with this image.</p> ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} DATE ${DATE} PING_PRODUCT_VERSION ${VERSION} PING_CENTRAL_SERVER_PORT 9022 PING_PRODUCT PingCentral Ping product name LICENSE_DIR ${SERVER_ROOT_DIR}/conf License directory LICENSE_FILE_NAME pingcentral.lic Name of license file LICENSE_SHORT_NAME PC Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh The command that the entrypoint will execute in the foreground to instantiate the container TAIL_LOG_FILES ${SERVER_ROOT_DIR}/log/application.log Files tailed once container has started PING_CENTRAL_LOG_LEVEL INFO PING_CENTRAL_BLIND_TRUST false PING_CENTRAL_VERIFY_HOSTNAME true"},{"location":"docker-images/pingcentral/#ports-exposed","title":"Ports Exposed","text":"<p>The following ports are exposed from the container.  If a variable is used, then it may come from a parent container</p> <ul> <li>9022</li> </ul>"},{"location":"docker-images/pingcentral/#running-a-pingcentral-container","title":"Running a PingCentral container","text":"<p>To run a PingCentral container with your devops configuration file: ```shell docker run -Pt \\            --name pingcentral \\            --env-file ~/.pingidentity/config \\            --env PING_IDENTITY_ACCEPT_EULA=YES \\            --env PING_IDENTITY_DEVOPS_USER \\            --env PING_IDENTITY_DEVOPS_KEY \\            --tmpfs /run/secrets \\            pingidentity/pingcentral:edge <pre><code>or with long options in the background:\n```shell\n  docker run \\\n           --name pingcentral \\\n           --publish 9022:9022 \\\n           --detach \\\n           --env-file ~/.pingidentity/config \\\n           --env PING_IDENTITY_ACCEPT_EULA=YES \\\n           --env PING_IDENTITY_DEVOPS_USER \\\n           --env PING_IDENTITY_DEVOPS_KEY \\\n           --tmpfs /run/secrets \\\n           pingidentity/pingcentral:edge\n</code></pre></p> <p>or if you want to specify everything yourself: <pre><code>  docker run \\\n           --name pingcentral \\\n           --publish 9022:9022 \\\n           --detach \\\n           --env PING_IDENTITY_ACCEPT_EULA=YES \\\n           --env PING_IDENTITY_DEVOPS_USER \\\n           --env PING_IDENTITY_DEVOPS_KEY \\\n           --tmpfs /run/secrets \\\n           pingidentity/pingcentral:edge\n</code></pre></p> <p>Follow Docker logs with:</p> <pre><code>docker logs -f pingcentral\n</code></pre> <p>If using the command above with the embedded server profile, log in with: * https://localhost:9022/   * Username: Administrator   * Password: 2Federate</p>"},{"location":"docker-images/pingcentral/#docker-container-hook-scripts","title":"Docker Container Hook Scripts","text":"<p>Please go here for details on all pingcentral hook scripts</p> <p>This document is auto-generated from pingcentral/Dockerfile</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcentral/hooks/","title":"Ping Identity DevOps <code>pingcentral</code> Hooks","text":"<p>There are no default hooks defined for the <code>pingcentral</code> image.</p> <p>Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image.</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/","title":"Ping Identity Docker Image - <code>pingcommon</code>","text":"<p>This docker image provides a busybox image to house the base hook scripts and default entrypoint.sh used throughout the Ping Identity DevOps product images.</p>"},{"location":"docker-images/pingcommon/#docker-container-hook-scripts","title":"Docker Container Hook Scripts","text":"<p>Please go here for details on all pingcommon hook scripts</p> <p>This document is auto-generated from pingcommon/Dockerfile</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/","title":"Ping Identity DevOps <code>pingcommon</code> Hooks","text":"<p>List of available hooks: * 01-start-server.sh * 02-get-remote-server-profile.sh * 03-build-run-plan.sh * 04-check-variables.sh * 05-expand-templates.sh * 06-copy-product-bits.sh * 07-apply-server-profile.sh * 09-build-motd.sh * 10-start-sequence.sh * 17-check-license.sh * 18-setup-sequence.sh * 20-restart-sequence.sh * 50-before-post-start.sh * 90-shutdown-sequence.sh * pingcommon.lib.sh * pingsecrets.lib.sh * pingstate.lib.sh</p> <p>These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon)</p> <p>This document is auto-generated from pingcommon/opt/staging/hooks</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/01-start-server.sh/","title":"Ping Identity DevOps <code>pingcommon</code> Hook - <code>01-start-server.sh</code>","text":"<p>This document is auto-generated from pingcommon/opt/staging/hooks/01-start-server.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/02-get-remote-server-profile.sh/","title":"Ping Identity DevOps <code>pingcommon</code> Hook - <code>02-get-remote-server-profile.sh</code>","text":"<p>This hook will get bits from a git repo based on SERVER_PROFILE_* variables  passed to the container.  If no SERVER_PROFILES are passed, then nothing will  occur when running this hook.  These bits will be placed into the STAGING_DIR location (defaults to  ${BASE_DIR}/staging).  Server Profiles may be layered to copy in profiles from a parent/ancestor server  profile.  An example might be a layer of profiles that look like:  - Dev Environment Configs (DEV_CONFIG)    - Dev Certificates (DEV_CERT)      - Base Configs (BASE)  This would result in a set of SERVER_PROFILE variables that looks like:  - SERVER_PROFILE_URL=...git url of DEV_CONFIG...  - SERVER_PROFILE_PARENT=DEV_CERT  - SERVER_PROFILE_DEV_CERT_URL=...git url of DEV_CERT...  - SERVER_PROFILE_DEV_CERT_PARENT=BASE  - SERVER_PROFILE_BASE_URL=...git url of BASE...  In this example, the bits for BASE would be pulled, followed by DEV_CERT, followed  by DEV_CONFIG  If other source maintenance repositories are used (i.e. bitbucket, s3, ...)  then this hook could be overridden by a different hook</p> <p>This document is auto-generated from pingcommon/opt/staging/hooks/02-get-remote-server-profile.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/03-build-run-plan.sh/","title":"Ping Identity DevOps <code>pingcommon</code> Hook - <code>03-build-run-plan.sh</code>","text":"<p>This script will building a run plan for the server as it starts up  Options for the RUN_PLAN and the PD_STATE are as follows:  RUN_PLAN (Initially set to UNKNOWN)           START   - Instructs the container to start from scratch.  This is primarily                     because a STARTUP_COMMAND (i.e. /opt/out/instance/bin/run.sh) isn't present.           RESTART - Instructs the container to restart.  This is primarily because the                     STARTUP_COMMAND (i.e. /opt/out/instance/bin/run.sh) is present and typically                     signifies that the server bits have been copied and run before</p> <p>NOTE: It will be common for products to override this hook to provide RUN_PLAN directions based on product specifics.</p> <p>This document is auto-generated from pingcommon/opt/staging/hooks/03-build-run-plan.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/04-check-variables.sh/","title":"Ping Identity DevOps <code>pingcommon</code> Hook - <code>04-check-variables.sh</code>","text":"<p>This document is auto-generated from pingcommon/opt/staging/hooks/04-check-variables.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/05-expand-templates.sh/","title":"Ping Identity DevOps <code>pingcommon</code> Hook - <code>05-expand-templates.sh</code>","text":"<p>Using the envsubst command, this will look through any files in the  STAGING_DIR that end in <code>.subst</code> or <code>.subst.default</code>  and substitute any variables the files with the the value of those  variables, if the variable is set.  Variables may come from (in order of precedence):   - The '.env' file from the profiles and intra container env variables   - The environment variables or env-file passed to container on startup   - The container's os  If a .zip file ends with <code>.zip.subst</code> (especially useful for pingfederate  for example with data.zip) then:   - file will be unzipped   - any files ending in <code>.subst</code> will be processed to substitute variables   - zipped back up in to the same file without the <code>.subst</code> suffix  If a file ends with <code>.subst.default</code> (intended to only be expanded as a  default if the file is not found) then it will be substituted:   - If the RUN_PLAN==START and the file is not found in staging   - If the RUN_PLAN==RESTART and the file is found in staging or the OUT_DIR</p> <p>Note: If a string of $name should be ignored during a substitution, then  A special variable ${DOLLAR} should be used.  This is not required any longer  and deprecated, but available for any older server-profile versions. Example: ${DOLLAR}{username} ==&gt; ${username}</p> <p>This document is auto-generated from pingcommon/opt/staging/hooks/05-expand-templates.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/06-copy-product-bits.sh/","title":"Ping Identity DevOps <code>pingcommon</code> Hook - <code>06-copy-product-bits.sh</code>","text":"<p>Copies the server bits from the image into the SERVER_ROOT_DIR if  it is a new fresh container.</p> <p>This document is auto-generated from pingcommon/opt/staging/hooks/06-copy-product-bits.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/07-apply-server-profile.sh/","title":"Ping Identity DevOps <code>pingcommon</code> Hook - <code>07-apply-server-profile.sh</code>","text":"<p>The server-profiles from:  * remote (i.e. git) and  * local (i.e. /opt/in)  have been merged into the ${STAGING_DIR}/instance (ie. /opt/staging/instance).  This is a candidate to be installed or overwritten into the ${SERVER_ROOT_DIR}  if one of the following items are true:  * Start of a new server (i.e. RUN_PLAN=START)  * Restart of a server with SERVER_PROFILE_UPDATE==true  To force the overwrite of files on a restart, ensure that the variable:      SERVER_PROFILE_UPDATE=true  is passed.</p> <p>This document is auto-generated from pingcommon/opt/staging/hooks/07-apply-server-profile.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/09-build-motd.sh/","title":"Ping Identity DevOps <code>pingcommon</code> Hook - <code>09-build-motd.sh</code>","text":"<p>Creates a message of the day (MOTD) file based on information provided by:  * Docker Variables  * Github MOTD file from PingIdentity Devops Repo  * Server-Profile motd file</p> <p>This document is auto-generated from pingcommon/opt/staging/hooks/09-build-motd.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/10-start-sequence.sh/","title":"Ping Identity DevOps <code>pingcommon</code> Hook - <code>10-start-sequence.sh</code>","text":"<p>This document is auto-generated from pingcommon/opt/staging/hooks/10-start-sequence.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/17-check-license.sh/","title":"Ping Identity DevOps <code>pingcommon</code> Hook - <code>17-check-license.sh</code>","text":"<p>Check for license file  - If LICENSE_FILE found make call to check-license api unless MUTE_LICENSE_VERIFICATION set to true  - If LICENSE_FILE not found and PING_IDENTITY_DEVOPS_USER and PING_IDENTITY_DEVOPS_KEY defined    make call to obtain a license from license server</p> <p>This document is auto-generated from pingcommon/opt/staging/hooks/17-check-license.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/18-setup-sequence.sh/","title":"Ping Identity DevOps <code>pingcommon</code> Hook - <code>18-setup-sequence.sh</code>","text":"<p>This hook may be used to set the server if there is a setup procedure</p> <p>Note: The PingData (i.e. Directory, DataSync, PingAuthorize, DirectoryProxy)  products will all provide this</p> <p>This document is auto-generated from pingcommon/opt/staging/hooks/18-setup-sequence.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/20-restart-sequence.sh/","title":"Ping Identity DevOps <code>pingcommon</code> Hook - <code>20-restart-sequence.sh</code>","text":"<p>This hook is called when the container has been built in a prior startup  and a configuration has been found.</p> <p>This document is auto-generated from pingcommon/opt/staging/hooks/20-restart-sequence.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/50-before-post-start.sh/","title":"Ping Identity DevOps <code>pingcommon</code> Hook - <code>50-before-post-start.sh</code>","text":"<p>This is called after the start or restart sequence has finished and before  the server within the container starts</p> <p>This document is auto-generated from pingcommon/opt/staging/hooks/50-before-post-start.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/90-shutdown-sequence.sh/","title":"Ping Identity DevOps <code>pingcommon</code> Hook - <code>90-shutdown-sequence.sh</code>","text":"<p>This script may be implemented to gracefully shutdown the container</p> <p>Note: this is most useful in Kubernetes but can be called arbitrarily by  by control/config frameworks</p> <p>This document is auto-generated from pingcommon/opt/staging/hooks/90-shutdown-sequence.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/pingcommon.lib.sh/","title":"Ping Identity DevOps <code>pingcommon</code> Hook - <code>pingcommon.lib.sh</code>","text":"<p>This document is auto-generated from pingcommon/opt/staging/hooks/pingcommon.lib.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/pingsecrets.lib.sh/","title":"Ping Identity DevOps <code>pingcommon</code> Hook - <code>pingsecrets.lib.sh</code>","text":"<p>This document is auto-generated from pingcommon/opt/staging/hooks/pingsecrets.lib.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingcommon/hooks/pingstate.lib.sh/","title":"Ping Identity DevOps <code>pingcommon</code> Hook - <code>pingstate.lib.sh</code>","text":"<p>This document is auto-generated from pingcommon/opt/staging/hooks/pingstate.lib.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdatacommon/","title":"Ping Identity Docker Image - <code>pingdatacommon</code>","text":"<p>This docker image provides a busybox image based off of <code>pingidentity/pingcommon</code> to house the base hook scripts used throughout the Ping Identity DevOps PingData product images.</p>"},{"location":"docker-images/pingdatacommon/#related-docker-images","title":"Related Docker Images","text":"<ul> <li><code>pingidentity/pingcommon</code> - Parent Image</li> </ul>"},{"location":"docker-images/pingdatacommon/#environment-variables","title":"Environment Variables","text":"<p>The following environment <code>ENV</code> variables can be used with this image.</p> ENV Variable Default Description REGENERATE_JAVA_PROPERTIES false Flag to force a run of dsjavaproperties --initialize. When this is false, the java.properties file will only be regenerated on a restart when there is a change in JVM or a change in the product-specific java options, such as changing the MAX_HEAP_SIZE value."},{"location":"docker-images/pingdatacommon/#docker-container-hook-scripts","title":"Docker Container Hook Scripts","text":"<p>Please go here for details on all pingdatacommon hook scripts</p> <p>This document is auto-generated from pingdatacommon/Dockerfile</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdatacommon/hooks/","title":"Ping Identity DevOps <code>pingdatacommon</code> Hooks","text":"<p>List of available hooks: * 03-build-run-plan.sh * 18-setup-sequence.sh * 181-install-extensions.sh * 183-run-setup.sh * 185-apply-tools-properties.sh * 20-restart-sequence.sh * pingdata.lib.sh</p> <p>These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon)</p> <p>This document is auto-generated from pingdatacommon/opt/staging/hooks</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdatacommon/hooks/03-build-run-plan.sh/","title":"Ping Identity DevOps <code>pingdatacommon</code> Hook - <code>03-build-run-plan.sh</code>","text":"<p>This script is called to check if there is an existing server  and if so, it will return a 1, else 0</p> <p>This document is auto-generated from pingdatacommon/opt/staging/hooks/03-build-run-plan.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdatacommon/hooks/18-setup-sequence.sh/","title":"Ping Identity DevOps <code>pingdatacommon</code> Hook - <code>18-setup-sequence.sh</code>","text":"<p>Quarterbacks all the scripts associated with the setup of a  PingData product</p> <p>This document is auto-generated from pingdatacommon/opt/staging/hooks/18-setup-sequence.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdatacommon/hooks/181-install-extensions.sh/","title":"Ping Identity DevOps <code>pingdatacommon</code> Hook - <code>181-install-extensions.sh</code>","text":"<p>This document is auto-generated from pingdatacommon/opt/staging/hooks/181-install-extensions.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdatacommon/hooks/183-run-setup.sh/","title":"Ping Identity DevOps <code>pingdatacommon</code> Hook - <code>183-run-setup.sh</code>","text":"<p>This document is auto-generated from pingdatacommon/opt/staging/hooks/183-run-setup.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdatacommon/hooks/185-apply-tools-properties.sh/","title":"Ping Identity DevOps <code>pingdatacommon</code> Hook - <code>185-apply-tools-properties.sh</code>","text":"<p>This document is auto-generated from pingdatacommon/opt/staging/hooks/185-apply-tools-properties.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdatacommon/hooks/20-restart-sequence.sh/","title":"Ping Identity DevOps <code>pingdatacommon</code> Hook - <code>20-restart-sequence.sh</code>","text":"<p>This hook is called when the container has been built in a prior startup  and a configuration has been found.</p> <p>This document is auto-generated from pingdatacommon/opt/staging/hooks/20-restart-sequence.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdatacommon/hooks/pingdata.lib.sh/","title":"Ping Identity DevOps <code>pingdatacommon</code> Hook - <code>pingdata.lib.sh</code>","text":"<p>This document is auto-generated from pingdatacommon/opt/staging/hooks/pingdata.lib.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdataconsole/","title":"Ping Identity Docker Image - <code>pingdataconsole</code>","text":"<p>This docker image provides a tomcat image with the PingDataConsole deployed to be used in configuration of the PingData products.</p>"},{"location":"docker-images/pingdataconsole/#related-docker-images","title":"Related Docker Images","text":"<ul> <li><code>tomcat:9-jre8</code> - Tomcat engine to serve PingDataConsole .war file</li> </ul>"},{"location":"docker-images/pingdataconsole/#environment-variables","title":"Environment Variables","text":"<p>The following environment <code>ENV</code> variables can be used with this image.</p> ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} DATE ${DATE} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingDataConsole Ping product name HTTP_PORT 8080 PingDataConsole HTTP listen port HTTPS_PORT 8443 PingDataConsole HTTPS listen port STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/catalina.sh The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS run The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS start The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/console.log Files tailed once container has started"},{"location":"docker-images/pingdataconsole/#run","title":"Run","text":"<p>To run a PingDataConsole container:</p> <pre><code>  docker run \\\n           --name pingdataconsole \\\n           --publish ${HTTPS_PORT}:${HTTPS_PORT} \\\n           --detach \\\n           --env PING_IDENTITY_ACCEPT_EULA=YES \\\n           --env PING_IDENTITY_DEVOPS_USER \\\n           --env PING_IDENTITY_DEVOPS_KEY \\\n           --tmpfs /run/secrets \\\n           pingidentity/pingdataconsole:edge\n</code></pre> <p>Follow Docker logs with:</p> <pre><code>docker logs -f pingdataconsole\n</code></pre> <p>If using the command above with the embedded server profile, log in with: * http://localhost:${HTTPS_PORT}/console/login <pre><code>Server: pingdirectory:1636\nUsername: administrator\nPassword: 2FederateM0re\n</code></pre></p> <p>make sure you have a PingDirectory running</p>"},{"location":"docker-images/pingdataconsole/#docker-container-hook-scripts","title":"Docker Container Hook Scripts","text":"<p>Please go here for details on all pingdataconsole hook scripts</p> <p>This document is auto-generated from pingdataconsole/Dockerfile</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdataconsole/hooks/","title":"Ping Identity DevOps <code>pingdataconsole</code> Hooks","text":"<p>List of available hooks: * 02-get-remote-server-profile.sh.post * 04-check-variables.sh * 17-check-license.sh</p> <p>These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon)</p> <p>This document is auto-generated from pingdataconsole/opt/staging/hooks</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdataconsole/hooks/02-get-remote-server-profile.sh.post/","title":"Ping Identity DevOps <code>pingdataconsole</code> Hook - <code>02-get-remote-server-profile.sh.post</code>","text":"<p>This hook provides final steps to setup Ping Data Console.</p> <p>This document is auto-generated from pingdataconsole/opt/staging/hooks/02-get-remote-server-profile.sh.post</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdataconsole/hooks/04-check-variables.sh/","title":"Ping Identity DevOps <code>pingdataconsole</code> Hook - <code>04-check-variables.sh</code>","text":"<p>This document is auto-generated from pingdataconsole/opt/staging/hooks/04-check-variables.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdataconsole/hooks/17-check-license.sh/","title":"Ping Identity DevOps <code>pingdataconsole</code> Hook - <code>17-check-license.sh</code>","text":"<p>This document is auto-generated from pingdataconsole/opt/staging/hooks/17-check-license.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdatasync/","title":"Ping Identity DevOps Docker Image - <code>pingdatasync</code>","text":"<p>This docker image includes the Ping Identity PingDataSync product binaries and associated hook scripts to create and run a PingDataSync instance.</p>"},{"location":"docker-images/pingdatasync/#related-docker-images","title":"Related Docker Images","text":"<ul> <li><code>pingidentity/pingbase</code> - Parent Image <p>This image inherits, and can use, Environment Variables from pingidentity/pingbase</p> </li> <li><code>pingidentity/pingdatacommon</code> - Common Ping files (i.e. hook scripts)</li> </ul>"},{"location":"docker-images/pingdatasync/#environment-variables","title":"Environment Variables","text":"<p>In addition to environment variables inherited from pingidentity/pingbase, the following environment <code>ENV</code> variables can be used with this image.</p> ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} DATE ${DATE} PING_PRODUCT_VERSION ${VERSION} TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/sync Files tailed once container has started LICENSE_DIR ${PD_LICENSE_DIR} PD License directory. This value is set from the pingbase docker file LICENSE_FILE_NAME PingDirectory.lic Name of license file LICENSE_SHORT_NAME PD Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server PING_PRODUCT PingDataSync Ping product name STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container RETRY_TIMEOUT_SECONDS 180 The default retry timeout in seconds for manage-topology and remove-defunct-server ADMIN_USER_NAME admin Failover administrative user ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to /SECRETS_DIR/root-user-password ADMIN_USER_PASSWORD_FILE Location of file with the admin password, used as the password replication admin Defaults to /SECRETS_DIR/admin-user-password KEYSTORE_FILE Location of the keystore file containing the server certificate. If left undefined, the SECRETS_DIR will be checked for a keystore. If that keystore does not exist, the server will generate a self-signed certificate. KEYSTORE_PIN_FILE Location of the pin file for the keystore defined in KEYSTORE_FILE. You must specify a KEYSTORE_PIN_FILE when a KEYSTORE_FILE is present. This value does not need to be defined when allowing the server to generate a self-signed certificate. KEYSTORE_TYPE Format of the keystore defined in KEYSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the keystore format will be inferred based on the file extension of the KEYSTORE_FILE, defaulting to \"jks\". TRUSTSTORE_FILE Location of the truststore file for the server. If left undefined, the SECRETS_DIR will be checked for a truststore. If that truststore does not exist, the server will generate a truststore, containing its own certificate. TRUSTSTORE_PIN_FILE Location of the pin file for the truststore defined in TRUSTSTORE_FILE. You must specify a TRUSTSTORE_PIN_FILE when a TRUSTSTORE_FILE is present. This value does not need to be defined when allowing the server to generate a truststore. TRUSTSTORE_TYPE Format of the truststore defined in TRUSTSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the truststore format will be inferred based on the file extension of the TRUSTSTORE_FILE, defaulting to \"jks\". PD_PROFILE ${STAGING_DIR}/pd.profile Directory for the profile used by the PingData manage-profile tool UNBOUNDID_SKIP_START_PRECHECK_NODETACH true Setting this variable to true speeds up server startup time by skipping an unnecessary JVM check. PARALLEL_POD_MANAGEMENT_POLICY false Whether this container is running as a Pod in a Kubernetes StatefulSet, and that StatefulSet is using the Parallel podManagementPolicy. This property allows for starting up Pods in parallel to speed up the initial startup of PingDataSync topologies. This variable must be set to true when using the Parallel podManagementPolicy. Note: when using parallel startup, ensure the RETRY_TIMEOUT_SECONDS variable is large enough. The pods will be enabling replication simultaneously, so some pods will have to retry while waiting for others to complete. If the timeout is too low, a Pod may end up restarting unnecessarily. SKIP_WAIT_FOR_DNS false Set to true to skip the waiting for DNS step that is normally done just before attempting to join the topology. CERTIFICATE_NICKNAME There is an additional certificate-based variable used to identity the certificate alias used within the <code>KEYSTORE_FILE</code>. That variable is called <code>CERTIFICATE_NICKNAME</code>, which identifies the certificate to use by the server in the <code>KEYSTORE_FILE</code>. If a value is not provided, the container will look at the list certs found in the <code>KEYSTORE_FILE</code> and if one - and only one - certificate is found of type <code>PrivateKeyEntry</code>, that alias will be used. COLUMNS 120 Sets the number of columns in PingDataSync command-line tool output PD_REBUILD_ON_RESTART false Force a rebuild (replace-profile) of PingDataSync on restart. Used to ensure that the server configuration exactly matches the server profile. This variable will slow down startup times and should only be used when necessary."},{"location":"docker-images/pingdatasync/#ports-exposed","title":"Ports Exposed","text":"<p>The following ports are exposed from the container.  If a variable is used, then it may come from a parent container</p> <ul> <li>${LDAP_PORT}</li> <li>${LDAPS_PORT}</li> <li>${HTTPS_PORT}</li> <li>${JMX_PORT}</li> </ul>"},{"location":"docker-images/pingdatasync/#running-a-pingdatasync-container","title":"Running a PingDataSync container","text":"<pre><code>  docker run \\\n           --name pingdatasync \\\n           --publish 1389:1389 \\\n           --publish 8443:1443 \\\n           --detach \\\n           --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\\n           --env SERVER_PROFILE_PATH=simple-sync/pingdatasync \\\n           --env PING_IDENTITY_ACCEPT_EULA=YES \\\n           --env PING_IDENTITY_DEVOPS_USER \\\n           --env PING_IDENTITY_DEVOPS_KEY \\\n           --tmpfs /run/secrets \\\n           pingidentity/pingdatasync:edge\n</code></pre>"},{"location":"docker-images/pingdatasync/#docker-container-hook-scripts","title":"Docker Container Hook Scripts","text":"<p>Please go here for details on all pingdatasync hook scripts</p> <p>This document is auto-generated from pingdatasync/Dockerfile</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdatasync/hooks/","title":"Ping Identity DevOps <code>pingdatasync</code> Hooks","text":"<p>List of available hooks: * 03-build-run-plan.sh * 20-restart-sequence.sh * 80-post-start.sh * 90-shutdown-sequence.sh</p> <p>These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon)</p> <p>This document is auto-generated from pingdatasync/opt/staging/hooks</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdatasync/hooks/03-build-run-plan.sh/","title":"Ping Identity DevOps <code>pingdatasync</code> Hook - <code>03-build-run-plan.sh</code>","text":"<p>This script is called to determine the plan for the server as it starts up.</p> <p>This document is auto-generated from pingdatasync/opt/staging/hooks/03-build-run-plan.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdatasync/hooks/20-restart-sequence.sh/","title":"Ping Identity DevOps <code>pingdatasync</code> Hook - <code>20-restart-sequence.sh</code>","text":"<p>This hook is called when the container has been built in a prior startup  and a configuration has been found.</p> <p>This document is auto-generated from pingdatasync/opt/staging/hooks/20-restart-sequence.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdatasync/hooks/80-post-start.sh/","title":"Ping Identity DevOps <code>pingdatasync</code> Hook - <code>80-post-start.sh</code>","text":"<p>This script is mostly the same as the 80-post-start.sh hook in the  * Enabling PingDataSync failover</p> <p>This document is auto-generated from pingdatasync/opt/staging/hooks/80-post-start.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdatasync/hooks/90-shutdown-sequence.sh/","title":"Ping Identity DevOps <code>pingdatasync</code> Hook - <code>90-shutdown-sequence.sh</code>","text":"<p>This script handles removing the server from the topology during a shutdown.</p> <p>This document is auto-generated from pingdatasync/opt/staging/hooks/90-shutdown-sequence.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdelegator/","title":"Ping Identity Docker Image - <code>pingdelegator</code>","text":"<p>This docker image provides an NGINX instance with PingDelegator that can be used in administering PingDirectory Users/Groups.</p>"},{"location":"docker-images/pingdelegator/#related-docker-images","title":"Related Docker Images","text":"<ul> <li><code>pingidentity/pingbase</code> - Parent Image <p>This image inherits, and can use, Environment Variables from pingidentity/pingbase</p> </li> <li><code>pingidentity/pingcommon</code> - Common Ping files (i.e. hook scripts)</li> </ul>"},{"location":"docker-images/pingdelegator/#environment-variables","title":"Environment Variables","text":"<p>In addition to environment variables inherited from pingidentity/pingbase, the following environment <code>ENV</code> variables can be used with this image.</p> ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} DATE ${DATE} PING_PRODUCT_VERSION ${VERSION} PD_DELEGATOR_PUBLIC_HOSTNAME localhost PD_DELEGATOR_HTTP_PORT 6080 PD_DELEGATOR_HTTPS_PORT 6443 PING_PRODUCT PingDelegator Ping product name PF_ENGINE_PUBLIC_HOSTNAME localhost The hostname for the public Ping Federate instance used for SSO. PF_ENGINE_PUBLIC_PORT 9031 The port for the public Ping Federate instance used for SSO. NOTE: If using port 443 along with a base URL with no specified port, set to an empty string. PF_DELEGATOR_CLIENTID dadmin The client id that was set up with Ping Federate for Ping Delegator. PD_ENGINE_PUBLIC_HOSTNAME localhost The hostname for the DS instance the app will be interfacing with. PD_ENGINE_PUBLIC_PORT 1443 The HTTPS port for the DS instance the app will be interfacing with. PD_DELEGATOR_TIMEOUT_LENGTH_MINS 30 The length of time (in minutes) until the session will require a new login attempt PD_DELEGATOR_HEADER_BAR_LOGO The filename used as the logo in the header bar, relative to this application's build directory. Note about logos: The size of the image will be scaled down to fit 22px of height and a max-width of 150px. For best results, it is advised to make the image close to this height and width ratio as well as to crop out any blank spacing around the logo to maximize its presentation. e.g. '${SERVER_ROOT_DIR}/html/delegator/images/my_company_logo.png' PD_DELEGATOR_DADMIN_API_NAMESPACE The namespace for the Delegated Admin API on the DS instance. In most cases, this does not need to be set here. e.g. 'dadmin/v2' PD_DELEGATOR_PROFILE_SCOPE_ENABLED false Set to true if the \"profile\" scope is supported for the Delegated Admin OIDC client on PingFederate and you wish to use it to show the current user's name in the navigation. NGINX_WORKER_PROCESSES auto The number of NginX worker processes -- Default: auto NGINX_WORKER_CONNECTIONS 1024 The number of NginX worker connections -- Default: 1024 STARTUP_COMMAND nginx The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS -c ${SERVER_ROOT_DIR}/etc/nginx.conf The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS ${STARTUP_FOREGROUND_OPTS} The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container"},{"location":"docker-images/pingdelegator/#run","title":"Run","text":"<p>To run a PingDelegator container with HTTPS_PORT=6443 (6443 is simply a convention for PingDelegator so conflicts are reduced with other container HTTPS ports):</p> <pre><code>  docker run \\\n           --name pingdelegator \\\n           --publish 6443:6443 \\\n           --detach \\\n           --env PING_IDENTITY_ACCEPT_EULA=YES \\\n           --env PING_IDENTITY_DEVOPS_USER \\\n           --env PING_IDENTITY_DEVOPS_KEY \\\n           --tmpfs /run/secrets \\\n           pingidentity/pingdelegator:edge\n</code></pre> <p>PingDelegator requires running instances of PingFederate/PingDirectory.</p>"},{"location":"docker-images/pingdelegator/#docker-container-hook-scripts","title":"Docker Container Hook Scripts","text":"<p>Please go here for details on all pingdelegator hook scripts</p> <p>This document is auto-generated from pingdelegator/Dockerfile</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdelegator/hooks/","title":"Ping Identity DevOps <code>pingdelegator</code> Hooks","text":"<p>List of available hooks: * 02-get-remote-server-profile.sh.post * 04-check-variables.sh * 17-check-license.sh</p> <p>These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon)</p> <p>This document is auto-generated from pingdelegator/opt/staging/hooks</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdelegator/hooks/02-get-remote-server-profile.sh.post/","title":"Ping Identity DevOps <code>pingdelegator</code> Hook - <code>02-get-remote-server-profile.sh.post</code>","text":"<p>This hook may be used to set the server if there is a setup procedure</p> <p>Note: The PingData (i.e. Directory, DataSync, PingAuthorize, DirectoryProxy)  products will all provide this</p> <p>This document is auto-generated from pingdelegator/opt/staging/hooks/02-get-remote-server-profile.sh.post</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdelegator/hooks/04-check-variables.sh/","title":"Ping Identity DevOps <code>pingdelegator</code> Hook - <code>04-check-variables.sh</code>","text":"<p>This document is auto-generated from pingdelegator/opt/staging/hooks/04-check-variables.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdelegator/hooks/17-check-license.sh/","title":"Ping Identity DevOps <code>pingdelegator</code> Hook - <code>17-check-license.sh</code>","text":"<p>This document is auto-generated from pingdelegator/opt/staging/hooks/17-check-license.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdirectory/","title":"Ping Identity DevOps Docker Image - <code>pingdirectory</code>","text":"<p>This docker image includes the Ping Identity PingDirectory product binaries and associated hook scripts to create and run a PingDirectory instance or instances.</p>"},{"location":"docker-images/pingdirectory/#related-docker-images","title":"Related Docker Images","text":"<ul> <li><code>pingidentity/pingbase</code> - Parent Image <p>This image inherits, and can use, Environment Variables from pingidentity/pingbase</p> </li> <li><code>pingidentity/pingdatacommon</code> - Common Ping files (i.e. hook scripts)</li> </ul>"},{"location":"docker-images/pingdirectory/#environment-variables","title":"Environment Variables","text":"<p>In addition to environment variables inherited from pingidentity/pingbase, the following environment <code>ENV</code> variables can be used with this image.</p> ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} DATE ${DATE} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingDirectory Ping product name LICENSE_DIR ${PD_LICENSE_DIR} PD License directory. This value is set from the pingbase docker file LICENSE_FILE_NAME PingDirectory.lic Name of license File LICENSE_SHORT_NAME PD Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server REPLICATION_PORT 8989 Default PingDirectory Replication Port ADMIN_USER_NAME admin Replication administrative user STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container PD_DELEGATOR_PUBLIC_HOSTNAME localhost Public hostname of the DA app STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to /SECRETS_DIR/root-user-password ADMIN_USER_PASSWORD_FILE Location of file with the admin password, used as the password replication admin Defaults to /SECRETS_DIR/admin-user-password ENCRYPTION_PASSWORD_FILE Location of file with the passphrase for setting up encryption Defaults to /SECRETS_DIR/encryption-password KEYSTORE_FILE Location of the keystore file containing the server certificate. If left undefined, the SECRETS_DIR will be checked for a keystore. If that keystore does not exist, the server will generate a self-signed certificate. KEYSTORE_PIN_FILE Location of the pin file for the keystore defined in KEYSTORE_FILE. You must specify a KEYSTORE_PIN_FILE when a KEYSTORE_FILE is present. This value does not need to be defined when allowing the server to generate a self-signed certificate. KEYSTORE_TYPE Format of the keystore defined in KEYSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the keystore format will be inferred based on the file extension of the KEYSTORE_FILE, defaulting to \"jks\". TRUSTSTORE_FILE Location of the truststore file for the server. If left undefined, the SECRETS_DIR will be checked for a truststore. If that truststore does not exist, the server will generate a truststore, containing its own certificate. TRUSTSTORE_PIN_FILE Location of the pin file for the truststore defined in TRUSTSTORE_FILE. You must specify a TRUSTSTORE_PIN_FILE when a TRUSTSTORE_FILE is present. This value does not need to be defined when allowing the server to generate a truststore. TRUSTSTORE_TYPE Format of the truststore defined in TRUSTSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the truststore format will be inferred based on the file extension of the TRUSTSTORE_FILE, defaulting to \"jks\". TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/access ${SERVER_ROOT_DIR}/logs/errors ${SERVER_ROOT_DIR}/logs/failed-ops ${SERVER_ROOT_DIR}/logs/config-audit.log ${SERVER_ROOT_DIR}/logs/debug-trace ${SERVER_ROOT_DIR}/logs/debug-aci ${SERVER_ROOT_DIR}/logs/tools/.log ${SERVER_BITS_DIR}/logs/tools/.log Files tailed once container has started MAKELDIF_USERS 0 Number of users to auto-populate using make-ldif templates RETRY_TIMEOUT_SECONDS 180 The default retry timeout in seconds for dsreplication and remove-defunct-server PD_PROFILE ${STAGING_DIR}/pd.profile Directory for the profile used by the PingData manage-profile tool FIPS_MODE_ON false Turns on FIPS mode (currently with the Bouncy Castle FIPS provider) set to exactly \"true\" lowercase to turn on set to anything else to turn off FIPS_PROVIDER BCFIPS BCFIPS is the only provider currently supported -- do not edit PD_REBUILD_ON_RESTART false Force a rebuild (replace-profile) of a PingDirectoy on restart. Used to ensure that the server configuration exactly matches the server profile. This variable will slow down startup times and should only be used when necessary. UNBOUNDID_SKIP_START_PRECHECK_NODETACH true Setting this variable to true speeds up server startup time by skipping an unnecessary JVM check. REPLICATION_BASE_DNS Base DNs to include when enabling replication, in addition to the always-included USER_BASE_DN. Multiple base DNs can be specified here, separated by a <code>;</code> character RESTRICTED_BASE_DNS Base DNs to set as --restricted when enabling replication. Multiple base DNs can be specified here, separated by a <code>;</code> character. See the product documentation for more information on how to configure entry balancing. PARALLEL_POD_MANAGEMENT_POLICY false Whether this container is running as a Pod in a Kubernetes StatefulSet, and that StatefulSet is using the Parallel podManagementPolicy. This property allows for starting up Pods in parallel to speed up the initial startup of PingDirectory topologies. This variable must be set to true when using the Parallel podManagementPolicy. Note: when using parallel startup, ensure the RETRY_TIMEOUT_SECONDS variable is large enough. The pods will be enabling replication simultaneously, so some pods will have to retry while waiting for others to complete. If the timeout is too low, a Pod may end up restarting unnecessarily. SKIP_WAIT_FOR_DNS false Set to true to skip the waiting for DNS step that is normally done just before attempting to join the topology. CERTIFICATE_NICKNAME There is an additional certificate-based variable used to identity the certificate alias used within the <code>KEYSTORE_FILE</code>. That variable is called <code>CERTIFICATE_NICKNAME</code>, which identifies the certificate to use by the server in the <code>KEYSTORE_FILE</code>. If a value is not provided, the container will look at the list certs found in the <code>KEYSTORE_FILE</code> and if one - and only one - certificate is found of type <code>PrivateKeyEntry</code>, that alias will be used. PD_FORCE_DATA_REIMPORT false Set to true to force PingDirectory to export and re-import its backend data on restart. Note that this process can take a long time for large backends. LOAD_BALANCING_ALGORITHM_NAMES The load-balancing algorithm names to set for this server instance. This variable is only needed when enabling automatic server discovery with PingDirectoryProxy. Multiple algorithms can be specified here, separated by a <code>;</code> character COLUMNS 120 Sets the number of columns in PingDirectory command-line tool output"},{"location":"docker-images/pingdirectory/#ports-exposed","title":"Ports Exposed","text":"<p>The following ports are exposed from the container.  If a variable is used, then it may come from a parent container</p> <ul> <li>${LDAP_PORT}</li> <li>${LDAPS_PORT}</li> <li>${HTTPS_PORT}</li> <li>${JMX_PORT}</li> </ul>"},{"location":"docker-images/pingdirectory/#running-a-pingdirectory-container","title":"Running a PingDirectory container","text":"<p>The easiest way to test test a simple standalone image of PingDirectory is to cut/paste the following command into a terminal on a machine with docker.</p> <pre><code>  docker run \\\n           --name pingdirectory \\\n           --publish 1389:1389 \\\n           --publish 8443:1443 \\\n           --detach \\\n           --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\\n           --env SERVER_PROFILE_PATH=getting-started/pingdirectory \\\n           --env PING_IDENTITY_ACCEPT_EULA=YES \\\n           --env PING_IDENTITY_DEVOPS_USER \\\n           --env PING_IDENTITY_DEVOPS_KEY \\\n           --tmpfs /run/secrets \\\n           pingidentity/pingdirectory:edge\n</code></pre> <p>You can view the Docker logs with the command:</p> <pre><code>  docker logs -f pingdirectory\n</code></pre> <p>You should see the ouptut from a PingDirectory install and configuration, ending with a message the the PingDirectory has started.  After it starts, you will see some typical access logs.  Simply <code>Ctrl-C</code> after to stop tailing the logs.</p>"},{"location":"docker-images/pingdirectory/#running-a-sample-100sec-search-rate-test","title":"Running a sample 100/sec search rate test","text":"<p>With the PingDirectory running from the previous section, you can run a <code>searchrate</code> job that will send load to the directory at a rate if 100/sec using the following command.</p> <pre><code>docker exec -it pingdirectory \\\n        /opt/out/instance/bin/searchrate \\\n                -b dc=example,dc=com \\\n                --scope sub \\\n                --filter \"(uid=user.[1-9])\" \\\n                --attribute mail \\\n                --numThreads 2 \\\n                --ratePerSecond 100\n</code></pre>"},{"location":"docker-images/pingdirectory/#connecting-with-an-ldap-client","title":"Connecting with an LDAP Client","text":"<p>Connect an LDAP Client (such as Apache Directory Studio) to this container using the default ports and credentials</p> LDAP Port 1389 LDAP Base DN dc=example,dc=com Root Username cn=administrator Root Password 2FederateM0re"},{"location":"docker-images/pingdirectory/#stoppingremoving-the-container","title":"Stopping/Removing the container","text":"<p>To stop the container:</p> <pre><code>  docker container stop pingdirectory\n</code></pre> <p>To remove the container:</p> <pre><code>  docker container rm -f pingdirectory\n</code></pre>"},{"location":"docker-images/pingdirectory/#docker-container-hook-scripts","title":"Docker Container Hook Scripts","text":"<p>Please go here for details on all pingdirectory hook scripts</p> <p>This document is auto-generated from pingdirectory/Dockerfile</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdirectory/hooks/","title":"Ping Identity DevOps <code>pingdirectory</code> Hooks","text":"<p>List of available hooks: * 03-build-run-plan.sh * 07-apply-server-profile.sh * 182-pre-setup.sh * 20-restart-sequence.sh * 80-post-start.sh * 90-shutdown-sequence.sh * pingdirectory.lib.sh</p> <p>These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon)</p> <p>This document is auto-generated from pingdirectory/opt/staging/hooks</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdirectory/hooks/03-build-run-plan.sh/","title":"Ping Identity DevOps <code>pingdirectory</code> Hook - <code>03-build-run-plan.sh</code>","text":"<p>This script is called to determine the plan for the server as it starts up.</p> <p>This document is auto-generated from pingdirectory/opt/staging/hooks/03-build-run-plan.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdirectory/hooks/07-apply-server-profile.sh/","title":"Ping Identity DevOps <code>pingdirectory</code> Hook - <code>07-apply-server-profile.sh</code>","text":"<p>The server-profiles from:  * remote (i.e. git) and  * local (i.e. /opt/in)  have been merged into the ${STAGING_DIR}/instance (ie. /opt/staging/instance).  These files will be installed or overwritten into the ${SERVER_ROOT_DIR}.</p> <p>This document is auto-generated from pingdirectory/opt/staging/hooks/07-apply-server-profile.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdirectory/hooks/182-pre-setup.sh/","title":"Ping Identity DevOps <code>pingdirectory</code> Hook - <code>182-pre-setup.sh</code>","text":"<p>This document is auto-generated from pingdirectory/opt/staging/hooks/182-pre-setup.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdirectory/hooks/20-restart-sequence.sh/","title":"Ping Identity DevOps <code>pingdirectory</code> Hook - <code>20-restart-sequence.sh</code>","text":"<p>This hook is called when the container has been built in a prior startup  and a configuration has been found.</p> <p>This document is auto-generated from pingdirectory/opt/staging/hooks/20-restart-sequence.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdirectory/hooks/80-post-start.sh/","title":"Ping Identity DevOps <code>pingdirectory</code> Hook - <code>80-post-start.sh</code>","text":"<p>This hook configures pingdirectory replication  * Enabling Replication  * Get the new current topology  * Initialize replication</p> <p>This document is auto-generated from pingdirectory/opt/staging/hooks/80-post-start.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdirectory/hooks/90-shutdown-sequence.sh/","title":"Ping Identity DevOps <code>pingdirectory</code> Hook - <code>90-shutdown-sequence.sh</code>","text":"<p>This script handles removing the server from the topology during a shutdown.</p> <p>This document is auto-generated from pingdirectory/opt/staging/hooks/90-shutdown-sequence.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdirectory/hooks/pingdirectory.lib.sh/","title":"Ping Identity DevOps <code>pingdirectory</code> Hook - <code>pingdirectory.lib.sh</code>","text":"<p>This document is auto-generated from pingdirectory/opt/staging/hooks/pingdirectory.lib.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdirectoryproxy/","title":"Ping Identity DevOps Docker Image - <code>pingdirectoryproxy</code>","text":"<p>This docker image includes the Ping Identity PingDirectoryProxy product binaries and associated hook scripts to create and run a PingDirectoryProxy instance or instances.</p>"},{"location":"docker-images/pingdirectoryproxy/#related-docker-images","title":"Related Docker Images","text":"<ul> <li><code>pingidentity/pingbase</code> - Parent Image <p>This image inherits, and can use, Environment Variables from pingidentity/pingbase</p> </li> <li><code>pingidentity/pingdatacommon</code> - Common Ping files (i.e. hook scripts)\\</li> </ul>"},{"location":"docker-images/pingdirectoryproxy/#environment-variables","title":"Environment Variables","text":"<p>In addition to environment variables inherited from pingidentity/pingbase, the following environment <code>ENV</code> variables can be used with this image.</p> ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} DATE ${DATE} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingDirectoryProxy Ping product name LICENSE_FILE_NAME PingDirectory.lic Name of license File LICENSE_DIR ${PD_LICENSE_DIR} PD License directory. This value is set from the pingbase docker file LICENSE_SHORT_NAME PD Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server ADMIN_USER_NAME admin Replication administrative user STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server The command that the entrypoint will execute in the foreground to instantiate the container PD_DELEGATOR_PUBLIC_HOSTNAME localhost Public hostname of the DA app STARTUP_FOREGROUND_OPTS --nodetach The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to /SECRETS_DIR/root-user-password KEYSTORE_FILE Location of the keystore file containing the server certificate. If left undefined, the SECRETS_DIR will be checked for a keystore. If that keystore does not exist, the server will generate a self-signed certificate. KEYSTORE_PIN_FILE Location of the pin file for the keystore defined in KEYSTORE_FILE. You must specify a KEYSTORE_PIN_FILE when a KEYSTORE_FILE is present. This value does not need to be defined when allowing the server to generate a self-signed certificate. KEYSTORE_TYPE Format of the keystore defined in KEYSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the keystore format will be inferred based on the file extension of the KEYSTORE_FILE, defaulting to \"jks\". TRUSTSTORE_FILE Location of the truststore file for the server. If left undefined, the SECRETS_DIR will be checked for a truststore. If that truststore does not exist, the server will generate a truststore, containing its own certificate. TRUSTSTORE_PIN_FILE Location of the pin file for the truststore defined in TRUSTSTORE_FILE. You must specify a TRUSTSTORE_PIN_FILE when a TRUSTSTORE_FILE is present. This value does not need to be defined when allowing the server to generate a truststore. TRUSTSTORE_TYPE Format of the truststore defined in TRUSTSTORE_FILE. One of \"jks\", \"pkcs12\", \"pem\", or \"bcfks\" (in FIPS mode). If not defined, the truststore format will be inferred based on the file extension of the TRUSTSTORE_FILE, defaulting to \"jks\". TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/access ${SERVER_ROOT_DIR}/logs/errors ${SERVER_ROOT_DIR}/logs/failed-ops ${SERVER_ROOT_DIR}/logs/config-audit.log ${SERVER_ROOT_DIR}/logs/tools/.log ${SERVER_BITS_DIR}/logs/tools/.log Files tailed once container has started PD_PROFILE ${STAGING_DIR}/pd.profile Directory for the profile used by the PingData manage-profile tool UNBOUNDID_SKIP_START_PRECHECK_NODETACH true Setting this variable to true speeds up server startup time by skipping an unnecessary JVM check. CERTIFICATE_NICKNAME There is an additional certificate-based variable used to identity the certificate alias used within the <code>KEYSTORE_FILE</code>. That variable is called <code>CERTIFICATE_NICKNAME</code>, which identifies the certificate to use by the server in the <code>KEYSTORE_FILE</code>. If a value is not provided, the container will look at the list certs found in the <code>KEYSTORE_FILE</code> and if one - and only one - certificate is found of type <code>PrivateKeyEntry</code>, that alias will be used. RETRY_TIMEOUT_SECONDS 180 The default retry timeout in seconds for manage-topology and remove-defunct-server PINGDIRECTORY_HOSTNAME Set this variable to configure Proxy for automatic server discovery with PingDirectory hostname JOIN_PD_TOPOLOGY must be enabled for PINGDIRECTORY_HOSTNAME to take effect PINGDIRECTORY_LDAPS_PORT Set this variable to configure Proxy for automatic server discovery with PingDirectory LDAPS port JOIN_PD_TOPOLOGY must be enabled for PINGDIRECTORY_LDAPS_PORT to take effect JOIN_PD_TOPOLOGY false Setting this variable to true will configure proxy to join the topology of PingDirectory COLUMNS 120 Sets the number of columns in PingDirectoryProxy command-line tool output"},{"location":"docker-images/pingdirectoryproxy/#ports-exposed","title":"Ports Exposed","text":"<p>The following ports are exposed from the container.  If a variable is used, then it may come from a parent container</p> <ul> <li>${LDAP_PORT}</li> <li>${LDAPS_PORT}</li> <li>${HTTPS_PORT}</li> <li>${JMX_PORT}</li> </ul>"},{"location":"docker-images/pingdirectoryproxy/#running-a-pingdirectoryproxy-container","title":"Running a PingDirectoryProxy container","text":"<p>The easiest way to test test a simple standalone image of PingDirectoryProxy is to cut/paste the following command into a terminal on a machine with docker.</p> <pre><code>  docker run \\\n           --name pingdirectoryproxy \\\n           --publish 1389:1389 \\\n           --publish 8443:1443 \\\n           --detach \\\n           --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\\n           --env SERVER_PROFILE_PATH=baseline/pingdirectoryproxy \\\n           --env PING_IDENTITY_ACCEPT_EULA=YES \\\n           --env PING_IDENTITY_DEVOPS_USER \\\n           --env PING_IDENTITY_DEVOPS_KEY \\\n           --tmpfs /run/secrets \\\n           pingidentity/pingdirectoryproxy:edge\n</code></pre> <p>You can view the Docker logs with the command:</p> <pre><code>  docker logs -f pingdirectoryproxy\n</code></pre> <p>You should see the output from a PingDirectoryProxy install and configuration, ending with a message the the PingDirectoryProxy has started.  After it starts, you will see some typical access logs.  Simply <code>Ctrl-C</code> after to stop tailing the logs.</p>"},{"location":"docker-images/pingdirectoryproxy/#running-a-sample-100sec-search-rate-test","title":"Running a sample 100/sec search rate test","text":"<p>With the PingDirectoryProxy running from the previous section, you can run a <code>searchrate</code> job that will send load to the directory at a rate if 100/sec using the following command.</p> <pre><code>docker exec -it pingdirectoryproxy \\\n        /opt/out/instance/bin/searchrate \\\n                -b dc=example,dc=com \\\n                --scope sub \\\n                --filter \"(uid=user.[1-9])\" \\\n                --attribute mail \\\n                --numThreads 2 \\\n                --ratePerSecond 100\n</code></pre>"},{"location":"docker-images/pingdirectoryproxy/#connecting-with-an-ldap-client","title":"Connecting with an LDAP Client","text":"<p>Connect an LDAP Client (such as Apache Directory Studio) to this container using the default ports and credentials</p> LDAP Port 1389 LDAP Base DN dc=example,dc=com Root Username cn=administrator Root Password 2FederateM0re"},{"location":"docker-images/pingdirectoryproxy/#stoppingremoving-the-container","title":"Stopping/Removing the container","text":"<p>To stop the container:</p> <pre><code>  docker container stop pingdirectoryproxy\n</code></pre> <p>To remove the container:</p> <pre><code>  docker container rm -f pingdirectoryproxy\n</code></pre>"},{"location":"docker-images/pingdirectoryproxy/#docker-container-hook-scripts","title":"Docker Container Hook Scripts","text":"<p>Please go here for details on all pingdirectoryproxy hook scripts</p> <p>This document is auto-generated from pingdirectoryproxy/Dockerfile</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdirectoryproxy/hooks/","title":"Ping Identity DevOps <code>pingdirectoryproxy</code> Hooks","text":"<p>List of available hooks: * 03-build-run-plan.sh * 80-post-start.sh * 90-shutdown-sequence.sh</p> <p>These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon)</p> <p>This document is auto-generated from pingdirectoryproxy/opt/staging/hooks</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdirectoryproxy/hooks/03-build-run-plan.sh/","title":"Ping Identity DevOps <code>pingdirectoryproxy</code> Hook - <code>03-build-run-plan.sh</code>","text":"<p>This script is called to determine the plan for the server as it starts up.</p> <p>This document is auto-generated from pingdirectoryproxy/opt/staging/hooks/03-build-run-plan.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdirectoryproxy/hooks/80-post-start.sh/","title":"Ping Identity DevOps <code>pingdirectoryproxy</code> Hook - <code>80-post-start.sh</code>","text":"<p>This script is mostly the same as the 80-post-start.sh hook in the  * Enabling PingDirectoryProxy Automatic Server Discovery</p> <p>This document is auto-generated from pingdirectoryproxy/opt/staging/hooks/80-post-start.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingdirectoryproxy/hooks/90-shutdown-sequence.sh/","title":"Ping Identity DevOps <code>pingdirectoryproxy</code> Hook - <code>90-shutdown-sequence.sh</code>","text":"<p>This script handles removing the server from the topology during a shutdown.</p> <p>This document is auto-generated from pingdirectoryproxy/opt/staging/hooks/90-shutdown-sequence.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingfederate/","title":"Ping Identity DevOps Docker Image - <code>pingfederate</code>","text":"<p>This docker image includes the Ping Identity PingFederate product binaries and associated hook scripts to create and run both PingFederate Admin and Engine nodes.</p>"},{"location":"docker-images/pingfederate/#related-docker-images","title":"Related Docker Images","text":"<ul> <li><code>pingidentity/pingbase</code> - Parent Image <p>This image inherits, and can use, Environment Variables from pingidentity/pingbase</p> </li> <li><code>pingidentity/pingcommon</code> - Common Ping files (i.e. hook scripts)</li> </ul>"},{"location":"docker-images/pingfederate/#environment-variables","title":"Environment Variables","text":"<p>In addition to environment variables inherited from pingidentity/pingbase, the following environment <code>ENV</code> variables can be used with this image.</p> ENV Variable Default Description BASE ${BASE:-/opt} ROOT_USER administrator the default administrative user for PingData JAVA_HOME /opt/java STAGING_DIR ${BASE}/staging Path to the staging area where the remote and local server profiles can be merged OUT_DIR ${BASE}/out Path to the runtime volume SERVER_ROOT_DIR ${OUT_DIR}/instance Path from which the runtime executes IN_DIR ${BASE}/in Location of a local server-profile volume SERVER_BITS_DIR ${BASE}/server Path to the server bits BAK_DIR ${BASE}/backup Path to a volume generically used to export or backup data LOGS_DIR ${BASE}/logs Path to a volume generically used for logging PING_IDENTITY_ACCEPT_EULA NO Must be set to 'YES' for the container to start PING_IDENTITY_DEVOPS_FILE devops-secret File name for devops-creds passed as a Docker secret STAGING_MANIFEST ${BASE}/staging-manifest.txt Path to a manifest of files expected in the staging dir on first image startup CLEAN_STAGING_DIR false Whether to clean the staging dir when the image starts SECRETS_DIR /run/secrets Default path to the secrets TOPOLOGY_FILE ${STAGING_DIR}/topology.json Path to the topology file HOOKS_DIR ${STAGING_DIR}/hooks Path where all the hooks scripts are stored CONTAINER_ENV ${STAGING_DIR}/.env Environment Property file use to share variables between scripts in container SERVER_PROFILE_DIR /tmp/server-profile Path where the remote server profile is checked out or cloned before being staged prior to being applied on the runtime SERVER_PROFILE_URL A valid git HTTPS URL (not ssh) SERVER_PROFILE_URL_REDACT true When set to \"true\", the server profile git URL will not be printed to container output. SERVER_PROFILE_BRANCH A valid git branch (optional) SERVER_PROFILE_PATH The subdirectory in the git repo SERVER_PROFILE_UPDATE false Whether to update the server profile upon container restart SECURITY_CHECKS_STRICT false Requires strict checks on security SECURITY_CHECKS_FILENAME .jwk .pin Perform a check for filenames that may violate security (i.e. secret material) UNSAFE_CONTINUE_ON_ERROR If this is set to true, then the container will provide a hard warning and continue. LICENSE_DIR ${SERVER_ROOT_DIR} License directory PD_LICENSE_DIR ${STAGING_DIR}/pd.profile/server-root/pre-setup PD License directory. Separating from above LICENSE_DIR to differentiate for different products STARTUP_COMMAND The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container PING_IDENTITY_DEVOPS_KEY_REDACT true TAIL_LOG_FILES A whitespace separated list of log files to tail to the container standard output - DO NOT USE WILDCARDS like /path/to/logs/*.log COLORIZE_LOGS true If 'true', the output logs will be colorized with GREENs and REDs, otherwise, no colorization will be done.  This is good for tools that monitor logs and colorization gets in the way. LOCATION Docker Location default value If PingDirectory is deployed in multi cluster mode, that is, K8S_CLUSTER, K8S_CLUSTERS and K8S_SEED_CLUSTER are defined, LOCATION is ignored and K8S_CLUSTER is used as the location LOCATION_VALIDATION true Any string denoting a logical/physical location MAX_HEAP_SIZE 384m Heap size (for java products) JVM_TUNING AGGRESSIVE JAVA_RAM_PERCENTAGE 75.0 Percentage of the container memory to allocate to PingFederate JVM DO NOT set to 100% or your JVM will exit with OutOfMemory errors and the container will terminate VERBOSE false Triggers verbose messages in scripts using the set -x option. PING_DEBUG false Set the server in debug mode, with increased output PING_PRODUCT The name of Ping product, i.e. PingFederate, PingDirectory - must be a valid Ping product type. This variable should be overridden by child images. PING_PRODUCT_VALIDATION true i.e. PingFederate,PingDirectory ADDITIONAL_SETUP_ARGS List of setup arguments passed to Ping Data setup-arguments.txt file LDAP_PORT 1389 Port over which to communicate for LDAP LDAPS_PORT 1636 Port over which to communicate for LDAPS HTTPS_PORT 1443 Port over which to communicate for HTTPS JMX_PORT 1689 Port for monitoring over JMX protocol ORCHESTRATION_TYPE The type of orchestration tool used to run the container, normally set in the deployment (.yaml) file.  Expected values include: - compose - swarm - kubernetes Defaults to blank (i.e. No type is set) USER_BASE_DN dc=example,dc=com Base DN for user data DOLLAR '$' Variable with a literal value of '$', to avoid unwanted variable substitution PD_ENGINE_PUBLIC_HOSTNAME localhost PD (PingDirectory) public hostname that may be used in redirects PD_ENGINE_PRIVATE_HOSTNAME pingdirectory PD (PingDirectory) private hostname PDP_ENGINE_PUBLIC_HOSTNAME localhost PDP (PingDirectoryProxy) public hostname that may be used in redirects PDP_ENGINE_PRIVATE_HOSTNAME pingdirectoryproxy PDP (PingDirectoryProxy) private hostname PDS_ENGINE_PUBLIC_HOSTNAME localhost PDS (PingDataSync) public hostname that may be used in redirects PDS_ENGINE_PRIVATE_HOSTNAME pingdatasync PDS (PingDataSync) private hostname PAZ_ENGINE_PUBLIC_HOSTNAME localhost PAZ (PingAuthorize) public hostname that may be used in redirects PAZ_ENGINE_PRIVATE_HOSTNAME pingauthorize PAZ (PingAuthorize) private hostname PAZP_ENGINE_PUBLIC_HOSTNAME localhost PAZP (PingAuthorize-PAP) public hostname that may be used in redirects PAZP_ENGINE_PRIVATE_HOSTNAME pingauthorizepap PAZP (PingAuthorize-PAP) private hostname PF_ENGINE_PUBLIC_HOSTNAME localhost PF (PingFederate) engine public hostname that may be used in redirects PF_ENGINE_PRIVATE_HOSTNAME pingfederate PF (PingFederate) engine private hostname PF_ADMIN_PUBLIC_BASEURL https://localhost:9999 PF (PingFederate) admin public baseurl that may be used in redirects. PF_RUN_PF_ADMIN_BASEURL will override this value for PingFederate 11.3 and later. PF_ADMIN_PUBLIC_HOSTNAME localhost PF (PingFederate) admin public hostname that may be used in redirects. PF_RUN_PF_ADMIN_HOSTNAME will override this value for PingFederate 11.3 and later. PF_ADMIN_PRIVATE_HOSTNAME pingfederate-admin PF (PingFederate) admin private hostname PA_ENGINE_PUBLIC_HOSTNAME localhost PA (PingAccess) engine public hostname that may be used in redirects PA_ENGINE_PRIVATE_HOSTNAME pingaccess PA (PingAccess) engine private hostname PA_ADMIN_PUBLIC_HOSTNAME localhost PA (PingAccess) admin public hostname that may be used in redirects PA_ADMIN_PRIVATE_HOSTNAME pingaccess-admin PA (PingAccess) admin private hostname ROOT_USER_DN cn=${ROOT_USER} DN of the server root user ENV ${BASE}/.profile MOTD_URL https://raw.githubusercontent.com/pingidentity/pingidentity-devops-getting-started/master/motd/motd.json Instructs the image to pull the MOTD json from the following URL. If this MOTD_URL variable is empty, then no motd will be downloaded. The format of this MOTD file must match the example provided in the url: https://raw.githubusercontent.com/pingidentity/pingidentity-devops-getting-started/master/motd/motd.json PS1 \\${PING_PRODUCT}:\\h:\\w\\n&gt; Default shell prompt (i.e. productName:hostname:workingDir) PATH ${JAVA_HOME}/bin:${BASE}:${SERVER_ROOT_DIR}/bin:${PATH} PATH used by the container SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} DATE ${DATE} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingFederate Ping product name LICENSE_DIR ${SERVER_ROOT_DIR}/server/default/conf License directory LICENSE_FILE_NAME pingfederate.lic Name of license file LICENSE_SHORT_NAME PF Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh The command that the entrypoint will execute in the foreground to instantiate the container PING_IDENTITY_PASSWORD 2FederateM0re Specify a password for administrator user for interaction with admin API TAIL_LOG_FILES ${SERVER_ROOT_DIR}/log/server.log Files tailed once container has started PF_LOG_SIZE_MAX 10000 KB Defines the log file size max for ALL appenders PF_LOG_NUMBER 2 Defines the maximum of log files to retain upon rotation PF_LOG_LEVEL INFO General log level -- provide custom log4j2.xml in profile for more detailed control valid values are OFF, ERROR, WARN, INFO, DEBUG NOTE: PF_LOG_LEVEL only applies to PF versions before 11.2.0 PF_ADMIN_PORT 9999 Defines the port on which the PingFederate administrative console and API runs. PF_RUN_PF_ADMIN_HTTPS_PORT will override this for PingFederate 11.3 and later. PF_ENGINE_PORT 9031 Defines the port on which PingFederate listens for encrypted HTTPS (SSL/TLS) traffic. PF_RUN_PF_HTTPS_PORT will override this for PingFederate 11.3 and later. PF_ENGINE_SECONDARY_PORT -1 Defines a secondary HTTPS port that can be used for mutual SSL/TLS (client X.509 certificate) authentication for both end users and protocol requests. PF_RUN_PF_SECONDARY_HTTPS_PORT (default 9032) will override this value. The default value of -1 disables the port in the product. PF_ENGINE_DEBUG false Flag to turn on PingFederate Engine debugging Used in run.sh PF_ADMIN_DEBUG false Flag to turn on PingFederate Admin debugging Used in run.sh PF_DEBUG_PORT 9030 Defines the port on which PingFederate opens up a java debugging port. Used in run.sh SHOW_LIBS_VER true Defines a variable to allow showing library versions in the output at startup default to true SHOW_LIBS_VER_PRE_PATCH false Defines a variable to allow showing library version prior to patches being applied default to false This is helpful to ensure that the patch process updates all libraries affected OPERATIONAL_MODE STANDALONE Operational Mode Indicates the operational mode of the runtime server in run.properties Options include STANDALONE, CLUSTERED_CONSOLE, CLUSTERED_ENGINE. PF_RUN_PF_OPERATIONAL_MODE will override this for PingFederate 11.3 and later. PF_CONSOLE_AUTHENTICATION Defines mechanism for console authentication in run.properties. Options include none, native, LDAP, cert, RADIUS, OIDC. If not set, default is native. PF_RUN_PF_CONSOLE_AUTHENTICATION will override this for PingFederate 11.3 and later. PF_ADMIN_API_AUTHENTICATION Defines mechanism for admin api authentication in run.properties. Options include none, native, LDAP, cert, RADIUS, OIDC. If not set, default is native. PF_RUN_PF_ADMIN_API_AUTHENTICATION will override this for PingFederate 11.3 and later. HSM_MODE OFF Hardware Security Module Mode in run.properties Options include OFF, AWSCLOUDHSM, NCIPHER, LUNA, BCFIPS. PF_RUN_PF_HSM_MODE will override this for PingFederate 11.3 and later. PF_BC_FIPS_APPROVED_ONLY false Defines a variable that allows instantiating non-FIPS crypto/random PF_HSM_HYBRID false Hardware Security Module Hybrid Mode   When PF is in Hybrid mode, certs/keys can be created either on the local trust store or on the HSM.   This can used as a migration strategy towards an HSM setup. PF_RUN_PF_HSM_HYBRID will override this for PingFederate 11.3 and later. PF_LDAP_TYPE PingDirectory This is the type of the LDAP directory server. This property is needed by PingFederate to determine how to handle different implementations between the available LDAP directory server types. Valid options include: ActiveDirectory, SunDirectoryServer, OracleUnifiedDirectory, PingDirectory, and Generic. PF_LDAP_USERNAME This is the username for an account within the LDAP Directory Server that can be used to perform user lookups for authentication and other user level search operations.  Set if PF_CONSOLE_AUTHENTICATION or PF_ADMIN_API_AUTHENTICATION=LDAP PF_LDAP_LDAP_USERNAME will override this for PingFederate 11.3 and later. PF_LDAP_PASSWORD This is the password for the Username specified above. This property should be obfuscated using the 'obfuscate.sh' utility. Set if PF_CONSOLE_AUTHENTICATION or PF_ADMIN_API_AUTHENTICATION=LDAP PF_LDAP_LDAP_PASSWORD will override this for PingFederate 11.3 and later. CLUSTER_BIND_ADDRESS NON_LOOPBACK IP address for cluster communication.  Set to NON_LOOPBACK to allow the system to choose an available non-loopback IP address. PF_RUN_PF_CLUSTER_BIND_ADDRESS will override this for PingFederate 11.3 and later. PF_PROVISIONER_MODE OFF Provisioner Mode in run.properties Options include OFF, STANDALONE, FAILOVER. PF_RUN_PF_PROVISIONER_MODE will override this for PingFederate 11.3 and later. PF_PROVISIONER_NODE_ID 1 Provisioner Node ID in run.properties Initial active provisioning server node ID is 1 PF_RUN_PROVISIONER_NODE_ID will override this for PingFederate 11.3 and later. PF_PROVISIONER_GRACE_PERIOD 600 Node group ID in cluster-adaptive.conf file. Does not require a .subst file. Provisioner Failover Grace Period in run.properties Grace period, in seconds. Default 600 seconds PF_RUN_PROVISIONER_FAILOVER_GRACE_PERIOD will override this for PingFederate 11.3 and later. PF_JETTY_THREADS_MIN Override the default value for the minimum size of the Jetty thread pool Leave unset to let the container automatically tune the value according to available resources PF_RUN_PF_RUNTIME_THREADS_MIN will override this for PingFederate 11.3 and later. PF_JETTY_THREADS_MAX Override the default value for the maximum size of the Jetty thread pool Leave unset to let the container automatically tune the value according to available resources PF_RUN_PF_RUNTIME_THREADS_MAX will override this for PingFederate 11.3 and later. PF_ACCEPT_QUEUE_SIZE 512 The size of the accept queue. There is generally no reason to tune this but please refer to the performance tuning guide for further tuning guidance. PF_RUN_PF_RUNTIME_ACCEPTQUEUESIZE will override this for PingFederate 11.3 and later. PF_PINGONE_REGION The region of the PingOne tenant PingFederate should connect with. Valid values are \"com\", \"eu\" and \"asia\" PF_RUN_PF_PINGONE_ADMIN_URL_REGION will override this for PingFederate 11.3 and later. PF_PINGONE_ENV_ID The PingOne environment ID to use PF_RUN_PF_PINGONE_ADMIN_URL_ENVIRONMENT_ID will override this for PingFederate 11.3 and later. PF_CONSOLE_TITLE Docker PingFederate The title featured in the administration console -- this is generally used to easily distinguish between environments PF_RUN_PF_CONSOLE_TITLE will override this for PingFederate 11.3 and later. PF_NODE_TAGS This property defines the tags associated with this PingFederate node. Configuration is optional. When configured, PingFederate takes this property into consideration when processing requests. For example, tags may be used to determine the data store location that this PingFederate node communicates with. Administrators may also use tags in conjunction with authentication selectors and policies to define authentication requirements.  Administrators may define one tag or a list of space-separated tags. Each tag cannot contain any spaces. Other characters are allowed.  Example 1: PF_NODE_TAGS=north Example 1 defines one tag: 'north' Example 2: PF_NODE_TAGS=1 123 test Example 2 defines three tags: '1', '123' and 'test'  Example 3: PF_NODE_TAGS= Example 3 is also valid because the PF_NODE_TAGS property is optional. PF_RUN_NODE_TAGS will override this for PingFederate 11.3 and later. PF_CONSOLE_ENV This property defines the name of the PingFederate environment that will be displayed in the administrative console, used to make separate environments easily identifiable. PF_RUN_PF_CONSOLE_ENVIRONMENT will override this for PingFederate 11.3 and later. JAVA_RAM_PERCENTAGE 75.0 Percentage of the container memory to allocate to PingFederate JVM DO NOT set to 100% or your JVM will exit with OutOfMemory errors and the container will terminate BULK_CONFIG_DIR ${OUT_DIR}/instance/bulk-config BULK_CONFIG_FILE data.json ADMIN_WAITFOR_TIMEOUT 300 wait-for timeout for 80-post-start.sh hook script How long to wait for the PF Admin console to be available CREATE_INITIAL_ADMIN_USER false Set to true to create the initial admin user after PingFederate starts up. The initial admin user will only be created on the first startup of the server after the license is accepted."},{"location":"docker-images/pingfederate/#ports-exposed","title":"Ports Exposed","text":"<p>The following ports are exposed from the container.  If a variable is used, then it may come from a parent container</p> <ul> <li>9031</li> <li>9999</li> </ul>"},{"location":"docker-images/pingfederate/#running-a-pingfederate-container","title":"Running a PingFederate container","text":"<p>To run a PingFederate container:</p> <pre><code>  docker run \\\n           --name pingfederate \\\n           --publish 9999:9999 \\\n           --detach \\\n           --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\\n           --env SERVER_PROFILE_PATH=getting-started/pingfederate \\\n           --env PING_IDENTITY_ACCEPT_EULA=YES \\\n           --env PING_IDENTITY_DEVOPS_USER \\\n           --env PING_IDENTITY_DEVOPS_KEY \\\n           --tmpfs /run/secrets \\\n           pingidentity/pingfederate:edge\n</code></pre> <p>Follow Docker logs with:</p> <pre><code>docker logs -f pingfederate\n</code></pre> <p>If using the command above with the embedded server profile, log in with: * https://localhost:9999/pingfederate/app   * Username: Administrator   * Password: 2FederateM0re</p>"},{"location":"docker-images/pingfederate/#docker-container-hook-scripts","title":"Docker Container Hook Scripts","text":"<p>Please go here for details on all pingfederate hook scripts</p> <p>This document is auto-generated from pingfederate/Dockerfile</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingfederate/hooks/","title":"Ping Identity DevOps <code>pingfederate</code> Hooks","text":"<p>List of available hooks: * 04-check-variables.sh.pre * 05-expand-templates.sh.pre * 20-restart-sequence.sh.pre * 80-post-start.sh * 81-after-start-process.sh * 83-configure-admin.sh * 85-import-configuration.sh</p> <p>These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon)</p> <p>This document is auto-generated from pingfederate/opt/staging/hooks</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingfederate/hooks/04-check-variables.sh.pre/","title":"Ping Identity DevOps <code>pingfederate</code> Hook - <code>04-check-variables.sh.pre</code>","text":"<p>This document is auto-generated from pingfederate/opt/staging/hooks/04-check-variables.sh.pre</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingfederate/hooks/05-expand-templates.sh.pre/","title":"Ping Identity DevOps <code>pingfederate</code> Hook - <code>05-expand-templates.sh.pre</code>","text":"<p>This document is auto-generated from pingfederate/opt/staging/hooks/05-expand-templates.sh.pre</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingfederate/hooks/20-restart-sequence.sh.pre/","title":"Ping Identity DevOps <code>pingfederate</code> Hook - <code>20-restart-sequence.sh.pre</code>","text":"<p>This document is auto-generated from pingfederate/opt/staging/hooks/20-restart-sequence.sh.pre</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingfederate/hooks/80-post-start.sh/","title":"Ping Identity DevOps <code>pingfederate</code> Hook - <code>80-post-start.sh</code>","text":"<p>This script is used to import any configurations that are  needed after PingFederate starts</p> <p>This document is auto-generated from pingfederate/opt/staging/hooks/80-post-start.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingfederate/hooks/81-after-start-process.sh/","title":"Ping Identity DevOps <code>pingfederate</code> Hook - <code>81-after-start-process.sh</code>","text":"<p>This document is auto-generated from pingfederate/opt/staging/hooks/81-after-start-process.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingfederate/hooks/83-configure-admin.sh/","title":"Ping Identity DevOps <code>pingfederate</code> Hook - <code>83-configure-admin.sh</code>","text":"<p>This document is auto-generated from pingfederate/opt/staging/hooks/83-configure-admin.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingfederate/hooks/85-import-configuration.sh/","title":"Ping Identity DevOps <code>pingfederate</code> Hook - <code>85-import-configuration.sh</code>","text":"<p>This document is auto-generated from pingfederate/opt/staging/hooks/85-import-configuration.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingintelligence/","title":"Ping Identity DevOps Docker Image - <code>pingintelligence-ase</code>","text":"<p>DEPRECATION NOTICE: As of July 2024, the PingIntelligence Docker Image is deprecated. No new image versions will be published. Existing versions will be available as indicated in the Docker Image Support Policy.</p> <p>This docker image includes the Ping Identity PingIntelligence API Security Enforcer product binaries and associated hook scripts to create and run PingIntelligence ASE instances.</p>"},{"location":"docker-images/pingintelligence/#related-docker-images","title":"Related Docker Images","text":"<ul> <li><code>pingidentity/pingbase</code> - Parent Image <p>This image inherits, and can use, Environment Variables from pingidentity/pingbase</p> </li> <li><code>pingidentity/pingcommon</code> - Common Ping files (i.e. hook scripts)</li> </ul>"},{"location":"docker-images/pingintelligence/#environment-variables","title":"Environment Variables","text":"<p>In addition to environment variables inherited from pingidentity/pingbase, the following environment <code>ENV</code> variables can be used with this image.</p> ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} DATE ${DATE} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingIntelligence_ASE Ping product name LICENSE_FILE_NAME PingIntelligence.lic Name of license File LICENSE_DIR ${SERVER_ROOT_DIR}/config License directory LICENSE_SHORT_NAME pingintelligence Shortname used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start_ase.sh The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container ROOT_USER_PASSWORD_FILE ADMIN_USER_PASSWORD_FILE ENCRYPTION_PASSWORD_FILE PING_INTELLIGENCE_ADMIN_USER admin PingIntelligence global variables PingIntelligence default administrative user (this should probably not be changed) PING_INTELLIGENCE_ADMIN_PASSWORD 2FederateM0re PingIntelligence default administrative user credentials (this should be changed) PING_INTELLIGENCE_ASE_HTTP_PORT 8000 The ASE HTTP listener port PING_INTELLIGENCE_ASE_HTTPS_PORT 8443 The ASE HTTPS listener port PING_INTELLIGENCE_ASE_MGMT_PORT 8010 the ASE management port PING_INTELLIGENCE_ASE_TIMEZONE utc The timezone the ASE container is operating in PING_INTELLIGENCE_ASE_ABS_PUBLISH true Whether the ASE should poll the ABS service that publishes discovered APIs PING_INTELLIGENCE_ASE_ABS_PUBLISH_REQUEST_MINUTES 10 The interval in minute to poll the API discovery list PING_INTELLIGENCE_ASE_MODE sideband Defines running mode for API Security Enforcer (Allowed values are inline or sideband). PING_INTELLIGENCE_ASE_ENABLE_SIDEBAND_AUTHENTICATION false Enable client-side authentication with tokens in sideband mode PING_INTELLIGENCE_ASE_HOSTNAME_REWRITE false PING_INTELLIGENCE_ASE_KEYSTORE_PASSWORD OBF:AES:sRNp0W7sSi1zrReXeHodKQ:lXcvbBhKZgDTrjQOfOkzR2mpca4bTUcwPAuerMPwvM4 PING_INTELLIGENCE_ASE_ADMIN_LOG_LEVEL 4 For controller.log and balancer.log only 1-5 (FATAL, ERROR, WARNING, INFO, DEBUG) PING_INTELLIGENCE_ASE_ENABLE_CLUSTER false enable cluster PING_INTELLIGENCE_ASE_SYSLOG_SERVER Syslog server PING_INTELLIGENCE_ASE_CA_CERT_PATH Path the to CA certificate PING_INTELLIGENCE_ASE_ENABLE_HEALTH false enable the ASE health check service PING_INTELLIGENCE_ASE_ENABLE_ABS true Set this value to true, to allow API Security Enforcer to send logs to ABS. PING_INTELLIGENCE_ASE_ENABLE_ABS_ATTACK_LIST_RETRIEVAL true Toggle ABS attack list retrieval PING_INTELLIGENCE_ASE_BLOCK_AUTODETECTED_ATTACKS false Toggle whether ASE blocks auto-detected attacks PING_INTELLIGENCE_ASE_ATTACK_LIST_REFRESH_MINUTES 10 ABS attack list retieval frequency in minutes PING_INTELLIGENCE_ASE_HOSTNAME_REFRESH_SECONDS 60 Hostname refresh interval in seconds PING_INTELLIGENCE_ASE_DECOY_ALERT_INTERVAL_MINUTES 180 Alert interval for teh decoy services PING_INTELLIGENCE_ASE_ENABLE_XFORWARDED_FOR false Toggle X-Forwarded-For PING_INTELLIGENCE_ASE_ENABLE_FIREWALL true Toggle ASE Firewall PING_INTELLIGENCE_ASE_ENABLE_SIDEBAND_KEEPALIVE false Enable connection keepalive for requests from gateway to ASE in sideband mode When enabled, ASE sends 'Connection: keep-alive' header in response When disabled, ASE sends 'Connection: close' header in response PING_INTELLIGENCE_ASE_ENABLE_GOOGLE_PUBSUB false Enable Google Pub/Sub PING_INTELLIGENCE_ASE_ENABLE_ACCESS_LOG true Toggle the access log PING_INTELLIGENCE_ASE_ENABLE_AUDIT false Toggle audit logging PING_INTELLIGENCE_ASE_FLUSH_LOG_IMMEDIATELY true Toggle whether logs are flushed to disk immediately PING_INTELLIGENCE_ASE_HTTP_PROCESS 1 The number of processes for HTTP requests PING_INTELLIGENCE_ASE_HTTPS_PROCESS 1 The number of processes for HTTPS requests PING_INTELLIGENCE_ASE_ENABLE_SSL_V3 false Toggle SSLv3 -- this should absolutely stay disabled PING_INTELLIGENCE_TCP_SEND_BUFFER_BYTES 212992 Kernel TCP send buffer size in bytes PING_INTELLIGENCE_TCP_RECEIVE_BUFFER_BYTES 212992 enrel TCP receive buffer size in bytes PING_INTELLIGENCE_ASE_ATTACK_LIST_MEMORY 128MB PING_INTELLIGENCE_CLUSTER_PEER_NODE_CSV_LIST a comma-separated list of hostname:cluster_manager_port or IPv4_address:cluster_manager_port the ASE will try to connect to each server peer in the list PING_INTELLIGENCE_CLUSTER_ID ase_cluster The ASE cluster ID -- this must be unique PING_INTELLIGENCE_CLUSTER_MGMT_PORT 8020 The ASE cluster management port PING_INTELLIGENCE_CLUSTER_SECRET_KEY OBF:AES:nPJOh3wXQWK/BOHrtKu3G2SGiAEElOSvOFYEiWfIVSdummoFwSR8rDh2bBnhTDdJ:7LFcqXQlqkW9kldQoFg0nJoLSojnzHDbD3iAy84pT84 Secret key required to join the cluster PING_INTELLIGENCE_ABS_ENDPOINT a comma-separated list of abs nodes having hostname:port or ipv4:port as an address. PING_INTELLIGENCE_ABS_ACCESS_KEY access key for ase to authenticate with abs node PING_INTELLIGENCE_ABS_SECRET_KEY secret key for ase to authenticate with abs node PING_INTELLIGENCE_ABS_ENABLE_SSL true Setting this value to true will enable encrypted communication with ABS. PING_INTELLIGENCE_ABS_CA_CERT_PATH Configure the location of ABS's trusted CA certificates. PING_INTELLIGENCE_ABS_DEPLOYMENT_TYPE cloud Default deployment type -- Supported values (onprem/cloud) PING_INTELLIGENCE_ABS_DEPLOYMENT_TYPE_VALIDATION true Must be either cloud or onprem PING_INTELLIGENCE_GATEWAY_CREDENTIALS Obtain the appropriate JWT token in PinOne under Connections-&gt;PingIntelligence PING_INTELLIGENCE_GATEWAY_CREDENTIALS_REDACT true PING_STARTUP_TIMEOUT 8 The amount of time to wait for ASE to start before exiting TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/access.log Files tailed once container has started Other potentially useful log file to tail for debug purposes are logs/controller.log and logs/balancer.log"},{"location":"docker-images/pingintelligence/#running-a-pingintelligence-container","title":"Running a PingIntelligence container","text":"<p>To run a PingIntelligence container:</p> <pre><code>  docker run \\\n           --name pingintellgence \\\n           --publish 8443:8443 \\\n           --detach \\\n           --env PING_IDENTITY_ACCEPT_EULA=YES \\\n           --env PING_IDENTITY_DEVOPS_USER=user@pingone.com \\\n           --env PING_IDENTITY_DEVOPS_KEY=&lt;edvops key here&gt; \\\n           --env PING_INTELLIGENCE_GATEWAY_CREDENTIALS=&lt;PingIntelligence App JWT here&gt; \\\n           --ulimit nofile=65536:65536 \\\n           pingidentity/pingintelligence:edge\n</code></pre> <p>Follow Docker logs with:</p> <pre><code>docker logs -f pingintelligence\n</code></pre> <p>If using the command above, use cli.sh with:   * Username: admin   * Password: 2FederateM0re</p>"},{"location":"docker-images/pingintelligence/#docker-container-hook-scripts","title":"Docker Container Hook Scripts","text":"<p>Please go here for details on all pingintelligence hook scripts</p> <p>This document is auto-generated from pingintelligence/Dockerfile</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingintelligence/hooks/","title":"Ping Identity DevOps <code>pingintelligence</code> Hooks","text":"<p>List of available hooks: * 01-start-server.sh.pre * 04-check-variables.sh.post * 50-before-post-start.sh * 80-post-start.sh * pingintelligence.lib.sh</p> <p>These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon)</p> <p>This document is auto-generated from pingintelligence/opt/staging/hooks</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingintelligence/hooks/01-start-server.sh.pre/","title":"Ping Identity DevOps <code>pingintelligence</code> Hook - <code>01-start-server.sh.pre</code>","text":"<p>This document is auto-generated from pingintelligence/opt/staging/hooks/01-start-server.sh.pre</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingintelligence/hooks/04-check-variables.sh.post/","title":"Ping Identity DevOps <code>pingintelligence</code> Hook - <code>04-check-variables.sh.post</code>","text":"<p>This document is auto-generated from pingintelligence/opt/staging/hooks/04-check-variables.sh.post</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingintelligence/hooks/50-before-post-start.sh/","title":"Ping Identity DevOps <code>pingintelligence</code> Hook - <code>50-before-post-start.sh</code>","text":"<p>This document is auto-generated from pingintelligence/opt/staging/hooks/50-before-post-start.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingintelligence/hooks/80-post-start.sh/","title":"Ping Identity DevOps <code>pingintelligence</code> Hook - <code>80-post-start.sh</code>","text":"<p>This hook may be used to set the server if there is a setup procedure</p> <p>Note: The PingData (i.e. Directory, DataSync, PingAuthorize, DirectoryProxy)  products will all provide this</p> <p>This document is auto-generated from pingintelligence/opt/staging/hooks/80-post-start.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingintelligence/hooks/pingintelligence.lib.sh/","title":"Ping Identity DevOps <code>pingintelligence</code> Hook - <code>pingintelligence.lib.sh</code>","text":"<p>This document is auto-generated from pingintelligence/opt/staging/hooks/pingintelligence.lib.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingtoolkit/","title":"Ping Identity DevOps Docker Image - <code>pingtoolkit</code>","text":"<p>This docker image includes the Ping Identity PingToolkit and associated hook scripts to create a container that can pull in a SERVER_PROFILE run scripts.  The typical use case of this image would be an init container or a pod/container to perform tasks aside a running set of pods/containers.</p>"},{"location":"docker-images/pingtoolkit/#related-docker-images","title":"Related Docker Images","text":"<ul> <li><code>pingidentity/pingbase</code> - Parent Image <p>This image inherits, and can use, Environment Variables from pingidentity/pingbase</p> </li> <li><code>pingidentity/pingcommon</code> - Common Ping files (i.e. hook scripts)</li> </ul>"},{"location":"docker-images/pingtoolkit/#environment-variables","title":"Environment Variables","text":"<p>In addition to environment variables inherited from pingidentity/pingbase, the following environment <code>ENV</code> variables can be used with this image.</p> ENV Variable Default Description SHIM ${SHIM} IMAGE_VERSION ${IMAGE_VERSION} IMAGE_GIT_REV ${IMAGE_GIT_REV} DATE ${DATE} PING_PRODUCT_VERSION ${VERSION} PING_PRODUCT PingToolkit Ping product name STARTUP_COMMAND tail The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS -f /dev/null The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container"},{"location":"docker-images/pingtoolkit/#docker-container-hook-scripts","title":"Docker Container Hook Scripts","text":"<p>Please go here for details on all pingtoolkit hook scripts</p> <p>This document is auto-generated from pingtoolkit/Dockerfile</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingtoolkit/hooks/","title":"Ping Identity DevOps <code>pingtoolkit</code> Hooks","text":"<p>List of available hooks: * 17-check-license.sh</p> <p>These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon)</p> <p>This document is auto-generated from pingtoolkit/opt/staging/hooks</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"docker-images/pingtoolkit/hooks/17-check-license.sh/","title":"Ping Identity DevOps <code>pingtoolkit</code> Hook - <code>17-check-license.sh</code>","text":"<p>This document is auto-generated from pingtoolkit/opt/staging/hooks/17-check-license.sh</p> <p>Copyright \u00a9 2024 Ping Identity Corporation. All rights reserved.</p>"},{"location":"get-started/configVars/","title":"Configuration &amp; Environment Variables","text":"<p>Configuration and Environment variables allow users to cache secure and repetitive settings into a <code>pingctl</code> config file.  The default location of the file is <code>~/.pingidentity/config</code>.</p> <p>You can specify a given configuration item in one of three ways: the <code>pingctl</code> config file, the user's current environment variables, or through command line arguments.  The order of priority (highest to lowest) is:</p> <ul> <li>Command-Line argument overrides (when available)</li> <li><code>pingctl</code> config file</li> <li>Environment variable overrides</li> </ul>"},{"location":"get-started/configVars/#pingone-variables","title":"PingOne Variables","text":"<p>The standard PingOne variables used by <code>pingctl</code> are as follows:</p> Variable Description PINGONE_API_URL PingOne API URL (i.e. api.pingone.com/v1) PINGONE_AUTH_URL PingOne Auth URL (i.e. auth.pingone.com, auth.pingone.eu, auth.pingone.asia) PINGONE_ENVIRONMENT_ID PingOne Environment ID GUID PINGONE_WORKER_APP_CLIENT_ID PingOne Worker App ID GUID with access to PingOne Environment PINGONE_WORKER_APP_GRANT_TYPE PingOne Worker App Grant Type to use.  Should be one of authorization_code, implicit or client_credential PINGONE_WORKER_APP_REDIRECT_URI PingOne Worker App available redirect_uri.  Defaults to http://localhost:8000 PINGONE_WORKER_APP_CLIENT_SECRET PingOne Worker App Secret providing authentication to PingOne Worker App ID GUID"},{"location":"get-started/configVars/#legacy-ping-devops-variables","title":"Legacy ping-devops Variables","text":"<p>Prior to the <code>pingctl</code> CLI tool, <code>ping-devops</code> was used to help with the management of docker, docker-console, and kustomize deployments (this utility has been deprecated).  In configuring the legacy tool, several variables were used when deploying docker images into different environments.</p> <p>The legacy variables still supported and managed by <code>pingctl</code> are as follows:</p> Variable Description PING_IDENTITY_ACCEPT_EULA Specify <code>YES</code> or <code>NO</code> to accept Ping Identity EULA PING_IDENTITY_DEVOPS_USER Ping DevOps User PING_IDENTITY_DEVOPS_KEY Ping DevOps Key PING_IDENTITY_DEVOPS_HOME Home directory/path of your DevOps projects PING_IDENTITY_DEVOPS_REGISTRY Default Docker registry from which to pull images PING_IDENTITY_DEVOPS_TAG Default DevOps tag to use for deployments (i.e. 2205)"},{"location":"get-started/configVars/#pingctl-variables","title":"pingctl Variables","text":"<p>Additional variables honored by <code>pingctl</code> are:</p> Variable Description PINGCTL_CONFIG Location of the <code>pingctl</code> configuration file. Set as an environment variable only.  Default: <code>~/.pingidentity/config</code> PINGCTL_DEFAULT_OUTPUT Specifies default format of data returned. Command-Line arg <code>-o</code>. Default: <code>table</code> PINGCTL_DEFAULT_POPULATION Specifies default population to use for PingOne commands. Command-Line arg <code>-p</code>. Default: <code>Default</code> PINGCTL_OUTPUT_COLUMNS_{resource_type} Specify custom format of table csv data to be returned.   Command-Line arg <code>-c</code>. See more detail below PINGCTL_OUTPUT_SORT_{resource_type} Specify column to use for sorting data.   Command-Line arg <code>-s</code>. See more detail below"},{"location":"get-started/configVars/#pingctl_output_columns","title":"PINGCTL_OUTPUT_COLUMNS","text":"<p>There are two classes of variables provided by <code>PINGCTL_OUTPUT</code>:</p> <ul> <li> <p><code>PINGCTL_OUTPUT_COLUMNS_{resource}</code> - Specifies the columns to display whenever a <code>pingctl pingone get {resource}</code> command is used.</p> <p>Same as the <code>-c</code> option on the command-line (see pingctl pingone get command).</p> <p>Format of value should be constructed with <code>HeadingName:jsonName,HeadingName:jsonName</code>.  The best way to understand is by looking at the example of the default <code>USERS</code> resource:</p> <p>Example PINGCTL_OUTPUT_COLUMNS_USERS setting and output</p> <pre><code>PINGCTL_OUTPUT_COLUMNS_USERS=LastName:name.family,FirstName:name.given\n</code></pre> <p>Setting the above will generate output similar to:</p> <pre><code>```\n$ pingctl pingone get users\nLastName     FirstName\n--------     ---------\nBadham       Antonik\nAgn\u00e8s        Enterle\n--\n2 'USERS' returned\n```\n\nAlternatively, you can use the `-c` option as a command-line argument:\n\n```\n$ pingctl pingone get users -c \"LastName:name.family,FirstName:name.given,Username:username\"\nLastName     FirstName    Username\n--------     ---------    --------\nBadham       Antonik      antonik_adham\nAgn\u00e8s        Enterle      enterle_agn\u00e8s\n--\n2 'USERS' returned\n```\n</code></pre> </li> </ul>"},{"location":"get-started/configVars/#pingctl_output_sort","title":"PINGCTL_OUTPUT_SORT","text":"<ul> <li> <p><code>PINGCTL_OUTPUT_SORT_{resource}</code> - specifies the column on which to sort.</p> <p>Same as the <code>-s</code> option on the command-line (see pingctl pingone get command).</p> <p>Format of the value should be constructed with <code>jsonName</code>.  The name must be one of the entries in <code>PINGCTL_OUTPUT_COLUMNS_{resource}</code>.</p> <p>Example PINGCTL_OUTPUT_SORT_USERS setting and output</p> <pre><code>PINGCTL_OUTPUT_SORT_USERS=name.family\n</code></pre> <p>Setting the above will generate output similar to the following (note that the LastName (name.family) is sorted):</p> <pre><code>$ pingctl pingone get users\nLastName     FirstName\n--------     ---------\nAgn\u00e8s        Enterle\nBadham       Antonik\n--\n2 'USERS' returned\n</code></pre> <p>Alternatively, you can use the <code>-s</code> option as a command-line argument:</p> <pre><code>$ pingctl pingone get users -s \"name.given\"\nLastName     FirstName    Username\n--------     ---------    --------\nAgn\u00e8s        Enterle      enterle_agn\u00e8s\nBadham       Antonik      antonik_badham\n--\n2 'USERS' returned\n</code></pre> </li> </ul>"},{"location":"get-started/getStartedExample/","title":"Deploy an Example Stack","text":"<p>Video Demonstration</p> <p>A video demonstration of this example is available here.</p> <p>Versions Used</p> <p>This example was written using Docker Desktop with Kubernetes enabled on the Mac platform.  The version used for this guide was <code>4.27.1 (136059)</code>, which includes Docker Engine <code>v25.0.2</code> and Kubernetes <code>v1.29.1</code>.  The ingress-nginx controller version was <code>1.9.6</code>, deployed from Helm chart version <code>4.9.1</code>.</p> <p>Kubernetes Services Kubernetes versus Server-Deployed Applications</p> <p>If you are new to Kubernetes-based deployments, there is a distinct difference when running under Kubernetes compared to running applications on servers.  In a server model, many applications typically run on the same server, and you can access any of them using the same host. For example, many on-premise deployments of PingFederate also include the PingDataConsole, hosted on the same server.</p> <p>Under Kubernetes, however, each application that requires external access is associated with a <code>service</code>.  A service is a fixed endpoint in the cluster that routes traffic to a given application.  So, in this example, there are distinct service endpoints for PingFederate, PingDataConsole, and the other products.  </p> <p>In this demo, these service endpoints are load balanced using the Nginx ingress controller. By adding entries to the <code>/etc/hosts</code> file, you can access them using typical URL entries.</p> <p>The Ping Identity Helm Getting Started page has instructions on getting your environment configured for using the Ping Helm charts.</p> <p>For more examples, see Helm Chart Example Configurations.</p> <p>For more information on Helm with Ping products, see Ping Identity DevOps Helm Charts.</p>"},{"location":"get-started/getStartedExample/#what-you-will-do","title":"What You Will Do","text":"<p>After using Git to clone the <code>pingidentity-devops-getting-started</code> repository, you will use Helm to deploy a sample stack to a Kubernetes cluster.</p>"},{"location":"get-started/getStartedExample/#prerequisites","title":"Prerequisites","text":"<ul> <li>Register for the Ping DevOps program and install/configure <code>pingctl</code> with your User and Key</li> <li>Install Git</li> <li>Follow the instructions on the helm Getting Started page up through updating to the latest charts to ensure you have the latest version of our charts</li> <li>Access to a Kubernetes cluster. You can enable Kubernetes in Docker Desktop for a simple cluster, which was the cluster used for this guide (on the Mac platform).</li> </ul>"},{"location":"get-started/getStartedExample/#clone-the-getting-started-repository","title":"Clone the <code>getting-started</code> repository","text":"<ol> <li> <p>Clone the <code>pingidentity-devops-getting-started</code> repository to your local <code>${PING_IDENTITY_DEVOPS_HOME}</code> directory.</p> <p>The <code>${PING_IDENTITY_DEVOPS_HOME}</code> environment variable was set by running <code>pingctl config</code>.</p> <pre><code>cd \"${PING_IDENTITY_DEVOPS_HOME}\"\ngit clone \\\n  https://github.com/pingidentity/pingidentity-devops-getting-started.git\n</code></pre> </li> </ol>"},{"location":"get-started/getStartedExample/#deploy-the-example-stack","title":"Deploy the example stack","text":"<ol> <li> <p>Deploy the example stack of our product containers.</p> <p>Initial Deployment</p> <p>For this guide, avoid making changes to the <code>everything.yaml</code> file to ensure a successful first-time deployment.</p> <ol> <li> <p>Create a namespace for running the stack in your Kubernetes cluster.  </p> <pre><code># Create the namespace\nkubectl create ns pinghelm\n\n# Set the kubectl context to the namespace\nkubectl config set-context --current --namespace=pinghelm\n\n# Confirm\nkubectl config view --minify | grep namespace:\n</code></pre> </li> <li> <p>Deploy the ingress controller to Docker Desktop:</p> <pre><code>helm upgrade --install ingress-nginx ingress-nginx \\\n--repo https://kubernetes.github.io/ingress-nginx \\\n--namespace ingress-nginx --create-namespace\n</code></pre> </li> <li> <p>To wait for the Nginx ingress to reach a healthy state, run the following command.  You can also observe the pod status using k9s or by running <code>kubectl get pods --namespace ingress-nginx</code>. You should see one controller pod running when the ingress controller is ready.  This command should exit after no more than 90 seconds or so, depending on the speed of your computer:</p> <pre><code>kubectl wait --namespace ingress-nginx \\\n  --for=condition=ready pod \\\n  --selector=app.kubernetes.io/component=controller \\\n  --timeout=90s\n</code></pre> </li> <li> <p>Create a secret in the namespace you will be using to run the example (pinghelm) using the <code>pingctl</code> utility. This secret will obtain an evaluation license based on your Ping DevOps username and key:</p> <pre><code>pingctl k8s generate devops-secret | kubectl apply -f -\n</code></pre> </li> <li> <p>This example will use the Helm release name <code>demo</code> and DNS domain suffix <code>*pingdemo.example</code> for accessing applications.  Add all expected hosts to <code>/etc/hosts</code>:</p> <pre><code>echo '127.0.0.1 demo-pingaccess-admin.pingdemo.example demo-pingaccess-engine.pingdemo.example demo-pingauthorize.pingdemo.example demo-pingauthorizepap.pingdemo.example demo-pingdataconsole.pingdemo.example demo-pingdelegator.pingdemo.example demo-pingdirectory.pingdemo.example demo-pingfederate-admin.pingdemo.example demo-pingfederate-engine.pingdemo.example demo-pingcentral.pingdemo.example' | sudo tee -a /etc/hosts &gt; /dev/null\n</code></pre> </li> <li> <p>To install the chart, go to your local <code>\"${PING_IDENTITY_DEVOPS_HOME}\"/pingidentity-devops-getting-started/30-helm</code> directory and run the command shown here.  In this example, the release (deployment into Kubernetes by Helm) is called <code>demo</code>, forming the prefix for all objects created. The <code>ingress-demo.yaml</code> file configures the ingresses for the products to use the ping-local domain:</p> <pre><code>helm upgrade --install demo pingidentity/ping-devops -f everything.yaml -f ingress-demo.yaml\n</code></pre> <p>The latest product Docker images are automatically downloaded if they have not previously been pulled from Docker Hub.</p> <p>Sample output:</p> <pre><code>NAME: demo\nLAST DEPLOYED: Tue Feb  6 13:04:07 2024\nNAMESPACE: pinghelm\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\n#-------------------------------------------------------------------------------------\n# Ping DevOps\n#\n# Description: Ping Identity helm charts - 02/05/2024\n#-------------------------------------------------------------------------------------\n#\n#           Product          tag   typ  #  cpu R/L   mem R/L  Ing\n#    --------------------- ------- --- -- --------- --------- ---\n#    global                2401              0/0       0/0     \u221a\n#\n#  \u221a pingaccess-admin      2401    sts  1    0/2     1Gi/4Gi   \u221a\n#  \u221a pingaccess-engine     2401    dep  1    0/2     1Gi/4Gi   \u221a\n#  \u221a pingauthorize         2401    dep  1    0/2    1.5G/4Gi   \u221a\n#    pingauthorizepap\n#    pingcentral\n#  \u221a pingdataconsole       2401    dep  1    0/2    .5Gi/2Gi   \u221a\n#    pingdatasync\n#    pingdelegator\n#  \u221a pingdirectory         2401    sts  1  50m/2     2Gi/8Gi   \u221a\n#    pingdirectoryproxy\n#  \u221a pingfederate-admin    2401    dep  1    0/2     1Gi/4Gi   \u221a\n#  \u221a pingfederate-engine   2401    dep  1    0/2     1Gi/4Gi   \u221a\n#    pingintelligence\n#\n#    ldap-sdk-tools\n#    pd-replication-timing\n#    pingtoolkit\n#\n#-------------------------------------------------------------------------------------\n# To see values info, simply set one of the following on your helm install/upgrade\n#\n#    --set help.values=all         # Provides all (i.e. .Values, .Release, .Chart, ...) yaml\n#    --set help.values=global      # Provides global values\n#    --set help.values={ image }   # Provides image values merged with global\n#-------------------------------------------------------------------------------------\n</code></pre> <p>As you can see, PingAccess Admin and Engine, PingData Console, PingDirectory, PingAuthorize, and the PingFederate Admin and Engine are deployed from the provided <code>everything.yaml</code> values file.</p> <p>It will take several minutes for all components to become operational.</p> </li> <li> <p>To display the status of the deployed components, you can use k9s or issue the corresponding commands shown here:</p> <ul> <li>Display the services (endpoints for connecting) by running <code>kubectl get service --selector=app.kubernetes.io/instance=demo</code></li> </ul> <pre><code>NAME                            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                   AGE\ndemo-pingaccess-admin           ClusterIP   10.106.227.103   &lt;none&gt;        9090/TCP,9000/TCP         15m\ndemo-pingaccess-admin-cluster   ClusterIP   None             &lt;none&gt;        &lt;none&gt;                    15m\ndemo-pingaccess-engine          ClusterIP   10.108.102.245   &lt;none&gt;        3000/TCP                  15m\ndemo-pingauthorize              ClusterIP   10.110.95.132    &lt;none&gt;        443/TCP                   15m\ndemo-pingauthorize-cluster      ClusterIP   None             &lt;none&gt;        1636/TCP                  15m\ndemo-pingdataconsole            ClusterIP   10.97.81.22      &lt;none&gt;        8443/TCP                  15m\ndemo-pingdirectory              ClusterIP   10.102.91.214    &lt;none&gt;        443/TCP,389/TCP,636/TCP   15m\ndemo-pingdirectory-cluster      ClusterIP   None             &lt;none&gt;        1636/TCP                  15m\ndemo-pingfederate-admin         ClusterIP   10.99.145.24     &lt;none&gt;        9999/TCP                  15m\ndemo-pingfederate-cluster       ClusterIP   None             &lt;none&gt;        7600/TCP,7700/TCP         15m\ndemo-pingfederate-engine        ClusterIP   10.108.240.203   &lt;none&gt;        9031/TCP                  15m\n</code></pre> <ul> <li>To view the pods, run <code>kubectl get pods --selector=app.kubernetes.io/instance=demo</code> - you will need to run this at intervals until all pods have started (** Running ** status):</li> </ul> <pre><code>NAME                                        READY   STATUS    RESTARTS   AGE\ndemo-pingaccess-admin-0                     1/1     Running   0          7m29s\ndemo-pingaccess-engine-cf9987bb5-npspz      1/1     Running   0          7m52s\ndemo-pingauthorize-8bdfd4fd8-j82zg          1/1     Running   0          7m43s\ndemo-pingdataconsole-7c875985d4-mfjbq       1/1     Running   0          17m\ndemo-pingdirectory-0                        1/1     Running   0          7m38s\ndemo-pingfederate-admin-5786787dfd-5b5s5    1/1     Running   0          7m36s\ndemo-pingfederate-engine-5ff6546f4f-7jfnt   1/1     Running   0          7m31s\n</code></pre> <ul> <li>To see the ingresses you will use to access the product, run <code>kubectl get ingress</code>. If the ingress controller is configured properly, you should see <code>localhost</code> as the address as shown here:</li> </ul> <pre><code>NAME                       CLASS    HOSTS                                     ADDRESS     PORTS     AGE\ndemo-pingaccess-admin      nginx   demo-pingaccess-admin.pingdemo.example      localhost   80, 443   9m50s\ndemo-pingaccess-engine     nginx   demo-pingaccess-engine.pingdemo.example     localhost   80, 443   9m50s\ndemo-pingauthorize         nginx   demo-pingauthorize.pingdemo.example         localhost   80, 443   9m50s\ndemo-pingdataconsole       nginx   demo-pingdataconsole.pingdemo.example       localhost   80, 443   9m50s\ndemo-pingdirectory         nginx   demo-pingdirectory.pingdemo.example         localhost   80, 443   9m50s\ndemo-pingfederate-admin    nginx   demo-pingfederate-admin.pingdemo.example    localhost   80, 443   9m50s\ndemo-pingfederate-engine   nginx   demo-pingfederate-engine.pingdemo.example   localhost   80, 443   9m50s\n</code></pre> <p>Address must be localhost</p> <p>If the ingress controller is working properly, the ingress definitions will all report the ADDRESS column as <code>localhost</code> as shown above.  If you do not see this entry, then you will not be able to access the services later.  This problem is due to a known error with Docker Desktop and the embedded virtual machine (VM) used on the Mac and Windows platform in combination with the ingress controller. To correct the problem, uninstall the chart as instructed at the bottom of this page and restart Docker Desktop.  Afterward, you can re-run the helm command to install the Ping products as instructed above.  The issue appears to be related to a stale networking configuration under the covers of Docker Desktop.</p> <ul> <li>To see everything tied to the helm release run <code>kubectl get all --selector=app.kubernetes.io/instance=demo</code>:</li> </ul> <pre><code>NAME                                            READY   STATUS    RESTARTS   AGE\npod/demo-pingaccess-admin-0                     1/1     Running   0          107m\npod/demo-pingaccess-engine-cf9987bb5-npspz      1/1     Running   0          107m\npod/demo-pingauthorize-8bdfd4fd8-j82zg          1/1     Running   0          107m\npod/demo-pingdataconsole-7c875985d4-mfjbq       1/1     Running   0          116m\npod/demo-pingdirectory-0                        1/1     Running   0          107m\npod/demo-pingfederate-admin-5786787dfd-5b5s5    1/1     Running   0          107m\npod/demo-pingfederate-engine-5ff6546f4f-7jfnt   1/1     Running   0          107m\n\nNAME                                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                   AGE\nservice/demo-pingaccess-admin           ClusterIP   10.96.234.72     &lt;none&gt;        9090/TCP,9000/TCP         116m\nservice/demo-pingaccess-admin-cluster   ClusterIP   None             &lt;none&gt;        &lt;none&gt;                    116m\nservice/demo-pingaccess-engine          ClusterIP   10.106.190.217   &lt;none&gt;        3000/TCP                  116m\nservice/demo-pingauthorize              ClusterIP   10.104.246.123   &lt;none&gt;        443/TCP                   116m\nservice/demo-pingauthorize-cluster      ClusterIP   None             &lt;none&gt;        1636/TCP                  116m\nservice/demo-pingdataconsole            ClusterIP   10.96.166.28     &lt;none&gt;        8443/TCP                  116m\nservice/demo-pingdirectory              ClusterIP   10.107.42.8      &lt;none&gt;        443/TCP,389/TCP,636/TCP   116m\nservice/demo-pingdirectory-cluster      ClusterIP   None             &lt;none&gt;        1636/TCP                  116m\nservice/demo-pingfederate-admin         ClusterIP   10.105.26.94     &lt;none&gt;        9999/TCP                  116m\nservice/demo-pingfederate-cluster       ClusterIP   None             &lt;none&gt;        7600/TCP,7700/TCP         116m\nservice/demo-pingfederate-engine        ClusterIP   10.99.223.48     &lt;none&gt;        9031/TCP                  116m\n\nNAME                                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/demo-pingaccess-engine     1/1     1            1           116m\ndeployment.apps/demo-pingauthorize         1/1     1            1           116m\ndeployment.apps/demo-pingdataconsole       1/1     1            1           116m\ndeployment.apps/demo-pingfederate-admin    1/1     1            1           116m\ndeployment.apps/demo-pingfederate-engine   1/1     1            1           116m\n\nNAME                                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/demo-pingaccess-engine-cf9987bb5      1         1         1       116m\nreplicaset.apps/demo-pingauthorize-8bdfd4fd8          1         1         1       116m\nreplicaset.apps/demo-pingdataconsole-7c875985d4       1         1         1       116m\nreplicaset.apps/demo-pingfederate-admin-5786787dfd    1         1         1       116m\nreplicaset.apps/demo-pingfederate-engine-5ff6546f4f   1         1         1       116m\n\nNAME                                     READY   AGE\nstatefulset.apps/demo-pingaccess-admin   1/1     116m\nstatefulset.apps/demo-pingdirectory      1/1     116m\n</code></pre> <ul> <li>To view logs, look at the logs for the deployment of the product in question.  For example:</li> </ul> <pre><code> kubectl logs -f deployment/demo-pingfederate-admin\n</code></pre> </li> </ol> </li> <li> <p>These are the URLs and credentials to sign on to the management consoles for the products.</p> <p>Certificates</p> <p>This example uses self-signed certificates that will have to be accepted in your browser or added to your keystore.</p> <p>With the ingresses in place, you can access the products at these URLs:</p> Product Connection Details PingFederate <ul> <li>URL: https://demo-pingfederate-admin.pingdemo.example/pingfederate/app</li><li>Username: administrator</li><li>Password: 2FederateM0re</li></ul> PingDirectory <ul><li>URL: https://demo-pingdataconsole.pingdemo.example/console</li><li>Server: ldaps://demo-pingdirectory-cluster:1636</li><li>Username: administrator</li><li>Password: 2FederateM0re</li></ul> PingAccess <ul><li>URL: https://demo-pingaccess-admin.pingdemo.example/</li><li>Username: administrator</li><li>Password: 2FederateM0re</li></ul> PingAuthorize <ul><li>URL: https://demo-pingdataconsole.pingdemo.example/console</li><li>Server: ldaps://demo-pingauthorize-cluster:1636</li><li>Username: administrator</li><li>Password: 2FederateM0re</li></ul> </li> <li> <p>When you are finished, you can remove the demonstration components by running the uninstall command for helm:</p> <pre><code>helm uninstall demo\n</code></pre> </li> </ol>"},{"location":"get-started/getStartedExample/#next-steps","title":"Next Steps","text":"<p>Now that you have deployed a set of our product images using the provided chart, you can move on to deployments using configurations that more closely reflect use cases to be explored.  Refer to the helm examples) page for other typical deployments.</p> <p>Container logging</p> <p>Maintaining logs in a containerized model is different from the typical server-deployed application.  See this page for additional details.</p>"},{"location":"get-started/introduction/","title":"Introduction","text":"<p>This section outlines ways to easily deploy Ping products with pre-defined configurations.  After you have successfully deployed using this example, you can try other provided examples or move on to customizing the products to fit your needs and environment.</p> <p>You will need to register for the Ping DevOps program in order to obtain trial licenses for evaluating or testing with our products.</p> <p>After registering at the link above, you will be provided a username and key.  These credentials provide a temporary license for your evaluation.  See using your DevOps User and Key for instructions on use.</p> <p>Finally, to manage PingOne resources using credentials other than your own, you are required to have a PingOne Worker App. See this configuration page for more details on configuration.</p>"},{"location":"get-started/prereqs/","title":"Prerequisites","text":"<p>In order to use our resources, you will need the following components, software, or other information.</p>"},{"location":"get-started/prereqs/#product-license","title":"Product license","text":"<p>You must have a product license to run our images. You may either use an evaluation license or existing license.</p>"},{"location":"get-started/prereqs/#evaluation-license","title":"Evaluation License","text":"<p>Generate an evaluation license obtained with a valid DevOps user key.  </p> <p>When you register for Ping Identity's DevOps program, you are issued credentials that automate the process of retrieving an evaluation product license.</p> <p>DevOps User and Key</p> <p>For more information about using your DevOps program user and key in various ways (including Kubernetes and with stand-alone containers) see this how-to guide: Using Your Devops User and Key</p> <p>Evaluation License</p> <p>Evaluation licenses are short-lived (30 days) and must not be used in production deployments.</p> <p>Evaluation licenses can only be used with images published in the last 90 days.  If you want to continue to use an image that was published more than 90 days ago, you must obtain a product license.</p>"},{"location":"get-started/prereqs/#existing-license","title":"Existing License","text":"<p>If you possess a product license for the product, you can use it with supported versions of the image (including those over 90 days old mentioned above) by following these instructions to mount the product license.</p> <p>Mount paths</p> <p>The mount points and name of the license file vary by product.  The link above provides the proper location and name for these files.</p>"},{"location":"get-started/prereqs/#local-runtime-environment","title":"Local runtime environment","text":"<p>The initial example uses Kubernetes under Docker Desktop because it does not require a lot of configuration.</p> <p>In order to try Ping products in a manner most similar to typical production installations, you should consider using a Kubernetes environment. Kind (Kubernetes in Docker) provides a platform to get started with local Kubernetes development.  Instructions for setting up a Kind cluster are here.</p> <p>Other local Kubernetes environments include Rancher Desktop, Docker Desktop with Kubernetes enabled, and minikube.</p> <p>Rancher Desktop</p> <p>Rancher Desktop is compatible with Linux, MacOS, and Windows (using WSL). It also supports the docker container runtime, which provides support for running docker commands without installing individual docker components or Docker Desktop.  </p> <p>For running Docker Compose deployments of single products, any Docker Desktop installation or Linux system with Docker and <code>docker compose</code> installed can be used.</p>"},{"location":"get-started/prereqs/#applications-utilities","title":"Applications / Utilities","text":"<ul> <li>Helm cli</li> <li>kubectl</li> <li>Homebrew for package installation and management.  Homebrew can be used to install k9s, kubectl, helm, and other programs.    <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre></li> <li>pingctl <pre><code>brew install pingidentity/tap/pingctl\n</code></pre></li> </ul>"},{"location":"get-started/prereqs/#recommended-additional-utilities","title":"Recommended Additional Utilities","text":"<ul> <li>k9s <pre><code>brew install derailed/k9s/k9s\n</code></pre></li> <li>kubectx <pre><code>brew install kubectx\n</code></pre></li> <li> <p>docker-compose <pre><code>brew install docker-compose\n</code></pre></p> <p>Info</p> <p>Installing docker-compose is only necessary to deploy Docker containers when using Docker with Rancher Desktop. It is included with the Docker Desktop installation.</p> <p>See Rancher preferences to switch from containerd to dockerd (moby).</p> </li> </ul>"},{"location":"get-started/prereqs/#configure-the-environment","title":"Configure the Environment","text":"<ol> <li> <p>Open a terminal and create a local DevOps directory named <code>${HOME}/projects/devops</code>.</p> <p>Info</p> <p>${HOME}/projects/devops is the parent directory for all examples referenced in our documentation.</p> </li> <li> <p>Configure the environment as follows.</p> <pre><code>pingctl config\n</code></pre> <ol> <li> <p>Respond to all configuration questions, accepting the defaults if uncertain. Settings for custom variables aren't needed initially but may be necessary for additional capabilities.</p> </li> <li> <p>All responses are captured in your local <code>~/.pingidentity/config</code> file. Allow the configuration script to source this file in your shell profile (for example, <code>~/.bash_profile</code> in a bash shell).</p> </li> </ol> </li> <li> <p>[Optional] Export configured pingctl variables as environment variables</p> <ol> <li>Modify your shell profile (for example, <code>~/.bash_profile</code> in a bash shell) so that the generated <code>source ~/.pingidentity/config</code> command is surrounded by <code>set -a</code> and <code>set +a</code> statements.</li> </ol> <pre><code>set -a\n# Ping Identity - Added with 'pingctl config' on Fri Apr 22 13:57:04 MDT 2022\ntest -f '${HOME}/.pingidentity/config' &amp;&amp; source '${HOME}/.pingidentity/config'\nset +a\n</code></pre> <ol> <li>Verify configured variables are exported in your environment.<pre><code>1. Restart your shell or source your shell profile.\n\n2. Run `env | grep 'PING'`\n</code></pre> </li> </ol> </li> <li> <p>To display your environment settings, run:</p> <pre><code>pingctl info\n</code></pre> </li> <li> <p>For more information on the options available for <code>pingctl</code> see Configuration &amp; Environment Variables.</p> </li> </ol>"},{"location":"home/3rdPartySoftware/","title":"Third-Party Software","text":""},{"location":"home/3rdPartySoftware/#product-images","title":"Product Images","text":"<p>Ping Identity Docker images bundle various third-party tools to enable product functionality. For more details, click on the links below:</p> <ul> <li> <p>OpenJDK: GNU General Public License version 2.0</p> </li> <li> <p>OpenSSH: Based on BSD licensing</p> </li> <li> <p>Git: GNU General Public License version 2.0</p> </li> <li> <p>Gettext: GNU General Public License version 2.0</p> </li> <li> <p>Curl: Based on MIT/X license</p> </li> <li> <p>ca-certificates: GNU General Public License version 2.0</p> </li> <li> <p>Jq: MIT licensing</p> </li> <li> <p>Gnupg: GNU General Public License</p> </li> </ul>"},{"location":"home/3rdPartySoftware/#questions-about-3rd-party-software-or-services","title":"Questions about 3rd Party Software or Services","text":"<p>Thanks again for using this portal.  Our goal is to keep it current, relevant, and as error-free as possible.</p> <p>Please refer to the disclaimer as background for this section.</p> <p>As a reminder, use the methods on the Contact Us page if you encounter issues directly related to this portal, files or product container images provided by Ping, or with any suggestions you may have.</p> <p>When seeking help with content on this portal, consider the following:</p> <ol> <li>Product-related questions:  For questions about the Ping products that run inside these images, customers with valid support contracts should engage through our Support Portal.</li> <li>Questions specific to Ping product container images: Customers with valid support contracts should engage through our Support Portal. Please specify that the problem is with an image provided by the DevOps program</li> <li>Examples on this portal:  If you are following an example step-by-step as given on this portal, and it does not work, contact us at the link above.</li> <li>General questions about Ping DevOps: Consult our FAQ page, or use an internet search for the information.</li> <li> <p>Any code in these Github repositories: Contact us using information from the link above or use the bug tracking link on the repository:</p> <ol> <li>Getting Started</li> <li>Docker Builds</li> <li>Helm Charts</li> <li>Server Profiles</li> </ol> </li> </ol>"},{"location":"home/3rdPartySoftware/#other-software-mentioned-or-used-on-this-portal-not-comprehensive","title":"Other software mentioned or used on this portal (not comprehensive)","text":"<p>The link will take you to the main page for each product or service.  You should also explore the forums and documentation for each if you encounter an issue:</p> <ul> <li>Helm</li> <li>Docker Desktop</li> <li>Kubernetes</li> <li>kind</li> <li>minikube</li> <li>Openshift Local</li> <li>ingress-nginx</li> <li>externalDNS</li> <li>MetalLB</li> <li>kubectl</li> <li>k9s</li> <li>Prometheus</li> <li>Grafana</li> <li>telegraf-operator</li> </ul>"},{"location":"home/disclaimer/","title":"Disclaimer","text":"<p>Every effort is made by Ping Identity\u2019s DevOps team to provide supporting documents and examples for our products.</p> <p>However, Ping Identity cannot support custom scripts or template technology. For further support, please contact your Ping Identity representative.</p> <p>Copyright (C) 2023 Ping Identity Corporation</p> <p>All rights reserved.</p> <p>Ping Identity Corporation 1099 18th St Suite 2950 Denver, CO 80202 303.468.2900 http://www.pingidentity.com</p>"},{"location":"home/disclaimer/#disclaimer-of-warranties","title":"Disclaimer Of Warranties","text":"<p>THE SOFTWARE PROVIDED HEREUNDER IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT ANY WARRANTIES OR REPRESENTATIONS EXPRESS, IMPLIED OR STATUTORY; INCLUDING, WITHOUT LIMITATION, WARRANTIES OF QUALITY, PERFORMANCE, NONINFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.  NOR ARE THERE ANY WARRANTIES CREATED BY A COURSE OR DEALING, COURSE OF PERFORMANCE OR TRADE USAGE.  FURTHERMORE, THERE ARE NO WARRANTIES THAT THE SOFTWARE WILL MEET YOUR NEEDS OR BE FREE FROM ERRORS, OR THAT THE OPERATION OF THE SOFTWARE WILL BE UNINTERRUPTED.  IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"home/license/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li>Definitions.</li> </ol> <p>\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.</p> <ol> <li>Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.</li> <li>Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.</li> <li>Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:</li> </ol> <p>(a) You must give any other recipients of the Work or Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.</p> <ol> <li>Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.</li> <li>Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.</li> <li>Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.</li> <li>Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.</li> <li>Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.</li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <p>To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives.</p> <p>Copyright 2023 Ping Identity Corp.</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"home/portalUpdates/","title":"Recent portal updates","text":"<p>This page provides details on significant changes to this portal.</p>"},{"location":"home/portalUpdates/#september-2023","title":"September 2023","text":""},{"location":"home/portalUpdates/#a-statement-on-aws-efsebs","title":"A statement on AWS EFS/EBS","text":"<p>Some of the product documentation does not explicitly explain the recommended storage solution to use when running on AWS.  Clarification was added, particularly when running Ping products in containerized environments here.</p>"},{"location":"home/portalUpdates/#july-2023","title":"July 2023","text":""},{"location":"home/portalUpdates/#read-only-filesystem-example-and-explanation","title":"Read-only filesystem example and explanation","text":"<p>Some environments require the use of a read-only filesystem on containers in runtime.  As our product images mature to better align with this practice, a way to work around the situation with our images as they stand today can be found here.</p>"},{"location":"home/portalUpdates/#november-2022","title":"November 2022","text":""},{"location":"home/portalUpdates/#video-content-is-arriving","title":"Video content is arriving","text":"<p>In recent weeks, videos on topics relevant to the DevOps program at Ping have started to be released.  Look for links on some pages where the videos are relevant, or you can browse all videos here.</p>"},{"location":"home/portalUpdates/#july-2022","title":"July 2022","text":""},{"location":"home/portalUpdates/#removal-of-docker-compose-examples","title":"Removal of Docker Compose examples","text":"<p>With the focus of using Helm to provision to Kubernetes as the recommended practice, the multi-product Docker Compose examples were no longer maintained or supported and have been removed.  The only examples that remain are for single-product deployments. For orchestrating multiple products, see the Helm examples.</p>"},{"location":"home/portalUpdates/#removal-of-kubernetes-kustomize-examples","title":"Removal of Kubernetes Kustomize examples","text":"<p>Kustomize was the former method for for Kubernetes orchestration. As most of those examples were replaced by Helm, the Kustomize guides and files were also removed from this portal.</p>"},{"location":"home/portalUpdates/#confirmation-of-documentation","title":"Confirmation of Documentation","text":"<p>We received reports in recent weeks that some of examples were not working due to changes in Kubernetes version support, files that had been removed from this repository, and other reasons. All examples and guides on this portal have been reviewed and should work as written.</p>"},{"location":"home/supportPolicy/","title":"Ping Identity DevOps Support Policy","text":"<p>This DevOps Support Policy is an extension of the Ping Identity Support Policy.</p> <p>This Ping Identity Corporation (\"Ping Identity\") DevOps Support Policy (this \"Policy\") encompasses all support obligations that Ping Identity has toward you as Ping Identity\u2019s Customer (\"Customer\").</p> <p>Docker Image Support Policy</p> <p>The support policy for Ping Identity product Docker images is found at Docker Image Support Policy.</p>"},{"location":"home/supportPolicy/#included-in-support","title":"Included in Support","text":"<ul> <li>Providing base images for Ping Identity products to Customers</li> <li>Providing documentation and basic examples for Helm deployments using Ping Identity's Helm charts</li> <li>Providing the Customer with DevOps tooling, such as <code>config_export</code> and <code>pingctl</code></li> <li>Providing the Customer direction in using Server Profile to achieve the following:<ul> <li>Deployment</li> <li>Customization</li> <li>Saving Configuration</li> <li>Layering</li> <li>Environment Substitution</li> <li>Private Github repos</li> </ul> </li> </ul>"},{"location":"home/supportPolicy/#supported-orchestration-tools","title":"Supported orchestration tools","text":"Tool Description Kubernetes Also known as K8s, Kubernetes is an open-source system for automating deployment, scaling, and management of containerized software. Helm charts Helm is the easiest way to deploy Ping Identity software images in a Kubernetes environment. Docker images Docker images are maintained by Ping Identity and are a collection of preconfigured environments for Ping Identity products. GitHub repositories These repositories provide all of the components to build Docker images for your own development, testing and deployments."},{"location":"home/supportPolicy/#resources","title":"Resources","text":"<p>Ping Identity customers can create a case in the Ping Identity Support Portal.</p> <p>Non-Ping Identity customers can use the PingDevOps Community.</p>"},{"location":"how-to/addMOTD/","title":"Adding a MOTD","text":"<p>You can create a message of the day (MOTD) JSON file to be provide an MOTD file to our product containers when they start.</p>"},{"location":"how-to/addMOTD/#before-you-begin","title":"Before you begin","text":"<p>You must:</p> <ul> <li>Complete the Get Started example to set up your DevOps environment and run a test deployment of the products.</li> </ul>"},{"location":"how-to/addMOTD/#using-a-motd-file","title":"Using a MOTD file","text":"<p>To employ a MOTD file:</p> <ol> <li> <p>Edit the existing <code>motd.json</code> file:</p> <ol> <li>Edit the motd/motd.json file located in your local <code>pingidentity-devops-getting-started/motd</code> folder.</li> </ol> </li> <li> <p>Create a <code>motd.json</code> file in the location of your server profile:</p> <ol> <li>Create a <code>motd.json</code> file in the root of the server profile directory being referenced.</li> </ol> <p>This <code>motd.json</code> file will be appended to the <code>/etc/motd</code> file used by the provided image.</p> </li> </ol>"},{"location":"how-to/addMOTD/#testing-the-motd-file","title":"Testing the MOTD file","text":"<p>Test the new messages in the <code>motd.json</code> file using the <code>test-motd.sh</code> script. The script supplies the <code>JQ_EXPR</code> value used to pass the message data to the container.</p> <ol> <li> <p>To test the <code>motd.json</code> file locally for our example use cases, from the <code>pingidentity-devops-getting-started/motd</code> directory, enter:</p> <pre><code>./test-motd.sh local\n</code></pre> </li> <li> <p>To test the <code>motd.json</code> file you created in your server profile directory:</p> <ol> <li> <p>Copy the <code>test-motd.sh</code> script located in the <code>pingidentity-devops-getting-started/motd</code> directory to your server profile directory.</p> </li> <li> <p>Enter:</p> <pre><code>./test-motd.sh local\n</code></pre> </li> </ol> </li> <li> <p>To test the <code>motd.json</code> with a server profile located in a Github repository:</p> <ol> <li> <p>Ensure the <code>test-motd.sh</code> script is located in the local, cloned repository.</p> </li> <li> <p>From the local, cloned repository, enter:</p> <pre><code>./test-motd.sh github\n</code></pre> </li> </ol> </li> </ol>"},{"location":"how-to/addMOTD/#example-motdjson","title":"Example motd.json","text":"<p>The example below shows the messages that are displayed for all product images.</p> <p>For this example, the messages are only shown from the <code>validFrom</code> to <code>validTo</code> dates:</p> <pre><code>{\n    \"devops\" : [\n        {\n            \"validFrom\": 20220701,\n            \"validTo\": 20220730,\n            \"subject\": \"General Message 1\",\n            \"message\": [\"This is line # 1\",\n                        \"\",\n                        \"This is line # 3\",]\n        },\n        {\n            \"validFrom\": 20220801,\n            \"validTo\": 20220830,\n            \"subject\": \"General Message 2\",\n            \"message\": [\"Message goes here\"]\n        }\n    ],\n    \"pingfederate\" : [\n        {\n            \"validFrom\": 20220701,\n            \"validTo\": 20220830,\n            \"subject\": \"PingFederate Message 1\",\n            \"message\": [\"Message goes here\"]\n        }\n    ]\n}\n</code></pre>"},{"location":"how-to/assignPFNodeId/","title":"Assigning a Provisioner Node ID for PingFederate Pods","text":"<p>The PingFederate provisioner node id is set in the run.properties file used to configure the server. This value is used to set up failover for provisioning.</p> <p>When using failover, each provisioning server must be given a unique index. By default with no server profile, the <code>PF_PROVISIONER_NODE_ID</code> environment variable is used to set the node id, with a default value of 1.</p> <p>If it is necessary to set node ids for PingFederate servers, a StatefulSet can be used to provide consistent hostnames for individual Pods. The node id can then be parsed from these hostnames.</p> <p>Warning</p> <p>The use of a StatefulSet instead of a Deployment for PingFederate has some consequences. In particular, updates to the StatefulSet will be done as a rolling updates, which can increase the time needed for an update.</p> <p>In the Pod spec for the StatefulSet: <pre><code>        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n</code></pre></p> <p>Then, a hook script can be used to parse the pod ordinal, which can then be set as the node id. Note that this will overwrite the default <code>PF_PROVISIONER_NODE_ID</code> value.</p> <p>In your server profile, create a <code>02-get-remote-server-profiles.sh.post</code> script to update the environment variable: <pre><code>#!/usr/bin/env sh\n. \"${HOOKS_DIR}/pingcommon.lib.sh\"\n\n# Parse the pod ordinal\nPF_PROVISIONER_NODE_ID=${POD_NAME##*-}\n\n# Add one to the ordinal so that node id starts at 1 instead of 0\nPF_PROVISIONER_NODE_ID=$((PF_PROVISIONER_NODE_ID+1))\n\n# Save the node id to the environment used by the hook scripts\nexport_container_env PF_PROVISIONER_NODE_ID\n</code></pre></p> <p>Ensure your server profile uses this environment variable if you are providing a custom <code>instance/bin/run.properties.subst</code> file: <pre><code>provisioner.node.id=${PF_PROVISIONER_NODE_ID}\n</code></pre></p>"},{"location":"how-to/buildLocal/","title":"Building a Docker product image locally","text":"<p>This page describes the process to build a Docker image of our products with the build tools found in our Docker Builds repository and a local copy of a product .zip archive.</p> <p>Video demonstration</p> <p>For a video demonstration of this process, visit this link.</p> Docker Builds"},{"location":"how-to/buildLocal/#cloning-a-build-repository","title":"Cloning a build repository","text":"<p>Open a terminal and clone the <code>pingidentity-docker-builds</code> repo:</p> <pre><code>git clone https://github.com/pingidentity/pingidentity-docker-builds.git\n</code></pre>"},{"location":"how-to/buildLocal/#download-a-product-zip-archive","title":"Download a product .zip archive","text":"<ol> <li> <p>Go to Product Downloads and download the product to be used to build a Docker image.</p> <p>Zip file versus installer</p> <p>Ensure you download the product distribution .zip archive and not the Windows installer</p> </li> <li> <p>When the download has finished, rename the file to <code>product.zip</code>:</p> <pre><code>mv pingfederate-11.0.3.zip product.zip\n</code></pre> </li> <li> <p>Move <code>product.zip</code> to the Build Directory.</p> <p>In the <code>pingidentity-docker-builds</code> repository directory for each product, move the <code>product.zip</code> file to the <code>&lt;product&gt;/tmp</code> directory, where /&lt;product&gt; is the name of one of our available products. For example:</p> <pre><code>mv ~/Downloads/product.zip \\\n   ~/pingidentity/devops/pingidentity-docker-builds/pingfederate/tmp\n</code></pre> </li> </ol>"},{"location":"how-to/buildLocal/#build-the-docker-image","title":"Build the Docker image","text":"<p>Before building the image, display the <code>versions.json</code> file in the product directory. You must specify a valid version for the build script. Since the product .zip archive is being provided, it does not matter which version you select as long as it is valid. For example, you can see that <code>11.0.3</code> is a valid product version for PingFederate.</p> <pre><code>{\n    \"latest\": \"11.0.3\",\n    \"versions\": [\n        {\n            \"version\": \"11.0.3\",\n            \"preferredShim\": \"alpine:3.15.4\",\n            \"shims\": [\n                {\n                    \"shim\": \"alpine:3.15.4\",\n                    \"preferredJVM\": \"al11\",\n                    \"jvms\": [\n                        {\n                            \"jvm\": \"al11\",\n                            \"build\": true,\n                            \"deploy\": true,\n                            \"registries\": [\n                                \"DockerHub\",\n                                \"Artifactory\"\n                            ]\n                        }\n                    ]\n                },        \n</code></pre> <ol> <li> <p>Go to the base of the <code>pingidentity-docker-builds</code> repo. For example:</p> <pre><code>cd ~/pingidentity/devops/pingidentity-docker-builds\n</code></pre> </li> <li> <p>Run the <code>serial_build.sh</code> script with the appropriate options. For example:</p> <pre><code>./ci_scripts/serial_build.sh \\\n    -p pingfederate \\\n    -v 11.0.3 \\\n    -s alpine:3.15.4 \\\n    -j al11\n</code></pre> <p>When the build is completed, the product and base images are displayed. For example:  <pre><code>REPOSITORY                     TAG                                                       IMAGE ID       CREATED              SIZE\npingidentity/pingfederate      11.0.3-fsoverride-alpine_3.15.4-al11-master-f1ba-x86_64   404a2b14df0c   7 seconds ago        759MB\npingidentity/pingbase          master-f1ba-x86_64                                        eb7648692b55   About a minute ago   0B\npingidentity/pingjvm           al11-alpine_3.15.4-master-f1ba-x86_64                     af0e87d8fafd   About a minute ago   108MB\npingidentity/pingcommon        master-f1ba-x86_64                                        2e82b239e9bb   About a minute ago   997kB\npingidentity/pingdatacommon    master-f1ba-x86_64                                        13f35b12a918   About a minute ago   1.11MB\n</code></pre></p> <p>Our Docker images are built using common foundational layers required by the product layer such as the Java virtual machine (JVM), pingcommon, and pingdatacommon.</p> <p>As it is unlikely you will have the foundational layers on your local system, build the first time using the <code>serial_build.sh</code> script. This script will create the foundational images, and if you want to use the same foundational layers for other builds, you need only run the <code>build_product.sh</code> script to build the product layer.</p> <p>You must specify the appropriate options when you run <code>serial_build.sh</code>. For PingFederate, the options might look like this:</p> <ul> <li>-p (Product): pingfederate</li> <li>-v (Version): 11.0.3<ul> <li>Note: this is the version retrieved from the versions.json file</li> </ul> </li> <li>-s (Shim): alpine</li> <li>-j (Java): al11</li> </ul> </li> </ol> <p>Run from the repository root</p> <p>It is important to build from the base of the repository as shown in the example.</p>"},{"location":"how-to/buildLocal/#re-tagging-the-local-image","title":"Re-tagging the local image","text":"<p>To change the tag of the created image and push it to your own Docker registry, use the <code>docker tag</code> command:</p> <pre><code>docker tag [image id] \\\n   [Docker Registry]/[Organization]/[Image Name]:[tag]\n</code></pre> <p>For example:</p> <pre><code>docker tag 404a2b14df0c \\\n    gcp.io/pingidentity/pingfederate:localbuild\n\n# Display new tag\ndocker image ls\n\n# Output snippet\npingidentity/pingfederate            11.0.3-fsoverride-alpine_3.15.4-al11-master-f1ba-x86_64   404a2b14df0c   4 minutes ago   759MB\ngcp.io/pingidentity/pingfederate     localbuild                                                404a2b14df0c   4 minutes ago   759MB\n</code></pre>"},{"location":"how-to/buildPingDirectoryProfile/","title":"Building a profile from your current deployment","text":"<p>PingDirectory is built for GitOps through native tools for building profiles. To find the latest tools and profiles, search for \"DevOps\" in PingDirectory Docs. You can find details of the server profile structure there.</p> <p>A well-formed PingDirectory profile includes all the configuration details needed for starting up a server in a new or existing replication topology as a representation of what is actually running.</p> <p>Use this guide to build a PingDirectory profile from a running instance.</p>"},{"location":"how-to/buildPingDirectoryProfile/#before-you-begin","title":"Before you begin","text":"<p>You must:</p> <ul> <li>Complete Get Started</li> <li>Have a running PingDirectory instance of 8.0.0.0 or later with shell access on the machine or in the container</li> <li>Understand Product Container Anatomy</li> </ul> <p>You should:</p> <ul> <li>Review Customizing Server Profiles</li> </ul>"},{"location":"how-to/buildPingDirectoryProfile/#start-building","title":"Start Building","text":""},{"location":"how-to/buildPingDirectoryProfile/#generating-a-profile","title":"Generating a profile","text":"<p>To generate a profile, run <code>manage-profile generate-profile</code>.</p> <p>This can be called on a running container in Kubernetes like so:</p> <pre><code>## kubectl exec -it &lt;pod-name&gt; \\\n##  -- manage-profile generate-profile \\\n##  --profileRoot /tmp/pd.profile\n\nkubectl exec -it pingdirectory-0 \\\n  -- manage-profile generate-profile \\\n  --profileRoot /tmp/pd.profile\n</code></pre> <p>The Name Matters</p> <p>Although you don't have to name your profile <code>pd.profile</code>, the default location (the variable <code>PD_PROFILE</code>) that PingDirectory looks at is <code>PD_PROFILE=\"/opt/staging/pd.profile\"</code>.</p> <p>Sample Output:</p> <pre><code>Defaulted container \"pingdirectory\" out of: pingdirectory, telegraf, vault-agent-init (init)\nGenerating server profile\n\n...\n\nVariables such as PING_INSTANCE_NAME in setup-arguments.txt and in any other\nfiles in the profile will need to be provided through environment variables or\nthrough a profile variables file when using the generated profile with the\nmanage-profile tool. The PING_SERVER_ROOT and PING_PROFILE_ROOT variables are\nprovided by manage-profile\n\nSome changes may need to be made to the generated profile. Any desired LDIF\nfiles will need to be added to the profile. Any additional server root files,\nserver SDK extensions, and dsconfig commands can be manually added, and\nvariables-ignore.txt can be updated to ignore certain files during variable\nsubstitution. See the README file at /tmp/pd.profile/misc-files/README for\nmore information on the manual steps that must be taken for the generated\nprofile to be used with the manage-profile tool\n\nThe following files and directories in the server root were excluded from the\ngenerated profile, and can be manually added if necessary. These files can\nalso be included by generate-profile with the --includePath argument:\nconfig/truststore\nconfig/ads-truststore\nconfig/encryption-settings.pin\nconfig/tools.properties.orig\nconfig/encryption-settings/encryption-settings-db\nconfig/keystore.p12\nconfig/tools.properties\nconfig/encryption-settings/encryption-settings-db.old\nconfig/keystore.pin\nconfig/keystore\nconfig/ads-truststore.pin\nconfig/truststore.p12\nconfig/truststore.pin\n\nThe generated profile can be found at /tmp/pd.profile\n</code></pre> <p>Note other paths that are not included</p> <p>The <code>manage-profile generate-profile</code> command outputs valuable information about what is and isn't included in the generated profile.</p> <p>Don't put secrets in your profile!</p> <p>Secrets should not be included in your profile, so they are not included in the profile generation by default. However, if you have not already added encryption secrets or keystores to your environment, you can use the <code>--includePath</code> argument to collect items from the running server. These items should then be provided to the server on its next restart through some secrets management tool.</p>"},{"location":"how-to/buildPingDirectoryProfile/#extracting-the-generated-profile","title":"Extracting the generated profile","text":"<p>Following the Kubernetes example, you can copy out the generated profile with:</p> <pre><code>kubectl cp pingdirectory-0:/tmp/pd.profile pd.profile\n</code></pre> <p>Sample output:</p> <pre><code>% tree\n.\n\u2514\u2500\u2500 pd.profile\n    \u251c\u2500\u2500 dsconfig\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 00-config.dsconfig\n    \u251c\u2500\u2500 ldif\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 userRoot\n    \u251c\u2500\u2500 misc-files\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 README\n    \u251c\u2500\u2500 server-root\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 post-setup\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 pre-setup\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 PingDirectory.lic\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 README.md\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 config\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 encryption-settings.pin ## Added via --includePath\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 keystore.pin ## Added via --includePath\n    \u2502\u00a0\u00a0         \u2514\u2500\u2500 schema\n    \u2502\u00a0\u00a0             \u251c\u2500\u2500 80-format-counter-metrics.ldif\n    \u2502\u00a0\u00a0             \u251c\u2500\u2500 87-local-identities.ldif\n    \u2502\u00a0\u00a0             \u251c\u2500\u2500 88-grants.ldif\n    \u2502\u00a0\u00a0             \u251c\u2500\u2500 89-sessions.ldif\n    \u2502\u00a0\u00a0             \u2514\u2500\u2500 90-oauth-clients.ldif\n    \u251c\u2500\u2500 server-sdk-extensions\n    \u251c\u2500\u2500 setup-arguments.txt ## REMOVE this\n    \u2514\u2500\u2500 variables-ignore.txt\n\n11 directories, 13 files\n</code></pre> <p><code>setup-arguments.txt</code> is generated by our Docker image at startup and isn't needed in the profile, so you should remove it from the profile.</p> <pre><code>rm pd.profile/setup-arguments.txt\n</code></pre> <p>userRoot data is not included</p> <p>You might notice that userRoot data (i.e. users) isn't included. Profiles should contain configuration only, not data.</p>"},{"location":"how-to/buildPingDirectoryProfile/#storing-a-profile","title":"Storing a profile","text":"<p>To store the profile, at the root of your profile:</p> <p>Choose from:</p> <ul> <li>For an unmounted profile, add to <code>pd.profile</code>.</li> <li>For a mounted profile, add to <code>/opt/in/pd.profile</code>.</li> </ul>"},{"location":"how-to/buildPingDirectoryProfile/#including-other-files","title":"Including other files","text":"<p>In addition to what's generated with <code>manage-profile generate-profile</code>, you might want to include other files. These files should be siblings to <code>pd.profile</code> at the root of the profile.</p> <p>For an example structure, see baseline.</p>"},{"location":"how-to/buildPingDirectoryProfile/#profile-structure","title":"Profile structure","text":"<p>\"A good PingDirectory profile includes all the configuration needed for starting up a server in a new or existing replication topology.\"</p> <p>Review the following elements to see what to include in your profile.</p> <p>dsconfig commands</p> <p>Because this is how the PingDirectory server is configured, dsconfig commands belong in your profile.</p> <ul> <li><code>manage-profile generate-profile</code> outputs all of the dsconfig commands of a running server into one file: <code>00-config.dsconfig</code>.</li> </ul> <p>Keeping dsconfig commands in one file makes sense because they are ingested together but run in order by the server's inherent dependency knowledge of itself. You can work on PingDirectory in a dev environment and make many changes while working toward your desired configuration.</p> <ul> <li><code>generate-profile</code> exports a representation of your work.</li> </ul> <p>Multiple files</p> <p>You might see multiple files containing dsconfig commands in our profiles, which serves to show logical separation in our demos. Additionally, our demos might be built of multiple layers coming form different repositories so this prevents overwriting.</p> <p>users</p> <p>Data is expected to change at runtime, so this information does not belong in your profile structure.</p> <p>There is built-in protection to enforce this. <code>ldif/userRoot/*</code> is only imported on <code>GENESIS</code> - The first start of the first PingDirectory in a topology.</p> <p>The exceptions to this rule are ephemeral dev and demo environments. This is why you see user files in our sample profiles. These files are intended for bootstrapping demo and test instances.</p> <p>If you are in this category and wanted to include users, you could use:</p> <pre><code>kubectl exec -it pingdirectory-0 \\\n  -- export-ldif \\\n  --backendID userRoot \\\n  --ldifFile /tmp/userRoot.ldif \\\n  --doNotEncrypt\n\nkubectl cp pingdirectory-0:/tmp/userRoot.ldif \\\n  pd.profile/ldif/userRoot/00-users.ldif\n</code></pre> <p>schema</p> <p>Schema belongs in your profile structure because you might want to manage your schema as code, and <code>pd.profile/server-root/pre-setup/config/schema</code> is where to do that.</p> <p>java.properties</p> <p>The <code>config/java.properties</code> file in the server root is used by PingDirectory to manage arguments passed to Java for the server process and for any command-line utilities. If you need to set any custom values in this file, provide the entire file in your server profile at <code>instance/config/java.properties</code>. Note that this is outside of the <code>pd.profile</code> folder.</p> <p>encryption keys, keystores, truststores, and other secrets</p> <p>Any and all secrets should be provided by some sort of secrets management (Vault, bitnami sealed secrets, or at least kubernetes secrets), and as such, these do not belong in your profile structure.</p> <p>PingDirectory allows you to define file paths to secrets so they don't need to be in the profile.</p>"},{"location":"how-to/buildPingFederateProfile/","title":"Build a PingFederate profile from your current deployment","text":"<p>The term \"profile\" can vary in many instances. Here we will focus on two types of profiles for PingFederate: configuration archive, and bulk export. We will discuss the similarities and differences between two as well as how to build either from a running PingFederate environment.</p>"},{"location":"how-to/buildPingFederateProfile/#before-you-begin","title":"Before you begin","text":"<p>You must:</p> <ul> <li>Complete Get Started to set up your DevOps environment and run a test deployment of the products</li> <li>Understand our Product Container Anatomy</li> </ul> <p>You should:</p> <ul> <li>Review Customizing Server Profiles</li> </ul>"},{"location":"how-to/buildPingFederateProfile/#overview-of-profile-methods","title":"Overview of profile methods","text":"<p>There are two file-based profile methods that we cover:</p> <ul> <li>Bulk API Export<ul> <li>The resulting <code>.json</code> from the admin API at /bulk/export</li> <li>Typically saved as <code>data.json</code></li> </ul> </li> <li>Configuration Archive<ul> <li>Pulled either from the admin UI - Server &gt; Configuration Archive or from the admin API at <code>/configArchive</code></li> <li>We call the result of this output <code>data.zip</code> or the <code>/data</code> folder</li> </ul> </li> </ul> <p>A file-based profile means a \"complete profile\" looks like a subset of files that you would typically find in a running PingFederate filesystem.</p> <p>This subset of files represents the minimal number of files needed to achieve your PingFederate configuration. All additional files that are not specific to your configuration should be left out because the PingFederate Docker image fills them in. For more information, see Container Anatomy.</p> <p>Familiarity with the PingFederate filesystem will help you achieve the optimal profile. For more information, see profile structures.</p> <p>Save files</p> <p>You should save every file outside of <code>pingfederate/server/default/data</code> that you've edited.</p> <p>Additionally, all files that are included in the profile should also be environment agnostic. This typically means turning hostnames and secrets into variables that can be delivered from the Orchestration Layer.</p>"},{"location":"how-to/buildPingFederateProfile/#the-bulk-api-export-profile-method","title":"The Bulk API Export Profile Method","text":""},{"location":"how-to/buildPingFederateProfile/#about-this-method","title":"About this method","text":"<p>You will:</p> <ol> <li>Export a <code>data.json</code> from /bulk/export</li> <li>Configure and run bulkconfig tool</li> <li>Export Key Pairs</li> <li>base64 encode exported key pairs</li> <li>Add <code>data.json.subst</code> to your profile at <code>instance/bulk-config/data.json.subst</code></li> </ol> <p>In this guide, we will look at the above steps in detail to understand the purpose and flow. Use the steps for reference as needed.</p> <p>A PingFederate Admin Console imports a <code>data.json</code> on startup if it finds it in <code>instance/bulk-config/data.json</code>.</p> <p>The PF admin API <code>/bulk/export</code> endpoint outputs a large .json blob that is representative of the entire <code>pingfederate/server/default/data</code> folder, PingFederate 'core config', or a representation of anything you would configure from the PingFedera0te UI. This file can be considered as \"the configuration archive in .json format\".</p>"},{"location":"how-to/buildPingFederateProfile/#steps","title":"Steps","text":"<ol> <li> <p>On a running PingFederate instance or pod, run:</p> <pre><code>curl \\\n  --location \\\n  --request GET 'https://pingfederate-admin.ping-devops.com/pf-admin-api/v1/bulk/export' \\\n  --header 'X-XSRF-Header: PingFederate' \\\n  --user \"administrator:${passsword}\" &gt; data.json\n</code></pre> </li> <li> <p>Save data.json into a profile at <code>instance/bulk-config/data.json</code>.</p> </li> <li>Delete everything except <code>pf.jwk</code> in <code>instance/server/default/data</code>.</li> </ol>"},{"location":"how-to/buildPingFederateProfile/#result","title":"Result","text":"<p>You have a bulk API export \"profile\". This file is useful because the entire config is in a single file and if you store it in source control, then you only have to compare differences in one file. However, there is more value than being in one file.</p>"},{"location":"how-to/buildPingFederateProfile/#making-the-bulk-api-export-profile-worthy","title":"Making the bulk API export \"profile-worthy\"","text":"<p>By default, the resulting <code>data.json</code> from the export contains encrypted values, and to import this file, your PingFederate needs to have the corresponding master key (<code>pf.jwk</code>) in <code>pingfederate/server/default/data</code>.</p> <p>Encrypted values in single file</p> <p>In the DevOps world, we call this folder <code>instance/server/default/data</code>. However, each of the encrypted values also have the option to be replaced with an unencrypted form and, when required, a corresponding password.</p>"},{"location":"how-to/buildPingFederateProfile/#example","title":"Example","text":"<p>The SSL Server Certificate from the PingFederate Baseline Profile when exported to data.json has the following syntax:</p> <pre><code>{\n    \"resourceType\": \"/keyPairs/sslServer\",\n    \"operationType\": \"SAVE\",\n    \"items\": [\n        {\n            \"id\": \"sslservercert\",\n            \"fileData\": \"MIIRBwIBAzCCEMAGCSqGSIb3DQEHAaCCELEEghCtMIIQqTCCCeUGCSqGSIb3DQEHAaCCCdYEggnSMIIJzjCCCcoGCyqGSIb3DQEMCgECoIIJezCCCXcwKQYKKoZIhvcNAQwBAzAbBBQu6vDERQZX3uujWa7v_q3sYN4Q0gIDAMNQBIIJSFtdWbvLhzYrTqeKKiJqiqROgE0E4mkVvmEC6NwhhPbcH37IDNvVLu0umm--CDZnEmlyPpUucO345-U-6z-cskw4TbsjYIzM10MwS6JdsyYFTC3GwqioqndVgBUzDh8xGnfzx52zEehX8d-ig1F6xYsbEc01gTbh4lF5MA7E7VfoTa4hWqtceV8PQeqzJNarlZyDSaS5BLn1J6G9BYUze-M1xGhATz7F2l-aAt6foi0mwIBlc2fwsdEPuAALZgdG-q_V4gOJW2K0ONnmWhMgMLpCL42cmSb\n            ... more encrypted text ...\n            Yxpzp_srpy4LHNdgHqhVBhqtDrjeKJDRfc1yk21P5PpfEBxn5MD4wITAJBgUrDgMCGgUABBQLBpq8y79Pq1TzG1Xf6OAjZzBZaQQUC4kD4CkcrH-WTQhJHud850ddn08CAwGGoA==\",\n            \"encryptedPassword\": \"eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2Iiwia2lkIjoiRW1JY1UxOVdueSIsInZlcnNpb24iOiIxMC4xLjEuMCJ9..l6PJ55nSSvKHl0vSWTpkOA.i7hpnnu2yIByhyq_aGBCdaqS3u050yG8eMRGnLRx2Yk.Mo4WSkbbJyLISHq6i4nlVA\"\n        }\n    ]\n}\n</code></pre> <p>You can convert this master key dependent form to:</p> <pre><code>{\n    \"operationType\": \"SAVE\",\n    \"items\": [{\n        \"password\": \"2FederateM0re\",\n        \"fileData\": \"MIIRCQIBAzCCEM8GCSqGSIb3DQEHAaCCEMAEghC8MIIQuDCCC28GCSqGSIb3DQEHBqCCC2AwggtcAgEAMIILVQYJKoZIhvcNAQcBMBwGCiqGSIb3DQEMAQYwDgQIjXWLRGuGNIQCAggAgIILKOgCQ9onDqBPQsshsaS50OjWtj\\/7s47BUYal1YhO70fBup1a82WGHGhAvb\\/SY1yOhqQR+TloEBOPI5cExoGN\\/Gvw2Mw5\\/wkQZZMSHqxjz68KhN4B0hrsOf4rqShB7jsz9ebSml3r2w0sUZWR73GBtBt1Y3wIlXLS2WtqdtHra9VnUqp1eOk+xenjuWM+u2ndDD43GgKB3n8mNBSSVBqx6ne7aSRJRuAUd+HAzLvSeXjTPMObI1Jod2F+7\n        ... more base64 encoded exported .p12 ...\n        5QJ15OJp2iEoVBWxogKf64s2F0iIYPoo6yjNvlidZCevP564FwknWrHoD7R8cIBrhlCJQbEOpOhPg66r4MK1CeJ2poaKRlMS8HGcMRaTpaqD+pIlgmUS6xFw49vr9Kwfb7KteRsTkNR+I8A7HjUpuCMSUwIwYJKoZIhvcNAQkVMRYEFOb7g1xwDka5fJ4sqngEvzTyuWnpMDEwITAJBgUrDgMCGgUABBRlJ+D+FR\\/vQbaTGbKDFiBK\\/xDbqQQIAjLc+GgRg44CAggA\",\n        \"id\": \"sslservercert\"\n    }],\n    \"resourceType\": \"/keyPairs/sslServer\"\n}\n</code></pre> <p>The process:</p> <ol> <li>You exported the private key+cert of the server cert with alias <code>sslservercert</code>. When exported, a password is requested and <code>2FederateM0re</code> was used. This action results in the download of a password protected <code>.p12</code> file.</li> <li>The data.json key name <code>encryptedPassword</code> converted to simply <code>password</code>.</li> <li>The value for <code>fileData</code> is replaced with a base64 encoded version of the exported <code>.p12</code> file.</li> </ol> <p>This process can be used for all encrypted items and environment specific items:</p> <ul> <li>Key Pairs (.p12)</li> <li>Trusted Certs (x509)</li> <li>Admin Password</li> <li>Data Store Passwords</li> <li>Integration Kit Properties</li> <li>Hostnames</li> </ul> <p>Leaving these confidential items as unencrypted text in source control is unacceptable. The next logical step is to abstract the unencrypted values and replace them with variables. By doing this, the values can be stored in a secrets management tool (such as Hashicorp Vault) and the variablized file can be in source control.</p> <p>Converting each of the encrypted keys for their unencrypted counterparts and hostnames with variables is cumbersome and can be automated. As we know in DevOps, if it can be automated, it must be automated. For more information, see Using Bulk Config Tool.</p> <p>A variablized <code>data.json.subst</code> is a good candidate for committing to source control after removing any unencrypted text.</p>"},{"location":"how-to/buildPingFederateProfile/#using-bulk-config-tool","title":"Using Bulk Config Tool","text":"<p>The ping-bulkconfig-tool reads your data.json and can optionally:</p> <ul> <li>Search and replace (e.g. hostnames)</li> <li>Clean, add, and remove json members as required.</li> <li>Tokenize the configuration and maintain environment variables.</li> </ul> <p>The bulk export tool can process a bulk <code>data.json</code> export according to a configuration file with functions above. After running the tool, you are left with a <code>data.json.subst</code> and a list of environment variables waiting to be filled.</p> <p>The <code>data.json.subst</code> form of our previous example will look like: <pre><code>{\n    \"operationType\": \"SAVE\",\n    \"items\": [{\n        \"password\": \"${keyPairs_sslServer_items_sslservercert_sslservercert_password}\",\n        \"fileData\": \"${keyPairs_sslServer_items_sslservercert_sslservercert_fileData}\",\n        \"id\": \"sslservercert\"\n    }],\n    \"resourceType\": \"/keyPairs/sslServer\"\n}\n</code></pre></p> <p>Bulk Config Tool Limitations</p> <p>The bulk config tool can manipulate data.json but it cannot populate the resulting password or fileData variables because there is no API available on PingFederate to extract these. These variables can be filled using with externally generated certs and keys using tools like <code>openssl</code>, but that is out of scope for this document.</p> <p>The resulting <code>env_vars</code> file can be used as a guideline for secrets that should be managed externally and only delivered to the container/image as needed for its specific environment.</p>"},{"location":"how-to/buildPingFederateProfile/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>The bulk export utility comes in pre-compiled source code. Build a Docker image by running:</p> <pre><code>docker build -t ping-bulkexport-tools:latest .\n</code></pre> </li> <li> <p>Copy the data.json to: <code>pingidentity-devops-getting-started/99-helper-scripts/ping-bulkconfigtool/shared/data.json</code></p> </li> </ol>"},{"location":"how-to/buildPingFederateProfile/#example_1","title":"Example","text":"<p>A sample command of the ping-bulkconfig-tool</p> <pre><code>docker run --rm -v $PWD/shared:/shared ping-bulkexport-tools:latest /shared/pf-config.json /shared/data.json /shared/env_vars /shared/data.json.subst &gt; ./shared/convert.log\n</code></pre> <p>Where:</p> <ul> <li><code>-v $PWD/shared:/shared</code> - bind mounts <code>ping-bulkconfigtool/shared</code> folder to /shared in the container</li> <li><code>/shared/pf-config.json</code> - input path to config file which defines how to process the bulk export <code>data.json</code> file from PingFederate.</li> <li><code>/shared/data.json</code> - input path to data.json result of /pf-admin-api/v1/bulk/export PingFederate API endpoint.</li> <li><code>/shared/env_vars</code> - output path to store environment variables generated from processing</li> <li><code>/shared/data.json.subst</code> - output path to processed data.json</li> </ul> <p>After running the above command, you will see <code>env_vars</code> and <code>data.json.subst</code> in the <code>ping-bulkconfigtool/shared</code> folder.</p>"},{"location":"how-to/buildPingFederateProfile/#configure-bulk-tool","title":"Configure Bulk Tool","text":"<p>Instructions to the bulk config tool are sent by the <code>pf-config.json</code> file.  </p> <p>When using the <code>pf-config.json</code> file, any unused functions will require an empty array in the file.  For example, notice the add-config block at the top of this sample:</p> <pre><code>{\n  \"add-config\":[],\n  \"config-aliases\":[\n  ],\n  \"expose-parameters\":[\n  ]\n  ,\n  \"remove-config\":[\n    {\n    \"key\": \"id\",\n    \"value\": \"ProvisionerDS\"\n    }\n  ]\n}\n</code></pre> <p>In this file, available commands include:</p>"},{"location":"how-to/buildPingFederateProfile/#search-replace","title":"search-replace","text":"<ul> <li>A utility to search and replace string values in a bulk config json file.</li> <li>Can expose environment variables.</li> </ul> <p>Example: replacing an expected base hostname with a substitution: <pre><code>  \"search-replace\":[\n    {\n      \"search\": \"data-holder.local\",\n      \"replace\": \"${BASE_HOSTNAME}\",\n      \"apply-env-file\": false\n    }\n  ]\n</code></pre></p>"},{"location":"how-to/buildPingFederateProfile/#change-value","title":"change-value","text":"<ul> <li>Searches for elements with a matching identifier, and updates a parameter with a new value.</li> </ul> <p>Example: update keyPairId against an element with name=ENGINE: <pre><code>  \"change-value\":[\n    {\n          \"matching-identifier\":\n          {\n            \"id-name\": \"name\",\n            \"id-value\": \"ENGINE\"\n          },\n      \"parameter-name\": \"keyPairId\",\n      \"new-value\": 8\n    }\n  ]\n</code></pre></p>"},{"location":"how-to/buildPingFederateProfile/#remove-config","title":"remove-config","text":"<ul> <li>Remove configuration objects from the bulk export</li> </ul> <p>Example: To remove the ProvisionerDS data store: <pre><code>  \"remove-config\":[\n    {\n      \"key\": \"id\",\n      \"value\": \"ProvisionerDS\"\n    }\n  ]\n</code></pre></p> <p>Example: To remove all SP Connections: <pre><code>  \"remove-config\":[\n    {\n      \"key\": \"resourceType\",\n      \"value\": \"/idp/spConnections\"\n    }\n  ]\n</code></pre></p>"},{"location":"how-to/buildPingFederateProfile/#add-config","title":"add-config","text":"<ul> <li>Add configurations to the bulk export.</li> </ul> <p>This tool works with both PingFederate and PingAccess.  This example adds the CONFIG QUERY http listener in PingAccess: <pre><code>  \"add-config\":[\n      {\n        \"resourceType\": \"httpsListeners\",\n        \"item\":\n            {\n                \"id\": 4,\n                \"name\": \"CONFIG QUERY\",\n                \"keyPairId\": 5,\n                \"useServerCipherSuiteOrder\": true,\n                \"restartRequired\": false\n            }\n      }\n  ]\n</code></pre></p> <p>Example: Add an SP connection: <pre><code>  \"add-config\":[\n      {\n        \"resourceType\": \"/idp/spConnections\",\n        \"item\":\n        {\n                    \"name\": \"httpbin3.org\",\n                    \"active\": false,\n            ...\n        }\n      }\n  ]\n</code></pre></p>"},{"location":"how-to/buildPingFederateProfile/#expose-parameters","title":"expose-parameters","text":"<ul> <li>Navigates through the JSON and exchanges values for substitions.</li> <li>Exposed substition names will be automatically created based on the json path.<ul> <li>E.g. ${oauth_clients_items_clientAuth_testclient_secret}</li> </ul> </li> <li>Can convert encrypted/obfuscated values into clear text inputs (e.g. \"encryptedValue\" to \"value\") prior to substituting it. Doing so enables the injection of values in their raw form.</li> </ul> <p>Example: replace the \"encryptedPassword\" member with a substitution-enabled \"password\" member for any elements with \"id\" or \"username\" members. The following will remove \"encryptedPassword\" and create \"password\": \"${...}\": <pre><code>    {\n      \"parameter-name\": \"encryptedPassword\",\n      \"replace-name\": \"password\",\n      \"unique-identifiers\": [\n          \"id\",\n          \"username\"\n      ]\n    }\n</code></pre></p>"},{"location":"how-to/buildPingFederateProfile/#config-aliases","title":"config-aliases","text":"<ul> <li>The bulk config tool generates substitution names. However, there might be times you wish to simplify them or reuse existing environment variables.</li> </ul> <p>Example: Rename the Administrator's substitution name using the PING_IDENTITY_PASSWORD environment variable: <pre><code>  \"config-aliases\":[\n    {\n      \"config-names\":[\n        \"administrativeAccounts_items_Administrator_password\"\n      ],\n      \"replace-name\": \"PING_IDENTITY_PASSWORD\",\n      \"is-apply-envfile\": false\n    }\n  ]\n</code></pre></p>"},{"location":"how-to/buildPingFederateProfile/#sort-arrays","title":"sort-arrays","text":"<ul> <li>Configure the array members that need to be sorted. This function ensures the array is created consistently, simplifying git diff analysis.</li> </ul> <p>Example: Sort the roles and scopes arrays: <pre><code>  \"sort-arrays\":[\n        \"roles\",\"scopes\"\n  ]\n</code></pre></p>"},{"location":"how-to/buildPingFederateProfile/#additional-notes","title":"Additional Notes","text":"<ul> <li>The bulk API export is intended to be used as a bulk import. The <code>/bulk/import</code> endpoint is destructive and overwrites the entire current admin config.</li> <li>If you are in a clustered environment, the PingFederate image imports the <code>data.json</code> and replicates the configuration to engines in the cluster.</li> <li>Your data.json.subst <code>\"metadata\": {\"pfVersion\": \"10.1.2.0\"}</code> should match the PingFederate profile version.</li> </ul>"},{"location":"how-to/buildPingFederateProfile/#the-configuration-archive-profiles-method","title":"The Configuration Archive Profiles Method","text":""},{"location":"how-to/buildPingFederateProfile/#about-configuration-archive-based-profiles","title":"About configuration archive-based profiles","text":"<p>You should weigh the pros and cons of configuration archive-based profiles compared to bulk API export profiles. While not fully aligning with pur DevOps principles, many users prefer using bulk API export profiles in most scenarios.</p> <p>Pros: * The <code>/data</code> folder, as opposed to a <code>data.json</code> file, is better for profile layering. * Configuration is available on engines at startup, which:     * lowers dependency on the admin at initial cluster startup</p> <p>Cons:</p> <ul> <li>The <code>/data</code> folder contains key pairs in a <code>.jks</code> file , which makes externally managing keys very difficult.</li> <li>Encrypted data is scattered throughout the folder, creating a dependency on the master encryption key.</li> </ul>"},{"location":"how-to/buildPingFederateProfile/#about-this-method_1","title":"About this method","text":"<p>You will:</p> <ol> <li>Export a <code>data.zip</code> archive</li> <li>Optionally, variablize</li> <li>Replace the data folder</li> </ol>"},{"location":"how-to/buildPingFederateProfile/#installing-pingfederate-integration-kits","title":"Installing PingFederate Integration Kits","text":"<p>By default, PingFederate is shipped with a handful of integration kits and adapters. If you need other integration kits or adapters in the deployment, manually download them and place them inside the <code>server/default/deploy</code> directory of the server profile. You can find these resources in the product download page here.</p>"},{"location":"how-to/containerAnatomy/","title":"Deployment","text":"<p>Any configuration that is deployed with one of our product containers can be considered a \"server profile\". A profile typically looks like a set of files.</p> <p>You can use profiles in these ways:</p> <ul> <li>Pull at startup.</li> <li>Build into the image.</li> <li>Mount as a container volume.</li> </ul>"},{"location":"how-to/containerAnatomy/#pull-at-startup","title":"Pull at startup","text":"<p>Pass a Github-based URL and path as environment variables that point to a server profile.</p> <p>Pros:</p> <ul> <li>Easily sharable, inherently source-controlled</li> </ul> <p>Cons:</p> <ul> <li>Adds download time at container startup</li> </ul> <p>For profiles pulled at startup, the image uses the following variables to clone the repo at startup and pull the profile into the container:</p> <ul> <li><code>SERVER_PROFILE_URL</code> - The git URL with the server profile.</li> <li><code>SERVER_PROFILE_PATH</code> - The location from the base of the URL with the specific server profile.   This allows for several products server profile to be housed in the same git repo.</li> <li><code>SERVER_PROFILE_BRANCH</code> (optional) - If other than the default branch (usually master or main), allows   for specifying a different branch.  Example might be a user's development branch before merging into master.</li> </ul> <p>Although there is additional customizable functionality, this is the most common way that profiles are provided to containers because it is easy to provide a known starting state as well as track changes over time.  For more information, see Private Github Repos.</p>"},{"location":"how-to/containerAnatomy/#build-into-the-image","title":"Build into the image","text":"<p>Build your own image from one of our Docker images and copy the profile files in.</p> <p>Pros:</p> <ul> <li>No download at startup, and no egress required</li> </ul> <p>Cons:</p> <ul> <li>Tedious to build images when making iterative changes</li> </ul> <p>Building a profile into the image is useful when you have no access to the Github repository or if you're often spinning containers up and down.</p> <p>For example, if you made a Dockerfile at this location: https://github.com/pingidentity/pingidentity-server-profiles/tree/master/baseline, the relevant entries might look similar to this:</p> <pre><code>FROM: pingidentity/pingfederate:edge\nCOPY pingfederate/. /opt/in/.\n</code></pre>"},{"location":"how-to/containerAnatomy/#mount-as-a-docker-volume","title":"Mount as a Docker volume","text":"<p>Using <code>docker-compose</code> you can bind-mount a host file system location to a location in the container.</p> <p>Pros:</p> <ul> <li>Most iterative. There's no download time, and you can see the file system while you are working in the container.</li> </ul> <p>Cons:</p> <ul> <li>There's no great way to do this in Kubernetes or other platform orchestration tools.</li> </ul> <p>Mount the profile as a Docker volume when you're developing a server profile and you want to be able to quickly make changes to the profile and spin up a container against it.</p> <p>For example, if you have a profile in same directory as your <code>docker-compose.yaml</code> file, you can add a bind-mount volume to /opt/in like this:</p> <pre><code>volumes:\n   - ./pingfederate:/opt/in\n</code></pre>"},{"location":"how-to/devopsRegistration/","title":"Ping Identity DevOps Registration","text":"<p>Getting Support</p> <p>The team responsible for the Ping DevOps program does not have access to the user account system on the Ping Identity website.  If you have trouble with your account and are unable to follow these instructions to enroll, the issue is probably with your credentials in our system.  Please contact your sales representative at Ping Identity Support.</p>"},{"location":"how-to/devopsRegistration/#ping-identity-devops-registration","title":"Ping Identity DevOps Registration","text":"<p>Registering for Ping Identity's DevOps Program provides you with credentials that enable you to easily deploy and evaluate Ping Identity products using trial licenses automatically using tools and platforms like Helm or Kubernetes.</p> <p>To register for the DevOps Program:</p> <ul> <li>Make sure you have a registered account with Ping Identity.  If you're not sure, click the link to Sign On and follow the instructions to access your account.</li> <li>If you don't have an account, create one here.</li> <li>When signing on, select Support and Community in the Select Account list.</li> <li>After you're signed on, you're directed to your profile page.</li> <li>In the right-side menu, click Register for DevOps Program.   </li> </ul> <p>A confirmation message will be shown and the DevOps credentials will be forwarded to the email address associated with your Ping Identity account.</p> <p>Saving Credentials</p> <p>When you receive your key, follow the instructions below for saving these with the <code>pingctl</code> utility.</p> <p>Example:</p> <ul> <li><code>PING_IDENTITY_DEVOPS_USER=jsmith@example.com</code></li> <li><code>PING_IDENTITY_DEVOPS_KEY=e9bd26ac-17e9-4133-a981-d7a7509314b2</code></li> </ul>"},{"location":"how-to/devopsRegistration/#saving-your-devops-user-and-key","title":"Saving Your DevOps User and Key","text":"<p>The recommended way to save your DevOps User/Key is to use the Ping Identity DevOps utility <code>pingctl</code>.</p> <p>pingctl setup</p> <p>You can find installation instructions for <code>pingctl</code> in the pingctl Tool document.</p> <p>To save your DevOps credentials, run <code>pingctl config</code> and supply your credentials when prompted.</p> <p>When <code>pingctl</code> is installed and configured, it places your DEVOPS USER/KEY into a Ping Identity property file found at <code>~/.pingidentity/config</code>  with the following variable names set (see the following example).</p> <pre><code>PING_IDENTITY_DEVOPS_USER=jsmith@example.com\nPING_IDENTITY_DEVOPS_KEY=e9bd26ac-17e9-4133-a981-d7a7509314b2\n</code></pre> <p>After you've configured these settings, you can view them with the <code>pingctl info</code> command (credential values are masked by default, use <code>pingctl info -v</code> to show unmasked).</p>"},{"location":"how-to/devopsRegistration/#resending-your-devops-user-and-key","title":"Resending your DevOps User and Key","text":"<p>If you have misplaced or lost your DevOps User/Key, there are two convenient ways to recover it.</p> <ul> <li> <p>If you have configured <code>pingctl</code>, the <code>PING_IDENTITY_DEVOPS_USER</code> and <code>PING_IDENTITY_DEVOPS_KEY</code> can be printed by entering the following command:     <pre><code>pingctl info -v\n</code></pre></p> </li> <li> <p>If you did not save the credentials in the <code>pingctl</code> tool, you can recover your credentials by logging in to your Ping Identity account.</p> </li> <li>Navigate to Sign On and follow the instructions to access your account.</li> <li>When signing on, select Support and Community in the Select Account list.</li> <li>After you're signed on, you're directed to your profile page.</li> <li>In the right-side menu, click Register for DevOps Program again.  A confirmation message will be shown and the same DevOps credentials will be resent to the email address associated with your Ping Identity account.</li> </ul>"},{"location":"how-to/devopsUserKey/","title":"Using Your Devops User and Key","text":"<p>When starting one of our containers, the container attempts to find the DevOps registration information first in the DevOps property file located in <code>~/.pingidentity/config</code>. This property file was created when you set up the DevOps environment (see Get Started. If the DevOps registration information isn't found there, the container checks for environment variables assigned in the <code>docker run</code> command for standalone containers or in the YAML file for a stack.</p>"},{"location":"how-to/devopsUserKey/#display-your-devops-information","title":"Display Your Devops Information","text":"<p>To display your current DevOps environment information, run the <code>pingctl info</code> command.</p>"},{"location":"how-to/devopsUserKey/#for-kubernetes-and-helm","title":"For Kubernetes and Helm","text":"<p>Our Kubernetes and Helm examples by default will look for a Kubernetes secret named <code>devops-secret</code>.</p> <p>You must create a Kubernetes secret that contains the environment variables <code>PING_IDENTITY_DEVOPS_USER</code> and <code>PING_IDENTITY_DEVOPS_KEY</code>.</p> <ol> <li> <p>If you don't already know your DevOps credentials, display these using the following DevOps command.</p> <pre><code>pingctl info\n</code></pre> </li> <li> <p>Generate the Kubernetes secret from your DevOps credentials:</p> </li> </ol> <p>Choose from:</p> <ul> <li> <p>Use the <code>pingctl</code> utility.</p> <pre><code>pingctl k8s generate devops-secret | kubectl apply -f -\n</code></pre> </li> <li> <p>Generate the secret manually.</p> </li> </ul> <pre><code> kubectl create secret generic devops-secret \\\n   --from-literal=PING_IDENTITY_DEVOPS_USER=\"${PING_IDENTITY_DEVOPS_USER}\" \\\n   --from-literal=PING_IDENTITY_DEVOPS_KEY=\"${PING_IDENTITY_DEVOPS_KEY}\"\n</code></pre>"},{"location":"how-to/devopsUserKey/#for-standalone-docker-containers","title":"For Standalone Docker Containers","text":"<p>When using the <code>docker run</code> command to start a container, you can assign the <code>--env-file</code> argument to the file containing your DevOps registration information, as in the following example.</p> <pre><code>docker run \\\n  --name pingdirectory \\\n  --publish 1389:1389 \\\n  --publish 8443:1443 \\\n  --detach \\\n  --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\\n  --env SERVER_PROFILE_PATH=getting-started/pingdirectory \\\n  --env-file ~/.pingidentity/config \\\n  pingidentity/pingdirectory\n</code></pre>"},{"location":"how-to/devopsUserKey/#for-stacks","title":"For Stacks","text":"<p>When deploying a stack, you can use either of the following methods to assign the location of the file containing your DevOps registration information:</p> <ul> <li>The <code>env_file</code> configuration option</li> <li>The DevOps environment variables</li> </ul>"},{"location":"how-to/devopsUserKey/#pass-as-env-file","title":"Pass as Env File","text":"<p>Add the <code>env_file</code> configuration option to the YAML file for the stack. The <code>env_file</code> configuration option passes environment variable definitions into the container, as in the following example.</p> <pre><code>...\n  pingdirectory:\n    image: pingidentity/pingdirectory\n    env_file:\n    - ${HOME}/.pingidentity/config\n    environment:\n    - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git\n    - SERVER_PROFILE_PATH=getting-started/pingdirectory\n...\n</code></pre>"},{"location":"how-to/devopsUserKey/#pass-as-env-variables","title":"Pass as Env Variables","text":"<p>Add the <code>PING_IDENTITY_DEVOPS_USER</code> and <code>PING_IDENTITY_DEVOPS_KEY</code> DevOps environment variables to the YAML file for the stack, as in the following example.</p> <pre><code>...\n  pingdirectory:\n    image: pingidentity/pingdirectory\n    environment:\n      - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git\n      - SERVER_PROFILE_PATH=getting-started/pingdirectory\n      - PING_IDENTITY_ACCEPT_EULA=YES\n      - PING_IDENTITY_DEVOPS_USER=jsmith@example.com\n      - PING_IDENTITY_DEVOPS_KEY=e9bd26ac-17e9-4133-a981-d7a7509314b2\n...\n</code></pre>"},{"location":"how-to/existingLicense/","title":"Mount Existing Product License","text":""},{"location":"how-to/existingLicense/#mount-existing-product-license","title":"Mount Existing Product License","text":"<p>You can pass the license file to a container via mounting to the container's <code>/opt/in</code> directory.</p> <p>Note: You do not need to do this if you are using your DevOps User/Key. If you have provided license files via the volume mount and a DevOps User/Key, it will ignore the DevOps User/Key.</p> <p>The <code>/opt/in</code> directory overlays files onto the products runtime filesystem, the license needs to be named correctly and mounted in the exact location the product checks for valid licenses.</p>"},{"location":"how-to/existingLicense/#example-mounts","title":"Example Mounts","text":"Product File Name Mount Path PingFederate pingfederate.lic /opt/in/instance/server/default/conf/pingfederate.lic PingAccess pingaccess.lic /opt/in/instance/conf/pingaccess.lic PingDirectory PingDirectory.lic /opt/in/pd.profile/server-root/pre-setup/PingDirectory.lic PingDirectoryProxy PingDirectory.lic /opt/in/pd.profile/server-root/pre-setup/PingDirectory.lic PingDataSync PingDirectory.lic /opt/in/pd.profile/server-root/pre-setup/PingDirectory.lic PingAuthorize PingAuthorize.lic /opt/in/pd.profile/server-root/pre-setup/PingAuthorize.lic PingAuthorize PAP PingAuthorize.lic /opt/in/pd.profile/server-root/pre-setup/PingAuthorize.lic PingCentral pingcentral.lic /opt/in/instance/conf/pingcentral.lic"},{"location":"how-to/existingLicense/#volume-mount-syntax","title":"Volume Mount Syntax","text":""},{"location":"how-to/existingLicense/#docker","title":"Docker","text":"<p>Sample docker run command with mounted license:</p> <pre><code>docker run \\\n  --name pingfederate \\\n  --volume &lt;local/path/to/pingfederate.lic&gt;:/opt/in/instance/server/default/conf/pingfederate.lic\n  pingidentity/pingfederate:edge\n</code></pre> <p>Sample docker-compose.yaml with mounted license:</p> <pre><code>version: \"2.4\"\nservices:\n  pingfederate:\n    image: pingidentity/pingfederate:edge\n    volumes:\n      - path/to/pingfederate.lic:/opt/in/instance/server/default/conf/pingfederate.lic\n</code></pre>"},{"location":"how-to/existingLicense/#kubernetes","title":"Kubernetes","text":"<p>Create a Kubernetes secret from the license file</p> <pre><code>kubectl create secret generic pingfederate-license \\\n  --from-file=./pingfederate.lic\n</code></pre> <p>Then mount it to the pod</p> <pre><code>spec:\n  containers:\n  - name: pingfederate\n    image: pingidentity/pingfederate\n    volumeMounts:\n      - name: pingfederate-license-volume\n        mountPath: \"/opt/in/instance/server/default/conf/pingfederate.lic\"\n        subPath: pingfederate.lic\n  volumes:\n  - name: pingfederate-license-volume\n    secret:\n      secretName: pingfederate-license\n</code></pre>"},{"location":"how-to/existingLicense/#helm","title":"Helm","text":"<p>Create a Kubernetes secret from the license file</p> <pre><code>kubectl create secret generic pingfederate-license \\\n  --from-file=./pingfederate.lic\n</code></pre> <p>Add the secretVolumes within your values.yaml deployment file</p> <pre><code>pingfederate-admin:\n  ...\n  secretVolumes:\n    pingfederate-license:\n      items:\n        pingfederate.lic: /opt/in/instance/server/default/conf/pingfederate.lic\n</code></pre>"},{"location":"how-to/existingLicense/#note-on-updating-the-product-license-when-mounting-it-as-a-file","title":"Note on updating the product license when mounting it as a file","text":"<p>If you are updating the license file for a product, simply replacing the file on the filesystem may not update the license of the running server.</p> <p>For PingData products (PingDirectory, PingDataSync, PingAuthorize, and PingDirectoryProxy) the license can be updated by copying the new license to the expected location in the server profile - <code>pd.profile/server-root/pre-setup</code>. After doing so, dsconfig can be used to update the license on the running server. Ensure that the updated license file is still present in the server profile on subsequent restarts of the container.</p> <p>For example, for PingDirectory: <pre><code>dsconfig set-license-prop \\\n  --set \"directory-platform-license-key&lt;input-file.lic\"\n</code></pre></p> <p>The exact name of the license property in the above example will depend on which PingData product is being used.</p> <p>For non-PingData products, the license can be updated on the product with the typical method. This process will depend on the product, but will generally be done either through the administrative console or using an API call. See the product documentation for details.</p>"},{"location":"how-to/manage/","title":"Managing Deployments","text":"<p>In addition to Customizing Deployments, you must maintain your deployments over time as new versions of our products are released and as you tune your deployments to better reflect your changing needs.</p>"},{"location":"how-to/migratingRootToUnprivileged/","title":"Migrating from privileged images to unprivileged-by-default images","text":"<p>In the 2103 release, our product images were updated to run with an unprivileged user by default. Before this release, images ran as root by default. This document describes some important tips when moving from privileged to unprivileged images.</p>"},{"location":"how-to/migratingRootToUnprivileged/#checklist-before-migration","title":"Checklist before migration","text":"<ul> <li>To ensure that any configuration of the pods is maintained, build and commit a server profile from your current workload into a git repository.<ul> <li>See the Server Profile Structures page, and/or the product-specific guides for PingFederate and PingDirectory.</li> </ul> </li> <li>For PingDirectory, export your user data that will be imported into the new server(s). You can include the basic DIT structure in the server profile (in the <code>pd.profile/ldif/userRoot/</code> directory), but actual user data should be left out; the server profile should store configuration, not data. You can save the actual user data elsewhere and manually import it after the new pods have started.<ul> <li>You can use the <code>export-ldif</code> command to export user data, or you can schedule a task via LDAP. The exported ldif file will be written to the pod filesystem.</li> <li>You can use the <code>import-ldif</code> command to import user data, or you can schedule a task via LDAP. For the import to run, the file to be imported must exist on the pod filesystem.</li> </ul> </li> </ul>"},{"location":"how-to/migratingRootToUnprivileged/#potential-issues","title":"Potential issues","text":""},{"location":"how-to/migratingRootToUnprivileged/#persistent-volumes","title":"Persistent volumes","text":"<p>In Kubernetes, persistent volumes created with our older containers have files owned by the root user. When the default non-privileged user attempts to use these existing volumes, there might be file permission errors.</p> <p>To avoid this, you can either:</p> <ul> <li>Create a fresh deployment that doesn't use the old volumes.</li> <li>Continue to run the containers as root.</li> </ul> <p>Additionally, the containers using persistent volume claims need to set the securityContext <code>fsGroup</code> to a value allowing the container can write to the PVCs.  An example of setting this value in the statefulSet workload needs to include the following fsGroup setting.</p> <p>This example uses the same default groupId set by the image.  The Ping Identity Helm Charts already provide this setting by default for the containers.</p> <pre><code>spec:\n  template:\n    spec:\n      securityContext:\n        fsGroup:9999\n</code></pre>"},{"location":"how-to/migratingRootToUnprivileged/#default-ports","title":"Default ports","text":"<p>In our older images, certain default ports (<code>LDAP_PORT</code>, <code>LDAPS_PORT</code>, <code>HTTPS_PORT</code>, and <code>JMX_PORT</code>) were set to privileged values (<code>389</code>, <code>636</code>, <code>443</code>, and <code>689</code>, respectively). The newer images don't use these values because they run as a non-privileged user. The updated default ports are <code>1389</code>, <code>1636</code>, <code>1443</code>, and <code>1689</code>.</p> <p>If you need, you can maintain the old values by setting the corresponding environment variables and running the container as root.</p> <p>For our PingDirectory images, port changes aren't allowed on restart. If you're using a volume from an older image you may encounter an error due to changing port values.</p> <p>You must either:</p> <ul> <li>Create a fresh deployment for PingDirectory with the new images and import your data from the old deployment.</li> <li>Set the environment variables to match the original privileged values and continue to run the container as root.</li> </ul>"},{"location":"how-to/migratingRootToUnprivileged/#running-as-root-with-the-unprivileged-by-default-images","title":"Running as root with the unprivileged-by-default images","text":"<p>To run as root as mentioned in the two previous examples, you must use your container orchestrator:</p> <ul> <li>For pure Docker, the <code>-u</code> flag allows specifying the user the container should use.</li> <li>For Docker Compose, you can define a <code>user:</code>.</li> <li>In Kubernetes, you can set up a security context for the container to specify the user. To run as root, a user and group ID of <code>0:0</code> should be used.</li> </ul>"},{"location":"how-to/privateRepos/","title":"Using Private Git Repositories","text":"<p>In general, you do not want your server profiles to be public. Instead, you should persist your server profiles in private git repositories.</p> <p>To use server profiles with private repositories, you must either:</p> <ul> <li>Pull using HTTPS</li> <li>Pull using SSH</li> </ul> <p>For HTTPS with GitHub:</p> <ol> <li>Generate an access token in GitHub.</li> <li>Specify the access token in the URL you assign to the <code>SERVER_PROFILE_URL</code> environment variable in your YAML files.</li> </ol> <p>For SSH:</p> <ul> <li>Include your keys and known hosts in the image under <code>/home/ping/.ssh</code>.</li> </ul>"},{"location":"how-to/privateRepos/#cloning-from-github-using-https","title":"Cloning from GitHub using HTTPS","text":""},{"location":"how-to/privateRepos/#creating-a-github-access-token","title":"Creating a GitHub Access Token","text":"<ol> <li>In GitHub, go to <code>Settings</code> --&gt; <code>Developer Settings</code> --&gt; <code>Personal access tokens</code>.</li> <li>Click <code>Generate new token</code> and assign the token a name.</li> <li> <p>Grant the token privilege to the <code>repo</code> group.</p> <p>Copy the token to a secure location. You won't be able to view the token again.</p> </li> <li> <p>At the bottom of the page, click <code>Generate Token</code>.</p> </li> </ol>"},{"location":"how-to/privateRepos/#using-the-token-in-yaml","title":"Using The Token In YAML","text":"<p>To use the token in your YAML file, include it in the <code>SERVER_PROFILE_URL</code> environment variable using this format:</p> <pre><code>https://&lt;github-username&gt;:&lt;github-token&gt;@github.com/&lt;your-repository&gt;.git\n</code></pre> <p>For example:</p> <pre><code>SERVER_PROFILE_URL=https://github_user:zqb4famrbadjv39jdi6shvl1xvozut7tamd5v6eva@github.com/pingidentity/server_profile.git\n</code></pre>"},{"location":"how-to/privateRepos/#using-git-credentials-in-profile-url","title":"Using Git Credentials in Profile URL","text":"<p>Typically, variables in a <code>SERVER_PROFILE_URL</code> string are not replaced. However, certain Git user and password variables can be replaced.</p> <ul> <li> <p>To substitute for the user and password variables using values defined in your YAML files, include either or both <code>${SERVER_PROFILE_GIT_USER}</code> and <code>${SERVER_PROFILE_GIT_PASSWORD}</code> in your server profile URL. For example:</p> <pre><code>SERVER_PROFILE_URL=https://${SERVER_PROFILE_GIT_USER}:${SERVER_PROFILE_GIT_PASSWORD}@github.com/pingidentity/server_profile.git\n</code></pre> </li> <li> <p>When using layered server profiles, each layer can use the base user and password variables, or you can define values specific to that layer.</p> <p>For example, for a <code>license</code> server profile layer, you can use the <code>SERVER_PROFILE_LICENSE_GIT_USER</code> and <code>SERVER_PROFILE_LICENSE_GIT_PASSWORD</code> variables, and substitute for those variables using values defined in your YAML files.</p> </li> </ul>"},{"location":"how-to/privateRepos/#cloning-using-ssh","title":"Cloning using SSH","text":"<ul> <li>To clone using SSH, you can mount the necessary keys and known hosts files using a volume at <code>/home/ping/.ssh</code>, the home directory of the default user in our product images.</li> <li>To clone from GitHub, you must add the necessary SSH keys to your account through the account settings page.</li> </ul>"},{"location":"how-to/profiles/","title":"Customizing Server Profiles","text":"<p>When you deployed the full stack of product containers in Getting Started, you used the server profiles associated with each of our products. In the YAML files, you'll see entries like the following for each product instance:</p> <pre><code>environment:\n  - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git\n  - SERVER_PROFILE_PATH=baseline/pingaccess\n</code></pre> <p>Our pingidentity-server-profiles repository, indicated by the <code>SERVER_PROFILE_URL</code> environment variable, contains the server profiles we use for our DevOps deployment examples. The <code>SERVER_PROFILE_PATH</code> environment variable indicates the location of the product profile data to use. In the previous example, the PingAccess profile data is located in the <code>baseline/pingaccess</code> directory.</p> <p>We use environment variables for certain startup and runtime configuration settings of both standalone and orchestrated deployments. You can find environment variables that are common to all product images in the PingBase Image Directory. There are also product-specific environment variables. You can find these in the Docker Image Information for each available product.</p>"},{"location":"how-to/profiles/#before-you-begin","title":"Before you begin","text":"<p>You must:</p> <ul> <li>Complete Get Started to set up your DevOps environment and run a test deployment of the products.</li> <li>Understand the Anatomy of the Product Containers.</li> </ul>"},{"location":"how-to/profiles/#about-this-task","title":"About this task","text":"<p>You will:</p> <ul> <li> <p>Add or change the environment variables used for any of our server profiles to better fit your purposes.</p> <p>You can find these variables in the Server Profiles Repository for each product.</p> <p>For example, the location for the <code>env_vars</code> file for PingAccess is located in the baseline/pingaccess server profile.</p> </li> <li> <p>Modify one of our server profiles to reflect an existing Ping Identity product installation in your organization.</p> <p>You can do this by either:</p> <ul> <li>Forking our server profiles repository (<code>https://github.com/pingidentity/pingidentity-server-profiles</code>) to your Github repository</li> <li>Using local directories</li> </ul> </li> </ul>"},{"location":"how-to/profiles/#adding-or-changing-environment-variables","title":"Adding or Changing Environment Variables","text":"<ol> <li> <p>Select any environment variables to add from either:</p> <ul> <li>The product-specific environment variables in the Docker Images Information</li> <li>The environment variables common to all of our products in the PingBase Image Directory</li> </ul> </li> <li> <p>From the <code>baseline</code>, <code>getting-started</code>, or <code>simple-sync</code>    directories in the Server Profiles Repository, select the product whose profile you want to modify.</p> </li> <li> <p>Open the <code>env_vars</code> file associated with the product and either:</p> <ul> <li>Add any of the environment variables you've selected.</li> <li>Change the existing environment variables to fit your purpose.</li> </ul> </li> </ol>"},{"location":"how-to/profiles/#modifying-a-server-profile","title":"Modifying a Server Profile","text":"<p>You can modify one of our server profiles based on data from your existing Ping Identity product installation.</p> <p>Modify a server profile by either:</p> <ul> <li>Using your Github repository</li> <li>Using local directories</li> </ul>"},{"location":"how-to/profiles/#using-your-github-repository","title":"Using Your Github Repository","text":"<p>In this example PingFederate installation, using the Github Repository uses a server profile provided through a Github URL and assigned to the <code>SERVER_PROFILE_PATH</code> environment variable, such as <code>--env SERVER_PROFILE_PATH=getting-started/pingfederate</code>).</p> <ol> <li> <p>Export a configuration archive as a *.zip file from a PingFederate installation to a local directory.</p> <p>Make sure this is exported as a .zip rather than compressing it yourself.</p> </li> <li> <p>Sign on to Github and fork https://github.com/pingidentity/pingidentity-server-profiles into your own GitHub repository.</p> </li> <li> <p>Open a terminal, create a new directory, and clone your Github repository to a local directory. For example:</p> <pre><code>mkdir /tmp/pf_to_docker\ncd /tmp/pf_to_docker\ngit clone https://github.com/&lt;github-username&gt;/pingidentity-server-profiles.git\n</code></pre> <p>Where <code>&lt;github-username&gt;</code> is the name you used to sign on to the Github account.</p> </li> <li> <p>Go to the location where you cloned your fork of our <code>pingidentity-server-profiles</code> repository, and replace the <code>/data</code> directory in <code>getting-started/pingfederate/instance/server/default</code> with the <code>data</code> directory you exported from your existing PingFederate installation. For example:</p> <pre><code>cd pingidentity-server-profiles/getting-started/pingfederate/instance/server/default\nrm -rf data\nunzip -qd data &lt;path_to_your_configuration_archive&gt;/data.zip\n</code></pre> <p>Where <code>&lt;path_to_your_configuration_archive&gt;</code> is the location for your exported PingFederate configuration archive.</p> <p>You now have a local server profile based on your existing PingFederate installation.</p> <p>Pushing to Github</p> <p>You should push to Github only what is necessary for your customizations. Our Docker images create the <code>/opt/out</code> directory using a product's base install and layering a profile (set of files) on top.</p> </li> <li> <p>Push your changes (your local server profile) to the Github repository where you forked our server profile repository.</p> <p>You now have a server profile available through a Github URL.</p> </li> <li> <p>Deploy the PingFederate container.</p> <p>Saving Changes</p> <p>To save any changes you make after the container is running, add the entry <code>--volume &lt;local-path&gt;:/opt/out</code> to the <code>docker run</code> command, where &lt;local-path&gt; is a directory you haven't already created. For more information, see Saving Your Changes.</p> <p>As in this example, the environment variables <code>SERVER_PROFILE_URL</code> and <code>SERVER_PROFILE_PATH</code> direct Docker to use the server profile you've modified and pushed to Github:</p> <pre><code>docker run \\\n  --name pingfederate \\\n  --publish 9999:9999 \\\n  --publish 9031:9031 \\\n  --detach \\\n  --env SERVER_PROFILE_URL=https://github.com/&lt;your_username&gt;/pingidentity-server-profiles.git \\\n  --env SERVER_PROFILE_PATH=getting-started/pingfederate \\\n  --env-file ~/.pingidentity/config \\\n  pingidentity/pingfederate:edge\n</code></pre> <p>Private Repo</p> <p>If your GitHub server-profile repo is private, use the <code>username:token</code> format so the container can access the repository. For example, <code>https://github.com/&lt;your_username&gt;:&lt;your_access_token&gt;/pingidentity-server-profiles.git</code>. For more information, see Using Private Github Repositories.</p> </li> <li> <p>To display the logs as the container starts up, enter:</p> <pre><code>docker container logs -f pingfederate\n</code></pre> </li> <li> <p>In a browser, go to https://localhost:9999/pingfederate/app to display the PingFederate console.</p> </li> </ol>"},{"location":"how-to/profiles/#using-local-directories","title":"Using Local Directories","text":"<p>This method is particularly helpful when developing locally and the configuration isn't ready to be distributed (using Github, for example).</p> <p>We'll use PingFederate as an example. The local directories used by our containers to persist state and data, <code>/opt/in</code> and <code>/opt/out</code>, will be bound to another local directory and mounted as Docker volumes. This is our infrastructure for modifying the server profile.</p> <p>Bind Mounts in Production</p> <p>Docker recommends that you never use bind mounts in a production environment. This method is solely for developing server profiles. For more information, see the Docker Documentation.</p> <ul> <li> <p>The <code>/opt/out</code> directory</p> <p>All configurations and changes during our container runtimes (persisted data) are captured here. For example, the PingFederate image <code>/opt/out/instance</code> contains much of the typical PingFederate root directory: <pre><code>.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 SNMP\n\u251c\u2500\u2500 bin\n\u251c\u2500\u2500 connection_export_examples\n\u251c\u2500\u2500 etc\n\u251c\u2500\u2500 legal\n\u251c\u2500\u2500 lib\n\u251c\u2500\u2500 log\n\u251c\u2500\u2500 modules\n\u251c\u2500\u2500 sbin\n\u251c\u2500\u2500 sdk\n\u251c\u2500\u2500 server\n\u251c\u2500\u2500 tools\n\u2514\u2500\u2500 work\n</code></pre></p> </li> <li> <p>The <code>/opt/in</code> directory</p> <p>If a mounted <code>opt/in</code> directory exists, our containers reference this directory at startup for any server profile structures or other relevant files. This method is in contrast to a server profile provided using a Github URL assigned to the <code>SERVER_PROFILE_PATH</code> environment variable, such as, <code>--env SERVER_PROFILE_PATH=getting-started/pingfederate</code>.</p> <p>For the data each product writes to a mounted <code>/opt/in</code> directory, see Server profile structures.</p> <p>These directories are useful for building and working with local server-profiles. The <code>/opt/in</code> directory is particularly valuable if you don't want your containers to access Github for data (the default for our server profiles).</p> </li> </ul> <p>The following example deployment uses PingFederate.</p> <ol> <li> <p>Deploy PingFederate using our sample getting-started Server Profile, and mount <code>/opt/out</code> to a local directory:</p> <pre><code>docker run \\\n    --name pingfederate \\\n    --publish 9999:9999 \\\n    --detach \\\n    --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\\n    --env SERVER_PROFILE_PATH=getting-started/pingfederate \\\n    --env-file ~/.pingidentity/config \\\n    --volume /tmp/docker/pf:/opt/out \\\npingidentity/pingfederate:edge\n</code></pre> <p>Make sure the local directory (in this case, <code>/tmp/docker/pf</code>) isn't already created. Docker needs to create this directory for the mount to <code>/opt/out</code>.</p> </li> <li> <p>Go to the mounted local directory (in this case, <code>/tmp/docker/pf</code>), then make and save some configuration changes to PingFederate using the management console.</p> <p>As you save the changes, you'll be able to see the files in the mounted directory change. For PingFederate, an <code>instance</code> directory is created. This is a PingFederate server profile.</p> </li> <li> <p>Stop and remove the container and start a new container, adding another <code>/tmp/docker/pf</code> bind mounted volume, this time to <code>/opt/in</code>:</p> <pre><code>docker container rm pingfederate\n\ndocker run \\\n  --name pingfederate-local \\\n  --publish 9999:9999 \\\n  --detach \\\n  --volume /tmp/docker/pf:/opt/out \\\n  --volume /tmp/docker/pf:/opt/in \\\npingidentity/pingfederate:edge\n</code></pre> <p>The new container will now use the changes you made using the PingFederate console. In the logs, you can see where <code>/opt/in</code> is used:</p> <p><code>sh docker logs pingfederate-local</code></p> </li> <li> <p>Stop and remove the new container.</p> <p>Remember your <code>/tmp/docker/pf</code> directory will stay until you remove it (or until your machine is rebooted because this is in the <code>/tmp</code> directory):</p> <pre><code>docker container rm pingfederate-local\n</code></pre> <p>If you also want to remove your work, enter:</p> <pre><code>rm -rf /tmp/docker/pf\n</code></pre> </li> </ol>"},{"location":"how-to/profilesLayered/","title":"Layering server profiles","text":"<p>One of the benefits of our Docker images is the ability to layer product configuration. By using small, discrete portions of your configuration, you can build and assemble a server profile based on multiple installations of a product.</p> <p>A typical organization can have multiple installations of our products, each using different configurations. By layering the server profiles, you can reuse the configurations that are common across environments, leading to fewer configurations to manage.</p> <p>You can have as many layers as needed. Each layer of the configuration is copied on top of the container's filesystem (not merged).</p> <p>Layer Precedence</p> <p>The profile layers are applied starting at the top layer and ending at the base layer. This ordering might not be apparent at first.</p>"},{"location":"how-to/profilesLayered/#before-you-begin","title":"Before you begin","text":"<p>You must:</p> <ul> <li>Complete Get Started to set up your DevOps environment and run a test deployment of the products.</li> </ul>"},{"location":"how-to/profilesLayered/#about-this-task","title":"About this task","text":"<p>You will:</p> <ul> <li>Create a layered server profile.</li> <li>Assign the environment variables for the deployment.</li> <li>Deploy the layered server profile.</li> </ul>"},{"location":"how-to/profilesLayered/#creating-a-layered-server-profile","title":"Creating a layered server profile","text":"<p>For this guide, PingFederate is used along with the server profile located in the pingidentity-server-profiles repository. You should fork this repository to your Github repository, then pull your Github repository to a local directory. After you have finished creating the layered profile, you can push your updates to your Github repository and reference it as an environment variable to run the deployment.</p> <p>You will create separate layers for:</p> <ul> <li>Product license</li> <li>Extensions (such as, Integration Kits and Connectors)</li> </ul> <p>For this example, these layers will be applied on top of the PingFederate server profile. However, you can span configurations across multiple repositories if you want.</p> <p>You can find the complete working, layered server profile of the PingFederate example from this guide in the pingidentity-server-profiles/layered-profiles directory.</p> <p>Because PingFederate's configuration is file-based, the layering works by copying configurations on top of the PingFederate container\u2019s file system.</p> <p>Files Copied</p> <p>Files are copied, not merged. It is best practice to only layer items that will not be impacted by other configuration files.</p>"},{"location":"how-to/profilesLayered/#creating-the-base-directories","title":"Creating the base directories","text":"<p>Create a working directory named <code>layered_profiles</code> and within that directory create <code>license</code> and <code>extensions</code> directories. When completed, your directory structure should be:</p> <pre><code>\u2514\u2500\u2500 layered_profiles\n   \u251c\u2500\u2500 extensions\n   \u2514\u2500\u2500 license\n</code></pre>"},{"location":"how-to/profilesLayered/#constructing-the-license-layer","title":"Constructing the license layer","text":"<ol> <li>Go to the <code>license</code> directory and create a <code>pingfederate</code> subdirectory.</li> <li> <p>Create the PingFederate license file directory path under the <code>pingfederate</code> directory.</p> <p>The PingFederate license file resides in the <code>/instance/server/default/conf/</code> path.</p> <pre><code>mkdir -p instance/server/default/conf/\n</code></pre> <p>Your license profile path should look like this:</p> <pre><code>\u2514\u2500\u2500 license\n   \u2514\u2500\u2500 pingfederate\n      \u2514\u2500\u2500 instance\n         \u2514\u2500\u2500 server\n               \u2514\u2500\u2500 default\n                  \u2514\u2500\u2500 conf\n                     \u2514\u2500\u2500 pingfederate.lic\n</code></pre> </li> <li> <p>Copy your <code>pingfederate.lic</code> file to <code>license/pingfederate/instance/server/default/conf</code>.</p> <p>Using the DevOps evaluation license, when the PingFederate container is running, you can find the license in the container file system <code>/opt/out/instance/server/default/conf</code> directory.</p> <p>You can copy the <code>pingfederate.lic</code> file from the Docker file system using the syntax: <code>docker cp &lt;container&gt; &lt;source-location&gt; &lt;target-location&gt;</code></p> <p>For example:</p> <pre><code>docker cp \\\n   pingfederate \\\n   /opt/in/instance/server/default/conf/pingfederate.lic \\\n   ${HOME}/projects/devops/layered_profiles/license/pingfederate/instance/server/default/conf\n</code></pre> <p>Using the <code>pingctl</code> tool (update product and version accordingly):</p> <pre><code>pingctl license pingfederate 11.1 &gt; \\\n   ${HOME}/projects/devops/layered_profiles/license/pingfederate/instance/server/default/conf\n</code></pre> </li> </ol>"},{"location":"how-to/profilesLayered/#building-the-extensions-layer","title":"Building the extensions layer","text":"<ol> <li>Go to the <code>layered-profiles/extensions</code> directory and create a <code>pingfederate</code> subdirectory.</li> <li> <p>Create the PingFederate extensions directory path under the <code>pingfederate</code> directory.</p> <p>The PingFederate extensions reside in the <code>/instance/server/default/deploy</code> directory path.</p> <pre><code>mkdir -p instance/server/default/deploy\n</code></pre> </li> <li> <p>Copy the extensions you want to be available to PingFederate to the <code>layered-profiles/extensions/pingfederate/instance/server/default/deploy</code> directory .</p> <p>The extensions profile path should look similar to the following (extensions will vary based on your requirements):</p> <pre><code>\u2514\u2500\u2500 extensions\n   \u2514\u2500\u2500 pingfederate\n      \u2514\u2500\u2500 instance\n            \u2514\u2500\u2500 server\n               \u2514\u2500\u2500 default\n                  \u2514\u2500\u2500 deploy\n                        \u251c\u2500\u2500 pf-aws-quickconnection-2.0.jar\n                        \u251c\u2500\u2500 pf-azure-ad-pcv-1.2.jar\n                        \u2514\u2500\u2500 pf-slack-quickconnection-3.0.jar\n</code></pre> </li> </ol>"},{"location":"how-to/profilesLayered/#assigning-environment-variables","title":"Assigning environment variables","text":"<p>Although this deployment assigns the environment variables for use in a Docker Compose YAML file, you can use the following technique with any Docker or Kubernetes deployment.</p> <p>If you want to use your own Github repository for the deployment in the following examples, replace:</p> <pre><code>SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git\n</code></pre> <p>with:</p> <pre><code>SERVER_PROFILE_URL=https://github.com/&lt;your-username&gt;/pingidentity-server-profiles.git\n</code></pre> <p>Private Github Repo</p> <p>If your GitHub server-profile repo is private, use the <code>username:token</code> format so the container can access the repository. For example, <code>https://github.com/&lt;your_username&gt;:&lt;your_access_token&gt;/pingidentity-server-profiles.git</code>. For more information, see Using Private Github Repositories.</p> <ol> <li> <p>Create a new <code>docker-compose.yaml</code> file.</p> </li> <li> <p>Add your license profile to the YAML file.</p> <p>For example:</p> <pre><code>- SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git\n- SERVER_PROFILE_PATH=layered-profiles/license/pingfederate\n</code></pre> <p><code>SERVER_PROFILE</code> supports <code>URL</code>, <code>PATH</code>, <code>BRANCH</code> and <code>PARENT</code> variables.</p> </li> <li> <p>Using <code>SERVER_PROFILE_PARENT</code>, instruct the container to retrieve its parent configuration by specifying the <code>extensions</code> profile as the parent:</p> <pre><code>- SERVER_PROFILE_PARENT=EXTENSIONS\n</code></pre> <p><code>SERVER_PROFILE</code> can be extended to reference additional profiles. Because we specified the license profile's parent as <code>EXTENSIONS</code>, we can extend <code>SERVER_PROFILE</code> by referencing the <code>EXTENSIONS</code> profile (prior to the <code>URL</code> and <code>PATH</code> variables):</p> <pre><code>- SERVER_PROFILE_EXTENSIONS_URL=https://github.com/pingidentity/pingidentity-server-profiles.git\n- SERVER_PROFILE_EXTENSIONS_PATH=layered-profiles/extensions/pingfederate\n</code></pre> </li> <li> <p>Set <code>GETTING_STARTED</code> as the <code>EXTENSIONS</code> parent and declare the <code>URL</code> and <code>PATH</code>:</p> <pre><code>- SERVER_PROFILE_EXTENSIONS_PARENT=GETTING_STARTED\n- SERVER_PROFILE_GETTING_STARTED_URL=https://github.com/pingidentity/pingidentity-server-profiles.git\n- SERVER_PROFILE_GETTING_STARTED_PATH=getting-started/pingfederate\n</code></pre> <p>Because the <code>GETTING_STARTED</code> profile is the last profile to add, it will not have a parent.</p> <p>Your <code>environment</code> section of the <code>docker-compose.yaml</code> file should look similar to this:</p> <pre><code>environment:\n# **** SERVER PROFILES BEGIN ****\n# Server Profile - Product License\n- SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git\n- SERVER_PROFILE_PATH=layered-profiles/license/pingfederate\n- SERVER_PROFILE_PARENT=EXTENSIONS\n\n# Server Profile - Extensions\n- SERVER_PROFILE_EXTENSIONS_URL=https://github.com/pingidentity/pingidentity-server-profiles.git\n- SERVER_PROFILE_EXTENSIONS_PATH=layered-profiles/extensions/pingfederate\n- SERVER_PROFILE_EXTENSIONS_PARENT=GETTING_STARTED\n\n# Base Server Profile\n- SERVER_PROFILE_GETTING_STARTED_URL=https://github.com/pingidentity/pingidentity-server-profiles.git\n- SERVER_PROFILE_GETTING_STARTED_PATH=getting-started/pingfederate\n# **** SERVER PROFILE END ****\n</code></pre> </li> </ol>"},{"location":"how-to/profilesLayered/#deploying-the-layered-profile","title":"Deploying the layered profile","text":"<ol> <li>Push your profiles and updated <code>docker-compose.yaml</code> file to your GitHub repository.</li> <li>Deploy the stack with the layered profiles.</li> </ol> <p>To view this example in its entirety, including the profile layers and <code>docker-compose.yaml</code> file, see the pingidentity-server-profiles/layered-profiles directory.</p>"},{"location":"how-to/profilesPingDataExtensions/","title":"Including Extensions in PingData Server Profiles","text":"<p>Server SDK extension zip files can be included in your server profile for PingData products (PingAuthorize, PingDataSync, PingDirectory, and PingDirectoryProxy). The zip files can be included directly, or can be pulled from a remote URL when the container starts up.</p>"},{"location":"how-to/profilesPingDataExtensions/#the-pdprofileserver-sdk-extensions-directory","title":"The pd.profile/server-sdk-extensions/ directory","text":"<p>Any desired extension zip files should be included in the pd.profile/server-sdk-extensions/ directory of your server profile. Extension zip files in this directory will be installed during the setup process.</p>"},{"location":"how-to/profilesPingDataExtensions/#pulling-extension-zip-files-from-an-external-url","title":"Pulling extension zip files from an external URL","text":"<p>The hook scripts support pulling down extension zip files from a defined URL, to avoid having to commit zip archives to a Git repository. To do this, a <code>remote.list</code> file should be included in the extensions/ directory of your server profile. Any file with a name ending in <code>remote.list</code> in the extensions/ directory will be treated as a list of extensions.</p> <p>Ensure extensions are in the right folder</p> <p>When listing extensions to pull down via curl, the list must be placed in the extensions/ directory of the server profile. When directly including extension zip files, the zip files must be placed in the pd.profile/server-sdk-extensions/ directory of the server profile.</p> <p>For an example, see the extension list included in our baseline PingDirectory server profile.</p> <p>Separate multiple extensions with line breaks.</p> <p>A URL can also be specified in the downloaded zip file. To do this, add a space between the extension zip URL and the URL that will provide the SHA1 hash. For example: <pre><code>https://example.com/extension.zip https://example.com/extension/sha1\n</code></pre></p> <p>Set the <code>ENABLE_INSECURE_REMOTE_EXTENSIONS</code> environment variable to <code>true</code> to allow installing extensions without the SHA1 hash check. By default the SHA1 check will be required. If a SHA1 URL is provided and the SHA does not match or the URL cannot be reached, the extension will not be installed.</p>"},{"location":"how-to/profilesSubstitution/","title":"Environment Substitution","text":"<p>In a typical environment, a product configuration is moved from server to server. Hostnames, endpoints, DNS information, and more need a way to be easily modified.</p> <p>By removing literal values and replacing them with environment variables, configurations can be deployed in multiple environments with minimal change.</p> <p>When templating profiles with variables that reference other products, use the conventions defined in PingBase Image Directory.</p> <p>All of our configuration files can be parameterized by adding variables using the syntax: <code>${filename.ext}.subst</code>.</p> <p></p>"},{"location":"how-to/profilesSubstitution/#passing-values-to-containers","title":"Passing Values to Containers","text":"<p>Within the environment section of your container definition, declare the variable and the value for the product instance.</p> <p>Values can be defined in many sources, such as inline, env_vars files, and Kubernetes ConfigMaps.</p> <p></p>"},{"location":"how-to/profilesSubstitution/#how-it-works","title":"How it Works","text":"<ol> <li> <p>A container startup is initiated.</p> </li> <li> <p>The configuration pulls a server profile from Git or from a bind mounted <code>/opt/in</code> volume.</p> </li> <li> <p>All files with a <code>.subst</code> extension are identified.</p> </li> <li> <p>The environment variables in the identified <code>.subst</code> files are replaced with the actual environment values.</p> </li> <li> <p>The <code>.subst</code> extension is removed from all the identified files.</p> </li> <li> <p>The product instance for the container is started.</p> </li> </ol> <p></p>"},{"location":"how-to/prometheus/","title":"Enabling PingDirectory Metrics with Prometheus","text":"<p>In the past, enabling metrics for PingDirectory required a manual process to setup the statsd configuration to enable the data to be made available to Prometheus. However, PingDirectory now includes an HTTP servlet extension that can be enabled to expose metrics in Prometheus format.</p> <p>You can refer to the documentation for the <code>dsconfig</code> commands to enable the Prometheus metrics.  The link above is for PingDirectory 9.2, but the process is the same for newer versions.</p> <p>These <code>dsconfig</code> commands can be included in a server profile to ensure that the configuration is applied when the server is started. See here for an example.</p>"},{"location":"how-to/reEncryptPingDirectoryData/","title":"Re-encrypting backend data for a set of PingDirectory pods","text":"<p>PingDirectory uses encryption settings definitions to manage how data is encrypted in the database. When setting a new preferred encryption settings definition, the new definition will be used for all subsequent data encryption, but existing data remains encrypted with an older key.</p> <p>In many cases this is acceptable, and no additional work needs to be done. However in cases such as when an existing key might have been compromised, you will want to completely transition to using the new definition. This page describes the steps necessary to do this.</p> <p>For PingDirectory documentation on this scenario, see https://docs.pingidentity.com/r/en-us/pingdirectory-93/pd_sec_re-encrypt_database</p> <p>This page will describe how to follow the steps listed on the above page in a Kubernetes environment with several PingDirectory pods.</p>"},{"location":"how-to/reEncryptPingDirectoryData/#example-starting-helm-values-using-the-ping-devops-helm-chart","title":"Example starting Helm values using the ping-devops Helm chart","text":"<p>For demonstration, we will be using these Helm values with the ping-devops Helm chart: <pre><code>pingdirectory:\n  enabled: true\n  container:\n    replicaCount: 3\n  envs:\n    SERVER_PROFILE_URL: https://path/to/profile.git\n    SERVER_PROFILE_PATH: my-profiles/pingdirectory\n    ENCRYPTION_PASSWORD_FILE: /opt/staging/.sec/encryption-passphrase1.txt\n</code></pre></p>"},{"location":"how-to/reEncryptPingDirectoryData/#updating-the-encryption-settings-database-with-the-new-preferred-definition","title":"Updating the encryption settings database with the new preferred definition","text":"<p>The first step is to update the encryption settings database with your new preferred encryptions settings definition. For details on doing this manually, see the PingDirectory documentation.</p> <p>If you are using the <code>ENCRYPTION_PASSWORD_FILE</code> environment variable to control encryption for your pods, you can simply point that variable to a different file with a new passphrase and restart the pods. After the restart, the pods will use the new definition based on the <code>ENCRYPTION_PASSWORD_FILE</code> value. For example, with the environment variable updated:</p> <pre><code>pingdirectory:\n  enabled: true\n  container:\n    replicaCount: 3\n  envs:\n    SERVER_PROFILE_URL: https://path/to/profile.git\n    SERVER_PROFILE_PATH: my-profiles/pingdirectory\n    ENCRYPTION_PASSWORD_FILE: /opt/staging/.sec/encryption-passphrase2.txt\n</code></pre> <p>Whatever method you use to update the encryption settings database, ensure that each pod has the new definition in the encryption settings database before continuing. Use the <code>encryption-settings</code> command-line tool to view the contents of the encryption settings database.</p>"},{"location":"how-to/reEncryptPingDirectoryData/#disabling-replication-and-deleting-the-replication-database","title":"Disabling replication and deleting the replication database","text":"<p>Now replication must be disabled between the pods before the data is exported and re-imported, and the replication database must be deleted to ensure there are no remaining entries encrypted with the old definition.</p> <p>Exec into one of the pods and use the <code>dsreplication disable</code> command to disable replication between each of the servers. Ensure that each server is in its own single-server topology using the <code>dsreplication status</code> command.</p> <p>Run <code>rm -r /opt/out/instance/changelogDb/</code> on each of the pods individually, to remove any lingering entries from the replication database that may have been encrypted with the old definition.</p>"},{"location":"how-to/reEncryptPingDirectoryData/#scaling-down-to-one-pod-and-re-importing-the-data","title":"Scaling down to one pod and re-importing the data","text":"<p>The data must now be exported and re-imported with the server offline. To do this, we will scale down to a single pod (however we do not need to delete the persistent volumes of the other pods). We will also force the final pod to export and re-import its data so that it is encrypted with the new preferred definition. The <code>PD_FORCE_DATA_REIMPORT</code> environment variable can be used to force an export and re-import of the data before the server starts up.</p> <p>Note that the <code>PD_FORCE_DATA_REIMPORT</code> was added in the <code>2307</code> docker image release for PingDirectory. Prior to this a custom hook script would be needed to force the data export and re-import.</p> <pre><code>pingdirectory:\n  enabled: true\n  container:\n    replicaCount: 1\n  envs:\n    SERVER_PROFILE_URL: https://path/to/profile.git\n    SERVER_PROFILE_PATH: my-profiles/pingdirectory\n    ENCRYPTION_PASSWORD_FILE: /opt/staging/.sec/encryption-passphrase2.txt\n    PD_FORCE_DATA_REIMPORT: \"true\"\n</code></pre>"},{"location":"how-to/reEncryptPingDirectoryData/#scaling-back-up","title":"Scaling back up","text":"<p>Now we can scale back up to the full number of pods, and stop forcing the data export and re-import. When the removed pods restart, they will rejoin the topology and will initialize their data from the seed pod (pod 0), which will be encrypted with the new preferred definition.</p> <pre><code>pingdirectory:\n  enabled: true\n  container:\n    replicaCount: 3\n  envs:\n    SERVER_PROFILE_URL: https://path/to/profile.git\n    SERVER_PROFILE_PATH: my-profiles/pingdirectory\n    ENCRYPTION_PASSWORD_FILE: /opt/staging/.sec/encryption-passphrase2.txt\n</code></pre> <p>The backend data will now be encrypted with the new preferred definition.</p> <p>Note that in some cases an encryption settings definition may be used for more than encrypting backend data. For example, files can be encrypted using encryption settings definitions. By default some pin files in the server root will be encrypted if encryption was enabled during the first setup of the server, such as <code>config/keystore.pin</code>. If you need to manage how files are encrypted with encryption settings definitions, run <code>encrypt-file --help</code> for more information.</p>"},{"location":"how-to/s3Archive/","title":"S3 Archive of a PingDirectory Backup","text":"<p>Demonstration Only</p> <p>This guide is for demonstration purposes only. It is not intended for production use and is just one of many ways of archiving files to S3.  Other storage options might be available, depending on your provider.</p>"},{"location":"how-to/s3Archive/#before-you-begin","title":"Before you begin","text":"<p>You must:</p> <ul> <li>Complete Get Started to set up your DevOps environment and run a test deployment of the products</li> <li>Have some means of authenticating the sidecar container to S3.  This authentication can use an IAM role or other methods and is left for the user to implement.</li> </ul>"},{"location":"how-to/s3Archive/#high-level-backup-steps","title":"High-level backup steps","text":"<ul> <li>Configure some means of creating a backup of PingDirectory.  For this guide, an extension of the PingDirectory Backup and Sidecar is used.</li> <li>After the backup is made, use an archive script to upload the backup to S3.</li> <li>(Optional) Clean up the image filesystem of backups.</li> </ul>"},{"location":"how-to/s3Archive/#file-exploration","title":"File exploration","text":"<p>In the <code>30-helm/s3-sidecar</code> directory of this repository, you will find the following files:</p>"},{"location":"how-to/s3Archive/#dockerfile","title":"Dockerfile","text":"<p>This file extends the PingToolkit image, adding the AWS CLI.  You do not have to use the toolkit image as the base, it is used here for demonstration purposes.</p> <pre><code>## Dockerfile for AWS CLI\n## For demonstration purposes only\n## Not intended for production use\nFROM pingidentity/pingtoolkit:latest\n\nUSER root\n\n# Install AWS CLI\nRUN curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" \\\n    &amp;&amp; unzip awscliv2.zip \\\n    &amp;&amp; ./aws/install \\\n    &amp;&amp; sleep 5 \\\n    &amp;&amp; rm -rf ./aws*\n\nUSER 9031:0\n</code></pre> <p>After the image is built, it will need to be tagged and pushed to a repository that is accessible to the Kubernetes cluster.</p>"},{"location":"how-to/s3Archive/#pd-archive-backup-to-s3yaml","title":"pd-archive-backup-to-s3.yaml","text":"<p>This file will not be repeated in full here.  The top section creates ConfigMaps that define four sample scripts:</p> <ul> <li>archive.sh - This demonstration script is called by the backup.sh script to archive the backup to S3.  The bucket name and path will need to be updated to match your environment.</li> <li>fetch.sh - This demonstration script is called by the restore.sh script to fetch backup files from S3.  The bucket name and path will need to be updated to match your environment.</li> <li>backup.sh - This demonstration script is called by the sidecar container to create a backup of PingDirectory.  It then calls the archive.sh script (with no error handling or testing).</li> <li>restore.sh - This demonstration script would be executed either by a job or in the sidecar container to restore a backup of PingDirectory.</li> </ul> <p>These scripts are placed into the sidecar image under the /opt/in directory.</p> <p>Lines 218 and 219 will need modification to point to the registry and tag for the image with the AWS utilities added.</p>"},{"location":"how-to/s3Archive/#backup-operation","title":"Backup Operation","text":"<p>The process ran by this demonstration is straightforward.  Every 6 hours:</p> <ul> <li>A backup of the PingDirectory data is created</li> <li>The backup is archived to S3</li> </ul> <p>PingDirectory handles the removal of old backups based on the parameters set in the backup script.</p> <p>If you are observing the cluster at the time of the backup, an additional pod launches to execute the cronjob.  This pod terminates after the backup is complete.</p> <p>Over time, the S3 bucket will appear similar to the following.  For purposes of this screenshot, the backup and archive process ran every 5 minutes.</p> <p></p>"},{"location":"how-to/s3Archive/#restore-operation","title":"Restore Operation","text":"<p>In the event that a restore operation is needed, the restore.sh script can be used.  This script will:</p> <ul> <li>Download the backup from S3</li> <li>Restore the backup to the PingDirectory data directory</li> </ul> <p>A sample run of the script is shown below:</p> <pre><code>PingToolkit:demo-pingdirectory-0:/opt\n&gt; /opt/in/restore.sh &lt;admin-password&gt;\ndownload: s3://&lt;bucket-name&gt;/&lt;folder&gt;/userRoot/backup.info to userRoot/backup.info\ndownload: s3://&lt;bucket-name&gt;/&lt;folder&gt;/userRoot/backup.info.save to userRoot/backup.info.save\ndownload: s3://&lt;bucket-name&gt;/&lt;folder&gt;/userRoot/userRoot-backup-20231012191506Z to userRoot/userRoot-backup-20231012191506Z\ndownload: s3://&lt;bucket-name&gt;/&lt;folder&gt;/userRoot/userRoot-backup-20231012191006Z to userRoot/userRoot-backup-20231012191006Z\ndownload: s3://&lt;bucket-name&gt;/&lt;folder&gt;/userRoot/userRoot-backup-20231012192506Z to userRoot/userRoot-backup-20231012192506Z\ndownload: s3://&lt;bucket-name&gt;/&lt;folder&gt;/userRoot/userRoot-backup-20231012192006Z to userRoot/userRoot-backup-20231012192006Z\nReplication is not enabled\nuserRoot\nRestoring to the latest backups under /tmp/restore\nRestore order of backups: /tmp/restore/userRoot\n\n----- Doing a restore from /tmp/restore/userRoot -----\nRestore task 2023101219275710 scheduled to start immediately\n\nNOTE:  This tool is running as a task.  Killing or interrupting this tool will not have an impact on the task\nIf you wish to cancel the running task, that may be accomplished using the command:  manage-tasks --no-prompt --hostname localhost --port 1636 --bindDN \"cn=administrator\" --bindPassword \"********\" --cancel \"2023101219275710\"\n\n[12/Oct/2023:19:27:57 +0000] severity=\"SEVERE_WARNING\" msgCount=0 msgID=1880227932 message=\"Administrative alert type=backend-disabled id=2ecdf7c6-400e-4375-bc5c-8e4795c9c868 class=com.unboundid.directory.server.core.BackendConfigManager msg='Backend userRoot is disabled'\"\n[12/Oct/2023:19:27:57 +0000] severity=\"NOTICE\" msgCount=1 msgID=1880555611 message=\"Administrative alert type=config-change id=6f88ec8c-56c5-4146-9c71-ed387dd02d00 class=com.unboundid.directory.server.admin.util.ConfigAuditLog msg='A configuration change has been made in the Directory Server:  [12/Oct/2023:19:27:57.316 +0000] conn=-4 op=5857 dn='cn=Internal Client,cn=Internal,cn=Root DNs,cn=config' authtype=[Internal] from=internal to=internal command='dsconfig set-backend-prop --backend-name userRoot --set enabled:false''\"\n[12/Oct/2023:19:27:59 +0000] severity=\"NOTICE\" msgCount=2 msgID=8847445 message=\"Restored: .environment-open from backup with id '20231012192506Z' (size 76)\"\n[12/Oct/2023:19:27:59 +0000] severity=\"NOTICE\" msgCount=3 msgID=8847445 message=\"Restored: 00000000.jdb from backup with id '20231012192506Z' (size 11194781)\"\n[12/Oct/2023:19:27:59 +0000] severity=\"NOTICE\" msgCount=4 msgID=8847445 message=\"Restored: esTokenizer.ping from backup with id '20231012192506Z' (size 39)\"\n[12/Oct/2023:19:27:59 +0000] severity=\"SEVERE_WARNING\" msgCount=5 msgID=1880227932 message=\"Administrative alert type=je-environment-not-closed-cleanly id=2b0418d3-5a1a-4c86-9003-c0b9c5c8828e class=com.unboundid.directory.server.backends.jeb.RootContainer msg='The server has detected that the Berkeley DB JE environment located in directory '/opt/out/instance/db/userRoot' may not have been closed cleanly the last time it was opened (or that the backend has just been restored from a backup taken with the server online).  The database environment may need to replay changes from the end of the transaction log to guarantee the integrity of the data, and in some cases this may take a significant amount of time to complete'\"\n[12/Oct/2023:19:28:00 +0000] severity=\"NOTICE\" msgCount=6 msgID=8847402 message=\"The database backend userRoot using Berkeley DB Java Edition 7.5.12 and containing 20008 entries has started\"\n[12/Oct/2023:19:28:00 +0000] severity=\"NOTICE\" msgCount=7 msgID=1879507338 message=\"Starting group processing for backend userRoot\"\n[12/Oct/2023:19:28:00 +0000] severity=\"NOTICE\" msgCount=8 msgID=1879507339 message=\"Completed group processing for backend userRoot\"\n[12/Oct/2023:19:28:00 +0000] severity=\"INFORMATION\" msgCount=9 msgID=1891631108 message=\"Starting access control processing for backend userRoot\"\n[12/Oct/2023:19:28:00 +0000] severity=\"INFORMATION\" msgCount=10 msgID=12582962 message=\"Added 2 Access Control Instruction (ACI) attribute types found in context 'dc=example,dc=com' to the access control evaluation engine\"\n[12/Oct/2023:19:28:00 +0000] severity=\"NOTICE\" msgCount=11 msgID=1880555611 message=\"Administrative alert type=config-change id=a776d141-3eb9-44b5-9066-25e6e3a79f34 class=com.unboundid.directory.server.admin.util.ConfigAuditLog msg='A configuration change has been made in the Directory Server:  [12/Oct/2023:19:28:00.106 +0000] conn=-4 op=5868 dn='cn=Internal Client,cn=Internal,cn=Root DNs,cn=config' authtype=[Internal] from=internal to=internal command='dsconfig set-backend-prop --backend-name userRoot --set enabled:true''\"\nRestore task 2023101219275710 has been successfully completed\nRestore complete\n</code></pre>"},{"location":"how-to/saveConfigs/","title":"Saving your configuration changes","text":"<p>To save any configuration changes you make when using the products in the stack, you must set up a local Docker volume to persist state and data for the stack. If you don't do this, whenever you bring the stack down, your configuration changes will be lost.</p> <p>Mount a Docker volume location to the Docker <code>/opt/out</code> directory for the container. The location must be to a directory you haven't already created. Our Docker containers use the <code>/opt/out</code> directory to store application data.</p> <p>Mounting to /opt/out</p> <p>Make sure the local directory isn't already created. Docker needs to create this directory for the mount to <code>/opt/out</code>.</p> <p>You can mount a Docker volume for containers in a stack or for standalone containers.</p>"},{"location":"how-to/saveConfigs/#bind-mounting-for-a-stack","title":"Bind mounting for a stack","text":"<ol> <li>Add a <code>volumes</code> section under the container entry for each product in the <code>docker-compose.yaml</code> file you're using for the stack.</li> <li> <p>Under the <code>volumes</code> section, add a location to persist your data. For example:</p> <pre><code>pingfederate:\n.\n.\n.\nvolumes:\n- /tmp/compose/pingfederate_1:/opt/out\n</code></pre> </li> <li> <p>In the <code>environment</code> section, comment out the <code>SERVER_PROFILE_PATH</code> setting.</p> <p>The container then uses your <code>volumes</code> entry to supply the product state and data, including your configuration changes.</p> <p>When the container starts, this mounts <code>/tmp/compose/pingfederate_1</code> to the <code>/opt/out</code> directory in the container. You can also view the product logs and data in the <code>/tmp/compose/pingfederate_1</code> directory.</p> </li> <li> <p>Repeat this process for the remaining container entries in the stack.</p> </li> </ol>"},{"location":"how-to/saveConfigs/#bind-mounting-for-a-standalone-container","title":"Bind mounting for a standalone container","text":"<p>Add a <code>volume</code> entry to the <code>docker run</code> command:</p> <pre><code>docker run \\\n   --name pingfederate \\\n   --volume &lt;local-path&gt;:/opt/out \\\npingidentity/pingfederate:edge\n</code></pre>"},{"location":"how-to/saveConfigs/#getting-started-with-docker-compose-mounts","title":"Getting started with Docker Compose mounts","text":"<p>Within many of the docker-compose.yaml files in the Getting-Started repository, volume mounts to <code>opt/out</code> have been included to persist your configuration across container restarts.</p> <ul> <li> <p>To view the list of persisted volumes, enter:</p> <pre><code>docker volume list\n</code></pre> </li> <li> <p>To view the contents of the /opt/out/ volume when the container is running, enter:</p> <pre><code>docker container exec -it &lt;container id&gt; sh\ncd out\n</code></pre> </li> <li> <p>To view the contents of the /opt/out/ volume when the container is stopped, enter:</p> <pre><code>docker run --rm -i -v=&lt;volume name&gt;:/opt/out alpine ls\n</code></pre> </li> <li> <p>To remove a volume, enter:</p> <pre><code>docker volume rm &lt;volume name&gt;\n</code></pre> </li> <li> <p>To copy files from the container to your local filesystem, enter:</p> <pre><code>docker cp \\\n   &lt;container id&gt;:&lt;source path&gt; \\\n   &lt;destination path&gt;\neg.\ndocker cp \\\n   b867054293a1:/opt/out \\\n   ~/pingfederate/\n</code></pre> </li> <li> <p>To copy files from your local filesystem to the container, enter:</p> <pre><code>docker cp \\\n   &lt;source path&gt; \\\n   &lt;container id&gt;:&lt;destination path&gt;\neg.\ndocker cp \\\n   myconnector.jar \\\n   bb867054293a186:/opt/out/instance/server/default/deploy/\n</code></pre> </li> </ul>"},{"location":"how-to/secureContainers/","title":"Securing the Containers","text":""},{"location":"how-to/secureContainers/#docker-best-practices","title":"Docker Best Practices","text":"<p>Please visit the Docker website for more information on best practices to secure a container.</p>"},{"location":"how-to/secureContainers/#kubernetes-best-practices","title":"Kubernetes Best Practices","text":"<p>Please visit the Kubernetes website for more information on best practices to secure a deployment.</p>"},{"location":"how-to/secureContainers/#ping-identitys-docker-image-hardening-guide","title":"Ping Identity's Docker Image Hardening Guide","text":"<p>For best practices on securing your product Docker image, see Ping Identity's Hardening Guide.</p>"},{"location":"how-to/splunkLogging/","title":"Forwarding PingFederate and PingAccess logs to Splunk","text":"<p>This page provides an example of how PingFederate and PingAccess logs can be shipped to Splunk.  The principle of using a container for a single purpose is followed, and a sidecar for log collection and forwarding is placed in the appropriate Ping product pods.</p> <p>Video demonstration</p> <p>For a video demonstration of this process, visit this link.</p> <p>Splunk Demonstration Only</p> <p>This guide is for demonstration purposes only, but the principles will apply to a production implementation.  In addition, the process for other logging solutions will be similar.</p>"},{"location":"how-to/splunkLogging/#components-used","title":"Components Used","text":"<ol> <li>Ping DevOps Helm Chart</li> <li>Ping server-profiles repository</li> <li>Splunk Deployment</li> <li>Splunk Universal Forwarder Docker image</li> </ol>"},{"location":"how-to/splunkLogging/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to a Kubernetes cluster.  For this guide, a local Kubernetes cluster with the nginx-ingress controller and the MetalLB load balancer was used.  You might have to adjust how you access the product interface URLs, depending on your environment.</li> <li>Helm pingidentity/ping-devops chart &gt;= 0.9.11</li> </ul>"},{"location":"how-to/splunkLogging/#overall-process","title":"Overall Process","text":"<ol> <li>Configure the cluster environment</li> <li>Deploy Splunk Enterprise</li> <li>Configure Splunk and generate an HTTP Event Collector (HEC) token</li> <li>Create a configmap with the token for use by the Splunk Universal Forwarder (UF) sidecar</li> <li>Use Helm to deploy PingFederate and PingAccess with the sidecar attached to the engine pods</li> <li>Confirm logs and activity are visible in Splunk</li> </ol>"},{"location":"how-to/splunkLogging/#cluster-preparation","title":"Cluster preparation","text":"<pre><code># Create the namespace\nkubectl create ns splunk\n\n# Set the kubectl context to the namespace\nkubectl config set-context --current --namespace=splunk\n\n# Confirm\nkubectl config view --minify | grep namespace:\n</code></pre>"},{"location":"how-to/splunkLogging/#splunk-server-deployment","title":"Splunk Server deployment","text":"<p>Deploy the Splunk application:</p> <pre><code># Clone the `pingidentity-devops-getting-started` repository to a local directory\ngit clone \\\n  https://github.com/pingidentity/pingidentity-devops-getting-started.git\n\ncd pingidentity-devops-getting-started\n\n# Deploy Splunk\n# The splunk.yaml file assumes a load balancer is available in the cluster\nkubectl apply -f 20-kubernetes/splunk/splunk.yaml\n\n# Determine IP address assigned\n# 8000 is HTTP; 8088 is HTTPS\nkubectl get svc\n\nNAME     TYPE           CLUSTER-IP     EXTERNAL-IP       PORT(S)\nsplunk   LoadBalancer   10.105.171.4   192.168.163.172   8000:30416/TCP,8088:30364/TCP,9997:31770/TCP,9990:32292/UDP\n\n# Create corresponding entry in /etc/hosts\n# If your cluster has publicly-accessible IPs and DNS support, this step is not necessary\n# You would use the DNS entry assigned to the service.\n192.168.163.172 splunk.pingdemo.example\n</code></pre>"},{"location":"how-to/splunkLogging/#configure-splunk","title":"Configure Splunk","text":"<p>In this section, you will prepare Splunk for the logs from the products.</p> <p>Data Persistence</p> <p>In this demo, there is no data persistence for Splunk.  If you restart the Splunk pod, you will lose everything that is configured in the following steps.</p> <ul> <li>Navigate to the UI in a browser at <code>http://splunk.pingdemo.example:8000/en-US/account</code>. </li> <li>Login with the credentials admin / 2FederateM0re!</li> </ul>"},{"location":"how-to/splunkLogging/#create-an-index","title":"Create an index","text":"<ul> <li>Navigate to Settings &gt; Indexes and click the New Index button at the upper-right.</li> <li>Provide pinglogs as the Index Name.</li> <li>Accept all defaults and click Save.</li> </ul>"},{"location":"how-to/splunkLogging/#create-an-http-event-collector-hec","title":"Create an HTTP Event Collector (HEC)","text":"<ul> <li>Navigate to Settings &gt; Data inputs and click Add New in the <code>HTTP Event Collector</code> row.</li> <li>A wizard is launched and you are taken to the Select Source step.  Type <code>pinglogs</code> as the name and click the Next button in the upper panel.</li> <li>In the Input Settings step, add the <code>pinglogs</code> index to the Selected item(s) box by clicking on it in the Available item(s) list, then click the Review button in the upper panel.</li> <li>Confirm your entries and click the Submit button in the upper panel.</li> <li>A token is generated.  Save this token to a scratch file or buffer for use in configuring Splunk in a moment.</li> </ul>"},{"location":"how-to/splunkLogging/#add-the-ping-product-applications-to-splunk","title":"Add the Ping product applications to Splunk","text":"<ul> <li>Navigate to Apps &gt; Find More Apps.  The Apps link is at the upper-left of the UI.</li> <li>Filter the list of applications using <code>Ping</code>.  Add the PingFederate and PingAccess Apps for Splunk.</li> </ul> <p>Splunk Account</p> <p>You will need valid credentials from Splunk to install the applications.  You can use a free trial if necessary.</p> <p>PingDirectory App</p> <p>While not shown in this example, Ping also provides a Splunk App for PingDirectory.  You would need to attach the Splunk UF sidecar to your PingDirectory pods as done here for PingFederate and PingAccess.</p>"},{"location":"how-to/splunkLogging/#create-a-configmap","title":"Create a configmap","text":"<p>Use the HEC token generated earlier to update the file <code>20-kubernetes/splunk/splunk-config-init.yaml</code> (search for #CHANGEME).</p> <p>Apply the file:</p> <pre><code>kubectl apply -f 20-kubernetes/splunk/splunk-config-init.yaml\n</code></pre>"},{"location":"how-to/splunkLogging/#deploy-the-ping-stack-with-splunk-uf-as-a-sidecar","title":"Deploy the Ping stack with Splunk UF as a sidecar","text":"<pre><code># Create the DevOps secret for temporary Ping license\npingctl k8s generate devops-secret | kubectl apply -f -\n\n# Install Ping and Ingress\nhelm upgrade --install myping pingidentity/ping-devops -f 20-kubernetes/splunk/values.yaml -f 30-helm/ingress-demo.yaml\n</code></pre> <p>This command deploys PingDirectory, PingFederate, and PingAccess with:</p> <ul> <li>Baseline Server Profiles</li> <li>Splunk Logs Profile layer for the PingAccess and PingFederate engine pods</li> <li>Splunk UF sidecar for the PingAccess and PingFederate engine pods</li> </ul> <p>Server Profile Repository</p> <p>The <code>values.yaml</code> file in this guide is using a directory in the Ping server profiles repository. That profile folder has log4j configuration files that format the logs from the PingAccess and PingFederate product containers for use in Splunk.  These files are also in the backing repository for this portal under the <code>20-kubernetes/splunk/pingaccess</code> and <code>20-kubernetes/splunk/pingfederate</code> directories, respectively.</p>"},{"location":"how-to/splunkLogging/#confirm-in-splunk","title":"Confirm in Splunk","text":"<p>Eventually you should see product logs in Splunk by searching: <code>index=\"main\"</code>.  The first logs will appear when the PingFederate engine has launched fully.</p> <p></p> <p>To see the Splunk App dashboards in operation, generate some traffic in the products to populate them.  For example, for PingAccess, you can access https://myping-pingaccess-engine.pingdemo.example/anything, which will be rejected, but you will see the activity populated.  Also, you can login to the administrative console at http://myping-pingaccess-admin.pingdemo.example with the credentials administrator / 2FederateM0re.</p> <p></p>"},{"location":"how-to/splunkLogging/#references","title":"References","text":"<p>This list includes some of the references used in the creation of this document:</p> <ul> <li>PingFederate Logs formatting for Splunk</li> <li>PingFederate Dashboard reference</li> <li>Splunk Universal Forwarder (SUF) in Kubernetes</li> <li>Splunk configuration for inputs via HTTP</li> </ul>"},{"location":"how-to/upgradePingAccess/","title":"Upgrading PingAccess","text":"<p>In a DevOps environment, upgrades can be simplified through automation, orchestration, and separation of concerns.</p> <p>Notice</p> <ul> <li>Upgrading from PingAccess versions prior to 6.3.6 will not work using this method.</li> </ul>"},{"location":"how-to/upgradePingAccess/#caveats","title":"Caveats","text":"<ol> <li> <p>This Document Assumes Kubernetes and Helm</p> <p>The terms in this document will focus on deployments in a Kubernetes Environment using the ping-devops Helm chart. However, the concepts should apply to any containerized PingAccess Deployment.</p> </li> <li> <p>This Document will Become Outdated</p> <p>The examples referenced in this document point to a specific tag. This tag may not exist anymore at the time of reading. To correct the issue, update the tag on your file to <code>N-1</code> from the current PF version. </p> </li> <li> <p>Irrelevant Ingress</p> <p>The values.yaml files mentioned in this document expects an nginx ingress controller with class <code>nginx-public</code>. It is not an issue if your environment does not have this class. In such cases, the created ingresses will not be used.</p> </li> </ol>"},{"location":"how-to/upgradePingAccess/#configuration-forward","title":"Configuration Forward","text":"<p>Steps:</p> <ol> <li>Deploy your old version of PingAccess with server profile</li> <li>Export the configuration as a data.json file</li> <li>Copy the pa.jwk file to your server profile</li> <li>Deploy new PingAccess version with server profile</li> </ol> <p>Here we will walk through an example upgrade.</p>"},{"location":"how-to/upgradePingAccess/#deploy-your-old-version-of-pingaccess-with-server-profile","title":"Deploy your old version of PingAccess with server profile","text":"<p>Make sure you have a devops-secret</p> <p>If you are using this example as-is, you will need a devops-secret</p> <p>Be sure to change the ingress domain name value to your domain in 01-original.yaml</p> <p>Be sure to change the image tag value in 01-original.yaml</p> <p>In order to use the baseline server profile as outlined in this guide, you have to deploy PingFederate along with PingAccess</p> <p>Navigate to the getting started repository and deploy your old version of PingAccess.</p> <pre><code>$ helm upgrade --install pa-upgrade pingidentity/ping-devops -f 30-helm/pingaccess-upgrade/01-original.yaml\n</code></pre>"},{"location":"how-to/upgradePingAccess/#export-the-configuration-as-a-datajson-file","title":"Export the configuration as a data.json file","text":"<p>After your cluster is healthy, export the configuration as a json file and add it to your server profile so the start-up-deployer can use it to configure your upgraded PingAccess.</p> <pre><code>$ curl -k -u Administrator:2FederateM0re -H \"X-Xsrf-Header: PingAccess\" https://pa-upgrade-pingaccess-admin.ping-devops.com/pa-admin-api/v3/config/export &gt;~/&lt;insert path to server profile here&gt;/pingaccess/instance/data/data.json\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 22002  100 22002    0     0  42664      0 --:--:-- --:--:-- --:--:-- 43056\n</code></pre>"},{"location":"how-to/upgradePingAccess/#copy-the-pajwk-file-to-your-server-profile","title":"Copy the pa.jwk file to your server profile.","text":"<p>Copy the /conf/pa.jwk file. <pre><code>$ kubectl cp pa-upgrade-pingaccess-admin-0:/opt/out/instance/conf/pa.jwk ~/&lt;insert path to server profile here&gt;/pingaccess/instance/conf/pa.jwk\nDefaulted container \"pingaccess-admin\" out of: pingaccess-admin, wait-for-pingfederate-engine (init), generate-private-cert-init (init)\ntar: removing leading '/' from member names\n</code></pre> <p>Check to see that the data.json and pa.jwk files have been updated in your server-profile and push these changes to your repository</p>"},{"location":"how-to/upgradePingAccess/#deploy-new-pingaccess-version-with-server-profile","title":"Deploy new PingAccess version with server profile","text":"<p>Make sure to uninstall your old Ping Access cluster and remove any pvc's created.</p> <pre><code>$ helm uninstall pa-upgrade\nrelease \"pa-upgrade\" uninstalled\n$ kubectl get pvc\nNAME                                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nout-dir-pa-upgrade-pingaccess-admin-0   Bound    pvc-c1e5cd9b-35f5-4260-8704-3075fcf9b36e   4Gi        RWO            gp2            7m5s\n$ kubectl delete pvc out-dir-pa-upgrade-pingaccess-admin-0\npersistentvolumeclaim \"out-dir-pa-upgrade-pingaccess-admin-0\" deleted\n</code></pre> <p>Finally, update the PingAccess image version to the new target version and run.</p> <p>Be sure to change the ingress domain name value to your domain in 02-upgraded.yaml</p> <p>Be sure to change the image tag value in 02-upgraded.yaml</p> <p>Be sure to change the server profile url and path in 02-upgraded.yaml</p> <pre><code>helm upgrade --install pa-upgrade pingidentity/ping-devops -f 30-helm/pingaccess-upgrade/02-upgraded.yaml\n</code></pre> <p>At this time, you should have an upgraded PingAccess instance</p>"},{"location":"how-to/upgradePingCentral/","title":"Upgrading PingCentral","text":"<p>Example Only</p> <p>This example is for demonstration purposes only. It is not intended to reflect the complexities of a full production environment and will need to be adapted accordingly.</p> <p>Video walkthrough</p> <p>A video demonstration of this process can be found here.</p>"},{"location":"how-to/upgradePingCentral/#caveats","title":"Caveats","text":""},{"location":"how-to/upgradePingCentral/#kubernetes-and-helm","title":"Kubernetes and Helm","text":"<p>This document will focus on deployments in a Kubernetes environment using the ping-devops Helm chart. However, the concepts should apply to any containerized PingCentral deployment.</p>"},{"location":"how-to/upgradePingCentral/#this-document-will-become-outdated","title":"This document will become outdated","text":"<p>The examples referenced in this document point to a specific tag. This tag may not exist at the time of reading. To correct the issue, update the tag on your files appropriately.  This example uses versions 1.10 and 1.14, but the process should be similar for other versions in the future.</p>"},{"location":"how-to/upgradePingCentral/#mysql-is-used-as-the-pingcentral-datastore","title":"MySQL is used as the PingCentral Datastore","text":"<p>A separate MySql container is deployed manually to provide a backing store for PingCentral. In a production environment, you would likely use a managed database service.</p> <p>H2 Internal Database</p> <p>If you are not using an external database, and are on a version &lt;= 1.10, you will need to be aware of possible issues with the move to the H2v2 database internally.  See the PingCentral documentation for more details. It should be noted that the internal H2 database is not supported for production environments.</p>"},{"location":"how-to/upgradePingCentral/#the-ping-provided-baseline-profile-is-used-as-a-starting-point-for-pingcentral","title":"The Ping-provided baseline Profile is used as a starting point for PingCentral","text":"<p>The default baseline profile is used for this guide.  In a production environment, you would likely use a custom repository and profile.</p>"},{"location":"how-to/upgradePingCentral/#overall-process","title":"Overall Process","text":"<p>Steps:</p> <ol> <li>Deploy the old (1.10) version of PingCentral with the baseline server profile</li> <li>Create a test user in PingCentral and perform other validation steps</li> <li>Copy the <code>pingcentral.jwk</code> file to your server profile</li> <li>Deploy the new (1.14) PingCentral version with a custom server profile</li> <li>Validate the upgrade</li> </ol>"},{"location":"how-to/upgradePingCentral/#prerequisites","title":"Prerequisites","text":"<p>Assumptions and requirements:</p> <ul> <li>You have set up your DevOps environment and can run a test deployment of the products. For more information, see Get Started.</li> <li>This example was written using Docker Desktop with Kubernetes enabled on the Mac platform.  The version used was <code>4.27.1 (136059)</code>, which includes Docker Engine <code>v25.0.2</code> and Kubernetes <code>v1.29.1</code>.  The ingress-nginx controller version was <code>1.9.6</code>, deployed from Helm chart version <code>4.9.1</code>.</li> </ul>"},{"location":"how-to/upgradePingCentral/#environment-preparation","title":"Environment Preparation","text":""},{"location":"how-to/upgradePingCentral/#clone-the-getting-started-repository","title":"Clone the <code>getting-started</code> repository","text":"<ol> <li> <p>Clone the <code>pingidentity-devops-getting-started</code> repository to your local <code>${PING_IDENTITY_DEVOPS_HOME}</code> directory.</p> <p>pingctl utility</p> <p>The <code>${PING_IDENTITY_DEVOPS_HOME}</code> environment variable was set by running <code>pingctl config</code>.</p> <pre><code>cd \"${PING_IDENTITY_DEVOPS_HOME}\"\ngit clone \\\n  https://github.com/pingidentity/pingidentity-devops-getting-started.git\n</code></pre> </li> </ol>"},{"location":"how-to/upgradePingCentral/#prepare-the-environment-with-a-namespace-and-ingress-controller","title":"Prepare the environment with a namespace and ingress controller","text":"<ol> <li> <p>Create a namespace for running the stack in your Kubernetes cluster.  </p> <pre><code># Create the namespace\nkubectl create ns pingcentral-upgrade\n# Set the kubectl context to the namespace\nkubectl config set-context --current --namespace=pingcentral-upgrade\n# Confirm\nkubectl config view --minify | grep namespace:\n</code></pre> </li> <li> <p>Deploy the ingress controller to Docker Desktop:</p> <pre><code>helm upgrade --install ingress-nginx ingress-nginx \\\n--repo https://kubernetes.github.io/ingress-nginx \\\n--namespace ingress-nginx --create-namespace\n</code></pre> </li> <li> <p>Wait for the Nginx ingress to reach a healthy state by running the following command.  You can also observe the pod status using k9s or by running <code>kubectl get pods --namespace ingress-nginx</code>. You should see one controller pod running when the ingress controller is ready.  This command should exit after no more than 60 seconds or so, depending on the speed of your computer:</p> <pre><code>kubectl wait --namespace ingress-nginx \\\n  --for=condition=ready pod \\\n  --selector=app.kubernetes.io/component=controller \\\n  --timeout=90s\n</code></pre> </li> <li> <p>Create a secret in the namespace you will be using to run the example (upgrade) using the <code>pingctl</code> utility. This secret will obtain an evaluation license based on your Ping DevOps username and key:</p> <pre><code>pingctl k8s generate devops-secret | kubectl apply -f -\n</code></pre> </li> <li> <p>This example will use the Helm release name <code>demo</code> and DNS domain suffix <code>*pingdemo.example</code> for accessing applications.  Add the expected hostname to <code>/etc/hosts</code>:</p> <pre><code>echo '127.0.0.1 demo-pingcentral.pingdemo.example' | sudo tee -a /etc/hosts &gt; /dev/null\n</code></pre> </li> <li> <p>Navigate to your local directory where you cloned the repository (<code>\"${PING_IDENTITY_DEVOPS_HOME}\"/pingidentity-devops-getting-started/30-helm/</code>) directory and run the command shown here to deploy the MySQL pod using <code>kubectl</code>.  This deployment will be used as the backing store for PingCentral.</p> <pre><code>kubectl apply -f pingcentral-external-mysql-db/mysql.yaml\n</code></pre> </li> </ol>"},{"location":"how-to/upgradePingCentral/#fork-or-clone-the-ping-server-profile-repository","title":"Fork or clone the Ping server profile repository","text":"<ol> <li> <p>Fork the pingidentity-server-profiles repository to your GitHub account.  If you do not have a GitHub account, you can clone the repository instead.  For this guide, the repository will be forked to <code>test-server-profiles</code> which will be pulled locally to <code>${HOME}/projects/test-server-profiles</code>.</p> <pre><code>cd \"${HOME}/projects\"\ngit clone &lt;account&gt;/test-server-profiles.git ${HOME}/projects/\n</code></pre> </li> </ol>"},{"location":"how-to/upgradePingCentral/#deploy-the-old-version-of-pingcentral-with-the-ping-baseline-server-profile","title":"Deploy the old version of PingCentral with the Ping baseline server profile","text":"<ol> <li> <p>Install the initial version by running the command shown here.  In this example, the release <code>demo</code> forms the prefix for all objects created. The ingress is configured to use the ping-local domain:</p> <pre><code>cd \"${PING_IDENTITY_DEVOPS_HOME}\"/pingidentity-devops-getting-started/30-helm/\nhelm upgrade --install demo pingidentity/ping-devops -f pingcentral-upgrade/01-original.yaml\n</code></pre> </li> </ol> <p>This command will take a few minutes to complete.  You can monitor the progress using <code>kubectl get pods</code> or <code>k9s</code>.</p>"},{"location":"how-to/upgradePingCentral/#access-the-pingcentral-console","title":"Access the PingCentral Console","text":"<ol> <li> <p>Login to the PingCentral Administrative Console using the credentials <code>administrator/2Federate</code>.  The URL is https://demo-pingcentral.pingdemo.example.</p> </li> <li> <p>Select Users from the left navigation menu and click Add User. Create a user to be used to validate PingCentral after the upgrade (the user specifics and role do not matter for this example).</p> </li> </ol> <p>The left navigation panel should indicate version 1.10.0 at the bottom as shown here:     </p>"},{"location":"how-to/upgradePingCentral/#confirm-database-entries","title":"Confirm Database Entries","text":"<p>Shell into the MySQL pod and confirm that the user you created is present in the database.  Also, verify the version of PingCentral in the <code>DATABASECHANGELOG</code> table.</p> <pre><code>kubectl exec -it mysql-0 -- bash\n# Use the password from the mysql.yaml file (2Federate)\nmysql -u root -p\nuse pingcentral;\n# The user will be listed here\nselect * from users;\n# The version will be listed here, with the last lines of the table showing the most recent entries \n# at version v1.10 and v1.11 (46 lines total at the time of this writing)\nselect * from DATABASECHANGELOG;\n</code></pre>"},{"location":"how-to/upgradePingCentral/#add-the-pingcentraljwk-file-to-your-server-profile","title":"Add the <code>pingcentral.jwk</code> file to your server profile","text":"<ol> <li> <p>Copy the <code>pingcentral.jwk</code> file from the PingCentral pod.  This file will be used in the next step to configure the new PingCentral instance.  This example places the file in the repository directory from the fork created earlier.</p> <pre><code>kubectl cp demo-pingcentral-6d4bb97c98-m7vwb:/opt/out/instance/conf/pingcentral.jwk ${HOME}/projects/test-server-profiles/baseline/pingcentral/external-mysql-db/instance/conf/pingcentral.jwk\n</code></pre> </li> <li> <p>Check to see that the <code>pingcentral.jwk</code> file has been placed in your server-profile and push these changes to your repository.</p> <pre><code>cd \"${HOME}/projects/test-server-profiles\"\ngit add .\ngit commit -m \"Added pingcentral.jwk file\"\ngit push\n</code></pre> </li> </ol> <p>Pod Name</p> <p>The pod name from which you copy will vary.</p> <p>JWK Unique to PingCentral Instance</p> <p>The <code>pingcentral.jwk</code> file is used to encrypt and decrypt the PingCentral configuration.  It is unique to each PingCentral instance and must be copied to the new server profile.  Otherwise, the new pod will fail to start.</p> <p>File copy error</p> <p>If you receive an notification <code>tar: removing leading '/' from member names</code> when copying the file, it can be ignored.</p> <p>Security Warning</p> <p>Storing the <code>pingcentral.jwk</code> file in the server profile is not recommended for production environments.  In a production environment, you would likely use a managed key store service, vault, or other encrypted mechanism.</p>"},{"location":"how-to/upgradePingCentral/#deploy-the-new-version-of-pingcentral-with-a-custom-server-profile","title":"Deploy the new version of PingCentral with a custom server profile","text":"<ol> <li>Update the <code>/pingidentity-devops-getting-started/30-helm/pingcentral-upgrade/02-upgraded.yaml</code> file to point to the repository and profile directory that contains the JWK file. This example uses the <code>test-server-profiles</code> repository, but you should use your own information.</li> </ol> <pre><code>helm upgrade --install demo pingidentity/ping-devops -f pingcentral-upgrade/02-upgraded.yaml\n</code></pre> <p>The new pod will spin up, and when it is healthy, the old pod will be terminated.  At this time, you should have an upgraded PingCentral instance.  Log in to the administrative console as before.  The user you created earlier should still exist, and the version information at the lower left should indicate version 1.14.0.  In addition, a new left navigation item (Management) will be present that was not there before:</p> <p></p> <p>Finally, a check of the <code>DATABASECHANGELOG</code> table in the MySQL pod should show the new version of PingCentral as the last few entries in that table (51 entries as of this writing).  These updated entries indicate the database migration was successful.</p> <p>Not all versions have database migrations</p> <p>The 1.10 -&gt; 1.14 upgrade involved database updates.  Not all versions will have database updates, and corresponding <code>DATABASECHANGELOG</code> entries might not be present.</p>"},{"location":"how-to/upgradePingCentral/#cleanup","title":"Cleanup","text":"<p>After you have finished this demonstration, you can uninstall the Helm release and MySQL deployment, and delete the namespace:</p> <pre><code>helm uninstall demo\nkubectl delete -f pingcentral-external-mysql-db/mysql.yaml\nkubectl delete ns pingcentral-upgrade\n</code></pre> <p>Remove the entry from <code>/etc/hosts</code> if you do not plan to use the same hostname again:</p> <pre><code>sudo sed -i '' '/demo-pingcentral.pingdemo.example/d' /etc/hosts\n</code></pre> <p>You can also remove the <code>test-server-profiles</code> repository from your local machine and delete the forked repository from GitHub:</p> <pre><code>rm -rf \"${HOME}/projects/test-server-profiles\"\n</code></pre>"},{"location":"how-to/upgradePingDirectory/","title":"Upgrading PingDirectory","text":"<p>Because PingDirectory is essentially a database, in its container form, each node in a cluster has its own persisted volume. Additionally, because PingDirectory is an application that focuses on state, rolling out an upgrade isn't really the same as any other configuration update. However, the product software, and scripts in the image provide a process through which upgrades are drastically simplified.</p> <p>This use case focuses on a PingDirectory upgrade in a default Kubernetes environment where you upgrade a PingDirectory StatefulSet 9.0.0.1 to 9.1.0.0 using an incremental canary roll-out.</p>"},{"location":"how-to/upgradePingDirectory/#tips","title":"Tips","text":"<p>To ensure a successful upgrade process:</p> <ul> <li>Avoid combining configuration changes and version upgrades in the same rollout. This adds unnecessary complexity to debugging errors during an upgrade.</li> <li>Successfully complete an upgrade in a proper Dev/QA environment before trying anything in production.</li> <li>Upgrades will happen on one server at a time. Ensure you have enough resources on the remaining machines to prevent client impact.</li> <li>Follow a canary deployment pattern to ensure the changes will be successful before doing a full rollout. There is no good way to roll back a completed upgrade, so take any necessary steps to avoid this.</li> </ul>"},{"location":"how-to/upgradePingDirectory/#before-you-begin","title":"Before you begin","text":"<p>You must:</p> <ul> <li>Complete Get Started to set up your DevOps environment and run a test deployment of the products using Helm.</li> <li>Clone or download the <code>pingidentity-devops-getting-started/30-helm/pingdirectory-upgrade-partition</code> repository to your local <code>${HOME}/projects/devops</code> directory.</li> <li>Understand how to use our DevOps server profiles.</li> <li>Have access to a Kubernetes cluster.</li> <li>If you're upgrading in your own environment and using mounted licenses, have the license for the existing version in the <code>/opt/out</code> persisted volume and a license for the new version needs to be in <code>/opt/in</code>.</li> </ul> <p>The license locations aren't an needed if you're using our DevOps credentials in an evaluation context.</p>"},{"location":"how-to/upgradePingDirectory/#about-this-task","title":"About this task","text":"<p>You will:</p> <ul> <li>Start with a base stack.</li> <li>Set up a partition to make changes on just one node.</li> <li>Deploy changes to one node and fix any errors.</li> <li>Rollout changes to other nodes.</li> </ul>"},{"location":"how-to/upgradePingDirectory/#upgrade-process-overview","title":"Upgrade process overview","text":"<p>The key functionality for PingDirectory upgrades is the relationship between the image hooks and the <code>manage-profile</code> command in the product.</p> <p>The upgrade is processed as follows:</p> <ol> <li> <p>When a node starts for the first time, it' i's in <code>SETUP</code> mode and runs <code>manage-profile setup</code>.</p> </li> <li> <p>When a node restarts (for whatever reason), it runs <code>manage-profile replace-profile</code>.</p> <ul> <li> <p>This command compares the new profile to the old profile, and if there is a change, it tries standing up the server with the new profile.</p> </li> <li> <p>Errors are thrown if there's a configuration in the new profile that prevents it from being applied.</p> </li> </ul> </li> <li> <p>If <code>manage-profile replace-profile</code> detects a product version difference, it takes the same approach as any other restart. It attempts to migrate PingDirectory to the new version, and if it can't, the command fails.</p> <p>Because there is processing that happens automatically in the container during the upgrade, you should roll the change out to a small partition first and test it thoroughly before rolling it out to all. This partition also gives us room to revert back without impacting traffic in case something is not as expected.</p> <p>This process in standard Kubernetes is called a Canary Rollout and is derived from Kubernetes documentation on StatefulSet update strategies.</p> <p>\"Canary Rollout\" in this scenario focuses only on an incremental rollout of containers, not actually separating traffic. That could be done with additional tools like Istio or standing up another service and pairing separate labels and selectors.</p> </li> </ol>"},{"location":"how-to/upgradePingDirectory/#setting-up-the-base-stack","title":"Setting up the base stack","text":"<p>The YAML configuration files for this use case are in your cloned local copy of the <code>pingidentity-devops-getting-started</code> repository</p> <ol> <li> <p>To use the <code>1-initial.yaml</code> file in your local <code>pingidentity-devops-getting-started/30-helm/pingdirectory-upgrade-partition</code> directory to start with a PingDirectory StatefulSet using persistent volumes, enter:</p> <pre><code>helm upgrade --install releasename pingidentity/ping-devops -f 1-initial.yaml\n</code></pre> <p>All helm commands for this use case need to be run from the <code>pingidentity-devops-getting-started/30-helm/pingdirectory-upgrade-partition</code> directory.</p> <p>This stands up a two directory topology, each with its own Persistent Volume Claim using the default storage class.</p> </li> <li> <p>Wait for both nodes to be healthy before continuing and then enter:</p> <pre><code>kubectl get pods\n</code></pre> <p><code>pingdirectory-0</code> and <code>pingdirectory-1</code> should show <code>1/1</code> in the <code>READY</code> column.</p> </li> </ol>"},{"location":"how-to/upgradePingDirectory/#setting-up-a-partition","title":"Setting up a partition","text":"<p>To use the <code>2-partition.yaml</code> file in your local <code>pingidentity-devops-getting-started/30-helm/pingdirectory-upgrade-partition</code> directory to add a partition to <code>StatefulSet</code> for <code>updateStrategy</code>, enter:</p> <pre><code>helm upgrade --install releasename pingidentity/ping-devops -f 2-partition.yaml\n</code></pre> <p>This partition configuration signifies that any changes to <code>spec.template</code> will only be applied to nodes with a cardinal value higher than the partition value.</p> <p>The only other change is to the image tag. When this change is applied:</p> <ol> <li> <p>The <code>pingdirectory-1</code> pod is terminated and a new one with the new image is started.</p> </li> <li> <p>The new PingDirectory container is based on a specific version of PingDirectory, found in the <code>/opt/server</code> volume.</p> </li> <li> <p>The <code>manage-profile replace-profile</code> command is eventually triggered when the container detects PingDirectory is in a <code>RESTART</code> state. This command identifies the difference in the database version running, based on the persisted volume attached to <code>/opt/out</code>, and then attempts to upgrade. Information similar to the following will be displayed:</p> <pre><code>...\npingdirectory-1 Validating source and existing servers ..... Done\npingdirectory-1 Updating the server version from 8.0.0.1 to 8.1.0.0. Local database backends\npingdirectory-1 will be exported before the update in case a revert is necessary\npingdirectory-1 Exporting backend with backendID userRoot. This may take a while ..... Done\npingdirectory-1 Running the update tool ..... Done\n...\npingdirectory-1 Cleaning up after replace ..... Done\npingdirectory-1   manage-profile replace-profile returned 0\n</code></pre> <p>This process requires having licenses for both server versions available and in the right location.</p> <p>If <code>manage-profile replace-profile</code> completes without error, you'll see the container continue the migration and eventually start up PingDirectory.</p> <p>If <code>manage-profile replace-profile</code> fails, an error is displayed and the container exits. The errors are because of some conflict in the server profile. The partition you set up previously provides an isolated environment for working out errors.</p> <p>Work through any errors until you can get <code>manage-profile replace-profile</code> to complete successfully before continuing.</p> </li> </ol>"},{"location":"how-to/upgradePingDirectory/#rolling-out-the-changes","title":"Rolling out the changes","text":"<p>When you're confident your upgrade will occur smoothly:</p> <p>To use the <code>3-rollout-full-upgrade.yaml</code> file in your local <code>pingidentity-devops-getting-started/30-helm/pingdirectory-upgrade-partition</code> directory to deploy the rollout to the remaining nodes, enter:</p> <pre><code>helm upgrade --install releasename pingidentity/ping-devops -f 3-rollout-full-upgrade.yaml\n</code></pre> <p>This removes the partition.</p>"},{"location":"how-to/upgradePingfederate/","title":"Upgrading PingFederate","text":"<p>In a DevOps environment, upgrades can be simplified through automation, orchestration, and separation of concerns.</p> <p>General Steps:</p> <ul> <li>Persistent Volume Upgrade of <code>/opt/out/instance/server/default/data</code> on pingfederate-admin</li> <li>Server Profile Upgrade</li> <li>Migrate Cluster Discovery Settings</li> <li>Post Upgrade</li> </ul> <p>Persistent Volume Upgrade will include steps helpful to both pieces. Server Profile Upgrade will discuss extracting upgraded files.</p>"},{"location":"how-to/upgradePingfederate/#caveats","title":"Caveats","text":"<ol> <li> <p>This Document Assumes Kubernetes and Helm</p> <p>The terms in this document will focus on deployments in a Kubernetes Environment using the ping-devops Helm chart. However, the concepts should apply to any containerized PingFederate Deployment.</p> </li> <li> <p>This Document will Become Outdated</p> <p>The examples referenced in this document point to a specific tag. This tag may not exist anymore at the time of reading. To correct the issue, update the tag on your file to N -1 from the current PF version.</p> </li> <li> <p>Upgrades from Traditional Deployment</p> <p>It may be desirable to upgrade PingFederate along with migrating from a traditional environment. This is not recommended. Instead you should upgrade your current environment to the desired version of PingFederate and then create a profile that can be used in a containerized deployment.</p> </li> <li> <p>Persistent Volume on <code>/opt/out</code></p> <p>The suggested script should not be used if a persistent volume is attached to <code>/opt/out</code>. New software bits will not include special files built into the docker image. It is recommended to mount volumes on PingFederate Admin to <code>/opt/out/instance/server/default/data</code>. </p> </li> <li> <p>Irrelevant Ingress</p> <p>The values.yaml files mentioned in this document expects and nginx ingress controller with class <code>nginx-public</code>. It is not an issue if your environment doesn't have this, the created ingresses will not be used.</p> </li> </ol>"},{"location":"how-to/upgradePingfederate/#persistent-volume-upgrade","title":"Persistent Volume Upgrade","text":"<p>Steps needed in both Server-Profile upgrade and Persistent Volume upgrade include:</p> <ol> <li>Deploy your PingFederate version and server profile as background process</li> <li>Upgrade profile in container<ol> <li>Backup the files in your profile.</li> <li>Download the PingFederate software bits for the new version.</li> <li>Run upgrade utility</li> <li>diff to view the changes. (optional)</li> </ol> </li> <li>Reconfigure any variablized components.</li> <li>Export changes to your profile</li> </ol> <p>Here we will walk through an example upgrade.</p> <p>This Process Requires Container Exec Access</p> <p>Your orchestration user will need access to <code>kubectl exec -it &lt;pod&gt; -- sh</code> for multiple steps here.</p>"},{"location":"how-to/upgradePingfederate/#deploy-pingfederate-as-a-background-process","title":"Deploy PingFederate as a Background Process","text":"<p>Deploy your PingFederate version and server profile as background process with Helm:</p> <p>Make sure you have a devops-secret</p> <p>If you are using this example as is, you will need a devops-secret</p> <p>Be sure to change the ingress domain name value to your domain in 01-background.yaml</p> <p>Be sure to change the image tag value in 01-background.yaml</p> <pre><code>helm upgrade --install pf-upgrade pingidentity/ping-devops \\\n   --version 0.9.4 -f 30-helm/pingfederate-upgrade/01-background.yaml\n</code></pre> <p>The <code>args</code> section starts PingFederate as a background process and <code>tail -f /dev/null</code> as the foreground process.</p>"},{"location":"how-to/upgradePingfederate/#upgrade-profile-in-container","title":"Upgrade Profile in Container","text":"<p>The steps for upgrading can be automated with a script. Example scripts are included at <code>30-helm/pingfederate-upgrade/hooks</code>.</p> <p>To use the scripts:</p> <p>Copy the hooks folder to your PingFederate container</p> <pre><code>kubectl cp 30-helm/pingfederate-upgrade/hooks pf-upgrade-pingfederate-admin-0:/opt/staging\n</code></pre> <p>Copy the target PingFederate license to your PingFederate container See pingctl license documentation to retrieve an evaluation license, or provide an existing product license here.</p> <pre><code>kubectl cp pingfederate.lic pf-upgrade-pingfederate-admin-0:/tmp\n</code></pre> <p>Copy the target PingFederate software to your PingFederate container. See How to download Product Installation Files.</p> <pre><code>kubectl cp pingfederate-11.1.1.zip pf-upgrade-pingfederate-admin-0:/tmp\n</code></pre>"},{"location":"how-to/upgradePingfederate/#how-to-download-product-installation-files","title":"How to download Product Installation Files","text":"<ol> <li> <p>Navigate to Ping Identity's Download webpage.</p> </li> <li> <p>Select a Product download page, for example: PingFederate Download Page.</p> </li> <li> <p>Click on the download button for the desired installation method and product version.</p> </li> <li> <p>If prompted to sign in, please sign in and the download will begin. Alternatively, Sign In Here.</p> </li> <li> <p>If you do not have a Ping Identity account, you can create one on the Account Creation Page.</p> </li> </ol>"},{"location":"how-to/upgradePingfederate/#run-the-upgrade-utility","title":"Run the Upgrade Utility","text":"<p>The pf-upgrade.sh script will:</p> <ul> <li>Verify both the PingFederate software bits and new license file is on the container</li> <li>Backup the current /opt/out folder to /opt/current_bak</li> <li>Run the upgrade utility</li> <li>Overwrite /opt/out or /opt/out/instance/server/default/data with upgraded files</li> <li>Run diff between /opt/staging (server-profile location) and respective upgraded file. Diffs can be found in <code>/tmp/stagingDiffs</code></li> </ul> <p>Exec into the container and run the script.</p> <pre><code>kubectl exec -it pf-upgrade-pingfederate-admin-0 -- sh\ncd /opt/staging/hooks\nsh pf-upgrade.sh 10.3.4\n</code></pre> <p>At the conclusion of the script you will have an upgraded <code>/opt/out/instance/server/default/data</code> folder.</p>"},{"location":"how-to/upgradePingfederate/#server-profile-upgrade","title":"Server Profile Upgrade","text":"<p>If your profile is applied on each start of your container, you should keep your profile up to date with the product version you are deploying.</p> <p>After the previously run script, you can find upgraded profile files in <code>/opt/new_staging</code> These files will be hard-coded and you should follow Build a PingFederate Profile as needed for templating.</p> <p>Additionally, If you use the bulk-config data.json import it will not be found here. It should be imported via the standard process on the next container start.</p>"},{"location":"how-to/upgradePingfederate/#migrate-cluster-discovery-settings","title":"Migrate Cluster Discovery Settings","text":"<p>To simplify future upgrades, migrate the cluster discovery settings in the <code>tcp.xml</code> file to the <code>jgroups.properties</code> file.</p> <p>You can find the default <code>jgroups.properties</code> file here.</p> <p>For more information, see Migrate Cluster Discovery Settings in the PingFederate admin guide.</p>"},{"location":"how-to/upgradePingfederate/#post-upgrade","title":"Post Upgrade","text":"<p>To enable PingFederate admin as a foreground process, scale it down first.</p> <pre><code>kubectl scale sts pf-upgrade-pingfederate-admin --replicas=0\n</code></pre> <p>Finally, update PingFederate image version to new target PingFederate version and run as normal.</p> <p>Be sure to change the ingress domain name value to your domain in 02-upgraded.yaml</p> <p>Be sure to change the image tag value in 02-upgraded.yaml</p> <p><pre><code>helm upgrade --install pf-upgrade pingidentity/ping-devops --version 0.9.4 \\\n   -f 30-helm/pingfederate-upgrade/02-upgraded.yaml\n</code></pre> This will restart the admin console, and trigger a rolling update of all the engines.</p> <p>Old Profile</p> <p>The final yaml <code>30-helm/pingfederate-upgrade/02-upgraded.yaml</code> still points to the same profile. The steps that should have been completed in Server Profile Upgrade were not included.</p> <p>Connecting to the admin console will now show the upgraded version in cluster management.</p>"},{"location":"how-to/usingVault/","title":"Using Hashicorp Vault","text":"<p>This documentation provides details for using Hashicorp Vault and secrets with Ping Identity DevOps images.</p>"},{"location":"how-to/usingVault/#before-you-begin","title":"Before you begin","text":"<p>You must:</p> <ul> <li>Complete Get Started to set up your DevOps environment and run a test deployment of the products.</li> <li>Have a running Hashicorp Vault instance.</li> </ul>"},{"location":"how-to/usingVault/#about-this-task","title":"About this task","text":"<p>The following examples explain and show:</p> <ul> <li>How to use HashiCorp Vault Secrets in native PingIdentity DevOps images</li> <li>How to use HashiCorp Vault Injector in Kubernetes deployments</li> </ul>"},{"location":"how-to/usingVault/#kubernetes-hashicorp-vault-injector","title":"Kubernetes - HashiCorp Vault Injector","text":"<p>If you are using Kubernetes to deploy your containers, it's highly recommended to use the HashiCorp Vault Injector.  The following provides details on how to use secrets in a non-Kubernetes deployment, such as Docker-compose.</p> <p>If the HashiCorp Vault Injector Agent is installed, annotations can be added to the <code>.yaml</code> file of a Pod, Deployment, StatefulSet resource to pull in the secrets.  The snippet below provides an example set of annotations (placed in to the metadata of the container) to pull in a <code>pf.jwk</code> secret into a container.</p> <p>Helm Chart Stateful Set</p> <p>This is an StatefulSet example created using the PingIdentity DevOps Helm Chart. <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nspec:\n  template:\n    metadata:\n      annotations:\n        vault.hashicorp.com/agent-init-first: \"true\"\n        vault.hashicorp.com/agent-inject: \"true\"\n        vault.hashicorp.com/agent-inject-secret-devops-secret.env.json: secret/.../devops-secret.env\n        vault.hashicorp.com/agent-inject-template-devops-secret.env.json: |\n          {{ with secret \"secret/.../devops-secret.env\" -}}\n          {{ .Data.data | toJSONPretty }}\n          {{- end }}\n        vault.hashicorp.com/agent-inject-secret-devops-secret.env.json: secret/.../passwords\n        vault.hashicorp.com/agent-inject-template-passwords.json: |\n          {{ with secret \"secret/.../passwords\" -}}\n          {{ .Data.data | toJSONPretty }}\n          {{- end }}\n        vault.hashicorp.com/agent-pre-populate-only: \"true\"\n        vault.hashicorp.com/log-level: info\n        vault.hashicorp.com/preserve-secret-case: \"true\"\n        vault.hashicorp.com/role: k8s-default\n        vault.hashicorp.com/secret-volume-path: /run/secrets\n</code></pre></p>"},{"location":"how-to/usingVault/#secrets-variables","title":"Secrets - Variables","text":"<p>Using the previous example, the value for secret <code>secret/.../devops-secret.env</code> JSON will be pulled into the container as <code>/run/secrets/devops-secret.env.json</code>.</p> <p>Because this secret ends in the value of <code>.env</code>, it will further be turned into a property file with NAME=VALUE pairs and is available to the container environment on start up.</p> <p>Example of devops-secret.env transformed into files</p> <p><pre><code>{\n  \"PING_IDENTITY_DEVOPS_USER\": \"jsmith@example.com\",\n  \"PING_IDENTITY_DEVOPS_KEY\": \"xxxxx-xxxx-xxxxx-xxxxx-xxxx\"\n}\n</code></pre> creates the files <pre><code>    File: /run/secrets/devops-secret.env\nContents: PING_IDENTITY_DEVOPS_USER=\"jsmith@example.com\"\n          PING_IDENTITY_DEVOPS_KEY=\"xxxxx-xxxx-xxxxx-xxxxx-xxxx\"\n</code></pre></p>"},{"location":"how-to/usingVault/#secret-files","title":"Secret - Files","text":"<p>Using the previous example, the value for secret <code>secret/.../passwords</code> JSON will be pulled into the container as  <code>/run/secrets/passwords.json</code> and for every key/value in that secret, a file will be created with the name of the <code>key</code> and contents of <code>value</code>.</p> <p>Example of /run/secrets/passwords.json transformed into files</p> <p><pre><code>{\n  \"root-user-password\": \"secret-root-password\",\n  \"admin-password\": \"secret-admin-password\"\n}\n</code></pre> creates the files <pre><code>    File: /run/secrets/secret-root-password\nContents: secret-root-password\n\n    File: /run/secrets/secret-admin-password\nContents: secret-admin-password\n</code></pre></p>"},{"location":"how-to/usingVault/#native-devops-hashicorp-support","title":"Native DevOps HashiCorp Support","text":"<p>Vault secrets can also be used in native PingIdentity DevOps images regardless of the environment they are deployed in, for example, Kubernetes, Docker, and Docker-compose.  In these cases, there is no injector agent required.</p> <p>This does require some type of AuthN to your vault, such as USERNAME/PASSWORD or TOKEN.  HashiCorp Injector method is recommended.</p> <p>The following image depicts the components and steps for pulling secrets into a container at start-up.</p> <p></p> <p>You can use the following variables to deploy images that will pull secrets from the Vault.</p> Variable Example Description SECRETS_DIR /run/secrets Location for storing secrets.  See section below on using a <code>tmpfs</code> mounted filesystem to store secrets in a memory location. VAULT_TYPE hashicorp Type of vault used. Currently supporting hashicorp. VAULT_ADDR https://vault.example.com:8200 URL for the vault with secrets VAULT_TOKEN s.gvC3vd5aFz......JovV0b0A Active token used to authticate/authorize container to vault.  Optional if VAULT_AUTH_USERNAME/VAULT_AUTH_PASSWORD are provided. VAULT_AUTH_USERNAME demo Username of internal vault identity. Optional if VAULT_TOKEN is provided. VAULT_AUTH_PASSWORD 2FederateM0re Password of internal vault identity. Optional if VAULT_TOKEN is provided. VAULT_SECRETS /pingfederate/encryption-keys A list of secrets to pull into the container.  Must be the full secret path used in vault. <p>The following example shows how these would be used in a docker-compose.yaml file.  This example provides two secrets, as denoted by the VAULT_SECRETS setting.</p> <pre><code>services:\n  pingfederate:\n    image: pingidentity/pingfederate:edge\n    environment:\n      ...\n      ################################################\n      # Vault Info\n      ################################################\n      - VAULT_TYPE=hashicorp\n      - VAULT_ADDR=https://vault.ping-devops.com:8200\n      - VAULT_AUTH_USERNAME=demo\n      - VAULT_AUTH_PASSWORD=2FederateM0re\n      - VAULT_SECRETS=/demo/passwords\n                      /demo/getting-started/pingfederated/pf-keys\n</code></pre> <p>The secret types (Variables/Files) are processed the same way as with the HashiCorp Injector Method above.</p>"},{"location":"how-to/usingVault/#secrets-base64","title":"Secrets - Base64","text":"<p>Often, there are secrets that might be of a binary format, such as certificates.</p> <p>Special key name suffixes can be used to perform certain processing on the keys when the file is created.  The following table provides examples of how keys with special suffixes.</p> Key Suffix Description .b64 or .base64 Specifies that the value is base64 encoded and the resulting file should be decoded when written, without the suffix. <p>There is a message that is base64 encoded and stored in the vault as secret <code>/demo/b64-demo</code> and key <code>hello.b64</code></p> <pre><code>Secret: /demo/b64-demo\n\n  KEY         VALUE\n  ---         -----\n  hello.b64   SGVsbG8gV29ybGQhCg==\n</code></pre> <p>would result in the following file:</p> <pre><code>/run/secrets/hello\n  CONTENTS\n  --------\n  Hello World!\n</code></pre>"},{"location":"how-to/usingVault/#using-tmpfs-for-secrets","title":"Using tmpfs for Secrets","text":"<p>It's best practice to place secrets in a volume that won't be persisted to storage with the possibility that it might be improperly accessed at any point in the future, such as backups and environment variables.</p> <p>Kubernetes automatically provides the default <code>SECRETS_DIR</code> of <code>/run/secrets</code> for this.</p> <p>If using Docker, you should create a <code>tmpfs</code> type volume and size it to <code>32m</code> and mount it to a path of <code>/run/secrets</code>.</p> <p>Docker-compose version &gt; 2.4</p> <p>Requires Docker-compose version 2.4 or later, due to the options provided to the tmpfs volumes definition</p> <p>Creates a <code>/run/secrets</code> volume under tmpfs</p> <pre><code>version: \"2.4\"\n\nservices:\n  pingfederate:\n    image: pingidentity/pingfederate:edge\n    environment:\n    ...\n    tmpfs: /run/secrets\n\n      ---- or -----\n\n    volumes:\n      - type: tmpfs\n        target: /run/secrets\n        tmpfs:\n          size: 32m\n</code></pre> <p>See this mount by exec'ing into the container and running:</p> <pre><code>&gt; df -k /run/secrets\nFilesystem           1K-blocks      Used Available Use% Mounted on\ntmpfs                    16384         0     16384   0% /run/secrets\n</code></pre>"},{"location":"reference/HelmBasics/","title":"Helm Basics","text":"<p>Although this document cannot cover the depths of this tool, new Helm users might find other technical documentation too involved for the purpose of beginning use of Ping Identity container images. This document aims to equip new users with helpful terminology in simple terms, with a focus on relevant commands. For more in-depth documentation around Helm, check out helm.sh.</p> <p>Note</p> <p>This overview uses Ping Identity images and practices as a guide, but generally applies to any interactions using Helm with Kubernetes. With these assumptions, this document might feel incomplete or inaccurate to veterans. If you would like to contribute to this document, feel free to open a pull request!</p>"},{"location":"reference/HelmBasics/#helm","title":"Helm","text":"<p>Ping Identity DevOps and Helm</p> <p>All of our instructions and examples are based on the Ping Identity DevOps Helm chart. If you are not using the Ping Identity DevOps Helm chart in production, we still recommend using it to generate your direct Kubernetes manifest files. Using our provided chart to create your files in this manner gives Ping Identity the best opportunity to support your environment.</p> <p>Everything in Kubernetes is deployed by defining what you want and allowing Kubernetes to achieve the desired state (the declarative model).</p> <p>Helm simplifies your interaction by building deployment patterns into templates with variables. The Ping Identity Helm chart includes Kubernetes templates and default values maintained by Ping Identity. With these in hand, you only need to provide values for the template variables to match your environment.</p> <p>For example, a service definition looks like the following file. With this file, Kubernetes is instructed to create a <code>service</code> resource with the name <code>myping-pingdirectory</code>.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/instance: myping\n    app.kubernetes.io/name: pingdirectory\n  name: myping-pingdirectory\nspec:\n  ports:\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: 1443\n  - name: ldap\n    port: 389\n    protocol: TCP\n    targetPort: 1389\n  - name: ldaps\n    port: 636\n    protocol: TCP\n    targetPort: 1636\n  selector:\n    app.kubernetes.io/instance: myping\n    app.kubernetes.io/name: pingdirectory\n  type: ClusterIP\n</code></pre> <p>Using our Helm chart, you can automatically define this entire resource and all other required resources for a basic deployment by setting <code>pingdirectory.enabled=true</code>.</p>"},{"location":"reference/HelmBasics/#terminology","title":"Terminology","text":"<p>Manifests - the final Kubernetes YAML files that are sent to the cluster for resource creation. These files are standard Kubernetes files and will be similar to the service example shown above.</p> <p>Helm Templates - Go Template versions of Kubernetes YAML files. These templates enable the manifest creation to be parameterized.</p> <p>Values and values.yaml - A value is the setting you pass to a Helm chart from which the templates produce the manifests you want. Values can be passed individually on the command line, but more commonly they are collected and defined in a file named values.yaml. For example, if this file contained only this entry, the resulting Kubernetes manifest file would be over 200 lines long.</p> <pre><code>pingdirectory:\n  enabled: true\n</code></pre> <p>release - When you deploy resources with Helm, you provide a name for identification. The combination of this name and the resources that are deployed using it make up a <code>release</code>. When using Helm, it is a common pattern to prefix all resources managed by a release with the release name. In our examples, <code>myping</code> is the release name, so you will see products in Kubernetes running with names similar to <code>myping-pingfederate-admin</code>, <code>myping-pingdirectory</code>, and <code>myping-pingauthorize</code>.</p>"},{"location":"reference/HelmBasics/#building-the-helm-values-file","title":"Building the Helm Values File","text":"<p>This documentation focuses on the Ping Identity DevOps Helm chart and the values passed to the Helm chart to achieve your configuration. For your deployment to fit your goals, you must create a values.yaml file.</p> <p>The most simple values.yaml for our Helm chart would be:</p> <pre><code>global:\n  enabled: true\n</code></pre> <p>By default, this flag is set as <code>global.enabled=false</code>. These two lines are sufficient to turn on (deploy) every available Ping Identity software product with a basic configuration.</p>"},{"location":"reference/HelmBasics/#providing-your-own-server-profile","title":"Providing your own server profile","text":"<p>In the documentation, there is an example for providing your own server profile stored in GitHub to PingDirectory.  The documenation provides this snippet in the values.yaml specific to that feature:</p> <pre><code>pingdirectory:\n  envs:\n    SERVER_PROFILE_URL: https://github.com/&lt;your-github-user&gt;/pingidentity-server-profiles\n</code></pre> <p>This entry alone will not turn on PingDirectory, because the default value for <code>pingdirectory.enabled</code> is false. To complete the deployment, add the snippet to turn deploy and configure PingDirectory in the values.yaml file:</p> <pre><code>global:\n  enabled: true\npingdirectory:\n  envs:\n    SERVER_PROFILE_URL: https://github.com/&lt;your-github-user&gt;/pingidentity-server-profiles\n</code></pre> <p>This example snippet turns on all products, including PingDirectory, and overwrites the default <code>pingdirectory.envs.SERVER_PROFILE_URL</code> with <code>https://github.com/&lt;your-github-user&gt;/pingidentity-server-profiles</code>.</p> <p>This use of substitution and parameters is where the power of Helm to simplify ease of deployment begins to shine. To fully customize your deployment, review all available options by running:</p> <pre><code>helm show values pingidentity/ping-devops\n</code></pre> <p>This command prints all of the default values applied to your deployment. To overwrite any default values from the chart, copy the corresponding snippet and include it in your own values.yaml file with any modifications needed. Remember with YAML that tabbing and spacing matters. For most editors, copying all the way to the left margin and pasting at the very beginning of a line in your file should maintain proper indentation.</p> <p>Helm also provides a wide variety of plugins. A useful one is Helm diff.  This plugin shows what changes would be applied between Helm upgrade commands. For example, if this plugin indicates anything in a Deployment or Statefulset would change, you can expect the corresponding pods to be cycled. In this example, Helm diff is useful to note changes that would occur, particularly when you are not prepared for containers to be restarted.</p>"},{"location":"reference/HelmBasics/#additional-commands","title":"Additional Commands","text":"<p>As you go through the Helm examples, the goal is to build a values.yaml file that works in your environment.</p>"},{"location":"reference/HelmBasics/#deploy-a-release","title":"Deploy a release:","text":"<pre><code>helm upgrade --install &lt;release_name&gt; pingidentity/ping-devops -f /path/to/values.yaml\n</code></pre>"},{"location":"reference/HelmBasics/#delete-a-release","title":"Delete a release:","text":"<p>This command will remove all resources except PVC and PV objects associated with the release from the cluster:</p> <pre><code>helm uninstall &lt;release name&gt;\n</code></pre>"},{"location":"reference/HelmBasics/#delete-pvcs-associated-to-a-release","title":"Delete PVCs associated to a release:","text":"<pre><code>kubectl delete pvc --selector=app.kubernetes.io/instance=&lt;release_name&gt;\n</code></pre>"},{"location":"reference/HelmBasics/#exit-codes","title":"Exit Codes","text":"Exit Code Description Exit Code 0 Absence of an attached foreground process Exit Code 1 Indicates failure due to application error Exit Code 137 Indicates failure as container received SIGKILL (manual intervention or \u2018oom-killer\u2019 [OUT-OF-MEMORY]) Exit Code 139 Indicates failure as container received SIGSEGV Exit Code 143 Indicates failure as a container received SIGTERM"},{"location":"reference/HelmBasics/#example-configurations","title":"Example Configurations","text":"<p>The Helm examples page contains example configurations for running and configuring Ping products using the Ping Devops Helm Chart. Please review the Getting Started Page before trying them.</p>"},{"location":"reference/awsStorage/","title":"AWS Storage Considerations","text":"<p>AWS provides many storage options. When considering Ping products deployed in a containerized deployment, the choice typically comes down to two: elastic block storage (EBS) and elastic file system (EFS).  Though there are a number of differences between them, on the surface they act similar when attached to an Elastic Kubernetes Service (EKS) node or Elastic Compute Cloud (EC2) instance.</p> <p>However, Ping products (whether containerized or not) require high I/O performance, and Ping only recommends EBS volumes as the backing store.  EFS performance is significantly lower and is not supported.</p> <p>For additional product-specific requirements, visit the appropriate product page.</p>"},{"location":"reference/config/","title":"Image/Container Components and Configuration","text":""},{"location":"reference/config/#container-data-flows-and-running-state","title":"Container data flows and running state","text":"<p>The diagram below shows the topology of a container with flows of data into the container and how it transitions to the eventual running state.</p> <p></p> Data Class Default Location Use Description VAULT ext Secret information from external Vault (i.e. HashiCorp Vault).  Items like passwords, certificates, keys, etc... ORCH ext Environment variables from secrets, configmaps and/or env/envfile resources from orchestration (i.e. docker, k8s. SERVER PROFILE ext Product server profile from either an external repository (i.e. git) or external volume (i.e. aws s3). SERVER BITS /opt/server ro Uncompressed copy of the product software.  Provided by image. SECRETS /run/secrets ro Read Only secrets residing on non-persistent storage (i.e. /run/secrets). IN /opt/in ro Volume intended to receive all incoming server-profile information. ENV /opt/staging/.env mem Environment variable settings used by hooks and product to configure container. STAGING /opt/staging tmp Temporary space used to prepare configuration and store variable settings before being moved to OUT OUT /opt/out rw Combo of product bits/configuration resulting in running container configuration. PERSISTENT VOLUME rw Persistent location of product bits/configuration in external storage (i.e. AWS EBS) <p>Because of these many factors affecting how an image is deployed, the configuration options for use of the elements in the previous table can vary greatly, depending on factors such as:</p> <ul> <li>Deployment Environment - Kubernetes, Cloud Vendor, Local Docker</li> <li>CI/CD Tools - Kubectl, Helm, Kustomize, Terraform</li> <li>Source Maintenance - Git, Cloud Vendor Volumes</li> <li>Customer Environment - Development, Test, QA, Stage, Prod</li> <li>Security - Test/QA/Production Data, Secrets, Certificates, Secret Management Tools</li> </ul>"},{"location":"reference/config/#examples","title":"Examples","text":""},{"location":"reference/config/#file-flowchart-example","title":"File flowchart example","text":"<p>The following diagram shows how files can enter and flow through the container:</p> <p></p> <p>There is a video that goes through the above image in more detail here.</p>"},{"location":"reference/config/#production-example","title":"Production Example","text":"<p>The following diagram shows an example in a high-level production scenario in an Amazon Web Services (AWS) EKS environment, where:</p> <ul> <li>HashiCorp Vault is used to provide secrets to the container.</li> <li>Helm is used to create k8s resources and deploy them.</li> <li>AWS EBS volumes is used to persist the state of the container.</li> </ul> <p></p>"},{"location":"reference/config/#development-example","title":"Development Example","text":"<p>The following diagram shows an example in a high-level development scenario in an Azure AKS environment, where:</p> <ul> <li>No secrets management is used.</li> <li>Simple kubectl is used to deploy k8s resources.</li> <li>AWS EBS volumes is used to persist the state of the container.</li> </ul> <p></p>"},{"location":"reference/config/#customizing-the-containers","title":"Customizing the Containers","text":"<p>You can customize our product containers by:</p> <ul> <li> <p>Customizing server profiles</p> <p>The server profiles supply configuration, data, and environment information to the product containers at startup. You can use our server profiles as-is or as a baseline for creating your own.</p> <p>You can find these profiles in Baseline server profiles in our pingidentity-server-profiles repository.</p> </li> <li> <p>Environment substitution</p> <p>You can deploy configurations in multiple environments with minimal changes by removing literal values and replacing them with environment variables.</p> </li> <li> <p>Using DevOps hooks</p> <p>Hooks are shell scripts used to automate operations during the lifecycle of a product container.  These hook scripts are built into our images: some are common across all products, while others are product-specific. For more information about the repository that houses these scripts, visit the docker-builds repository overview.</p> </li> <li> <p>Using release tags</p> <p>We use sets of tags for each released build image. These tags identify whether the image is a specific stable release, the latest stable release, or current (potentially unstable) builds. You can find the release tag information in Docker images.</p> <p>You can try different tags in either the standalone startup scripts for the deployment examples or the YAML files for the orchestrated deployment examples.</p> </li> <li> <p>Adding a message of the day (MOTD)</p> <p>You can use a <code>motd.json</code> file to add message of the day information for inclusion in the images.</p> </li> </ul>"},{"location":"reference/containerLogging/","title":"Container Logging","text":"<p>This document provides an outline of how logging is handled in containerized environments.  Please refer to the provided links at the end of this page for details on implementing a logging solution for your deployments.</p> <p>Splunk Example</p> <p>While providing examples for all logging solutions is impractical, there is an example for using Splunk on this portal here.</p>"},{"location":"reference/containerLogging/#problem-statement","title":"Problem statement","text":"<p>In a containerized deployment model, it is expected that containers (or pods under Kubernetes) will be ephemeral.  Further, the standard practice for application logging in a container is to use <code>stdout</code> and, in some cases, <code>stderr</code> as the means of streaming logs. Ping product containers follow this practice. As a result, no logs will persist outside the lifecycle of the container or pod.  In particular, if a pod is failing or in crashloop due to a misconfiguration or error, it is impossible to troubleshoot the cause as the logs that might provide information on the crash are lost each time the pod attempts to restart. It is important, then, to insure that logs are stored external to the container.</p> <p>In Case You Missed It</p> <p>If a container is stopped for any reason, including crashes, all logging information from the container that is not stored elsewhere is lost.</p>"},{"location":"reference/containerLogging/#viewing-logs","title":"Viewing logs","text":"<p>In a Kubernetes deployment, you can view the streaming logs (stdout/stderr) of a container in a pod by issuing the <code>kubectl logs</code> command.  This function is useful for quickly examining logs from operational containers.</p>"},{"location":"reference/containerLogging/#persisting-logs","title":"Persisting logs","text":"<p>Because you cannot rely on the logs from the container itself for long-term use, you must implement some means of storing the logging information apart from the container itself.</p>"},{"location":"reference/containerLogging/#logging-sidecar","title":"Logging sidecar","text":"<p>In the Kubernetes model, a common method of maintaining logs is to use the sidecar model. A logging sidecar is included in the pod and configured to grab the stdout/stderr streams from the application container and persist them to the logging service. Many vendors provide a Docker image for this sidecar that contains the agent for their product. In addition, they usually provide support for configuring the container to connect to their service and format the logs for consumption, such as through environment variables, Kubernetes ConfigMaps, or other means.</p> <p>Advantages:</p> <ul> <li>Logs can be sent to different locations at the same time using multiple sidecars</li> <li>Access to the cluster node is not required - particularly useful for hosted Kubernetes environments</li> <li>No update to the application is required, assuming it dumps logging information to stdout/stderr</li> </ul> <p>Disadvantage:</p> <ul> <li>Additional resources are required for running the extra container(s), though they tend to be lightweight</li> </ul>"},{"location":"reference/containerLogging/#the-tail_log_files-environment-variable","title":"The TAIL_LOG_FILES environment variable","text":"<p>Many Ping products were designed and built for a server-deployed implementation.  As a result, they write log information to files (the old model for logging), rather than to <code>stdout</code>.  To ease containerization, an environment variable (<code>TAIL_LOG_FILES</code>) is included in the Docker images and this variable is fed to a function that streams these files to <code>stdout</code> as they are written.</p> <p>While Ping includes key log files as defaults, this variable can be modified.  You can add additional log files to this variable to include them in the <code>stdout</code> stream.  See each product Dockerfile for the default value of this variable for the product in question.</p>"},{"location":"reference/containerLogging/#references","title":"References","text":"<p>The list below is not intended to be comprehensive but should provide a good starting point for understanding how logging works and what you can do to retain logs from your deployments.  </p> <p>Examples only</p> <p>Any vendor listed here should not be considered an endorsement or recommendation by Ping Identity for that service. Refer to the documentation for the image in question for further assistance.</p> <ul> <li>Kubernetes Logging Documentation</li> <li>Docker Logging Documentation</li> <li>Docker Hub images, listed alphabetically:<ul> <li>AWS Cloudwatch</li> <li>Datadog</li> <li>Fluentd</li> <li>Graylog</li> <li>Rsyslog</li> <li>Sematext</li> <li>Splunk Forwarder</li> <li>Sumologic</li> </ul> </li> </ul>"},{"location":"reference/faqs/","title":"Frequently asked questions","text":""},{"location":"reference/faqs/#aws","title":"AWS","text":"What storage option should I use for container volumes on EKS?  Ping recommends the use of EBS volumes for container volumes on EKS.  EFS is not supported. For more information, please visit AWS Storage Considerations."},{"location":"reference/faqs/#docker-images","title":"Docker Images","text":"What OS and Java versions are included in Ping Docker images?  The operating system (OS) shims used for our images are Alpine and Red Hat UBI.  The UBI-based images are intended for Openshift deployments, while Alpine should be used in most other situations. For more information on the choice of Alpine, please visit Supported OS Shim.  The Java version currently included in our images is OpenJDK 11 and the distribution used is BellSoft Liberica. When are new Ping product Docker images released?  Typically, Docker images are released on a monthly basis during the first full week of the month.  The images are tagged YYMM, with the month indicating the complete month prior.  So, tag \"2303\", representing the work from March 2023, would be released in early April.  As we mature our processes, the frequency and timing of these images will more closely align with product releases.  How can I be informed when new images are available?  You can watch the docker-builds GitHub repository for the Ping Identity product line. Select the \"custom\" option to receive notification when a release occurs.  Releases in the docker-builds repository correspond to the publishing of images in Docker Hub.  What are the latest Ping product versions available as Docker images?  The latest Ping product images are tagged with {RELEASE}-{PRODUCT VERSION}. You can find more information about our latest product images by consulting the Product Version matrix.  Do the images come as product only or combined with an OS layer?  The DevOps program uses Alpine as its base OS shim for all images. For more information please visit Supported OS Shim.  I have created a custom product installation. If we require a specific image, can that be supplied by Ping?   We do not provide custom images, but you are welcome to build the image locally with your customized bits. For more information, see Build Local Images.  It is important to note using a custom image might affect support options and timing."},{"location":"reference/faqs/#container-operations","title":"Container Operations","text":"How do files move around when the container starts up?  To find out how our files are moved at start up, please visit File Flowchart.  How do I turn off the calls to the Message of the Day (MOTD)?  Set the environment variable in PingBase to: MOTD_URL=\"\" <p>For more information about the PingBase environment variables, please visit PingBase.</p> How do I get more verbosity in log outputs?  Set the environment variables in PingBase to: VERBOSE=\u201ctrue\u201d <p>For more information about the PingBase environment variables, please visit PingBase</p>"},{"location":"reference/faqs/#orchestration-helm-kubernetes","title":"Orchestration / Helm / Kubernetes","text":"How can I be informed when a new release of the Helm charts are available?  You can watch the Ping helm-charts GitHub repository. Select the \"custom\" option to receive notification when a release occurs.  As with the product Docker images, the Helm charts are usually updated once a month.  Kubernetes has dropped direct integration support for Docker. Does this change impact Ping product containers? <p>No. The underlying container runtime has not caused problems with our images.  Please let us know if you encounter errors.  The CRI-O and containerd runtimes have been tested without any known issues.</p>  For more background: \u2003The Kubernetes blog post on Docker removal is here. \u2003An excellent write up of how it looks is on this  page. My container environment is not allowed to make any external calls to services such as Github or Docker  Hub. Can I still use Ping Identity containers?  <p>Yes. This practice is common in production scenarios. To use Ping Identity containers in this situation:</p> \u20031. Use an Existing License. \u20032. Use an empty remote profile SERVER_PROFILE_URL=\"\".  Optionally, you can build your profile into the image, visit Server Profiles for more information. \u20033. Turn off license verification with MUTE_LICENSE_VERIFICATION=\"true\". \u20034. Turn off calls to the Message of the Day (MOTD) with MOTD_URL=\"\". How do we run the console and engines in a container environment?  The helm chart supports instantiating both consoles and engines.  Ingress to the consoles would have to be laid out for UI access. <p>For more information about the Ping's Helm Charts, please visit Ping Helm</p> Can I use Podman instead of Docker?  Yes, just like Docker, you will be able to use Podman for container orchestration.  Why does Ping recommand K8s vs docker? \u20031. Docker or a pure container solution like ECS by itself is generally not as robust or resilient as a K8s environment. While managed Docker services like ECS provide some of the functionality of Kubernetes, you are locked into that provider and you would have a different experience at Google, Azure, or another cloud provider. Kubernetes, even managed services like EKS, provides more flexibility and portability. \u20032. It is the model we use for our SaaS offerings, so internal teams at Ping are more familiar with this model. \u20033. Orchestration among multiple applications and services is native to Kubernetes, a bit of an add-on with Container-only services. \u20034. Workload management using Kubernetes native objects, such as Horizontal Pod Autoscaling, Node scaling and so on. \u20035. Management through Infrastructure-as-Code principles using Helm Charts and Values files."},{"location":"reference/faqs/#configuration-and-server-profile","title":"Configuration and Server Profile","text":"How do I customize a container?  There are many ways to customize the container for a Ping product. For example, you can create a customized server profile to save a configuration. <p>To find more ways on how to customize a container, see Customizing Containers.</p> How do I save product configurations?  In order to save configurations, create a server profile and store in a server profile repository.  This repository can be used to pass the configuration into the runtime environment. For help with creating a custom server profile, visit Server Profiles. <p></p> <p>Examples of how to get the profile data from the different products:</p> PingFederate Profile <pre><code>curl -k https://localhost:9999/pf-admin-api/v1/bulk/export?includeExternalResources=false \\\n-u administrator:2FederateM0re \\\n-H 'X-XSRF-Header: PingFederate' \\\n-o data.json\n</code></pre> \u2003 PingAccess Profile <pre><code>curl -k https://localhost:9000/pa-admin-api/v3/config/export \\\n-u administrator:2FederateM0re \\\n-H \"X-XSRF-Header: PingAccess\" \\\n-o data.json\n</code></pre> PingDirectory Profile <pre><code>kubectl exec -it pingdirectory-0 \\\n-- manage-profile generate-profile \\\n--profileRoot /tmp/pd.profile\n</code></pre> What should be in my server profile?  For more information about what information should be in the server profile consist, please visit Container Anatomy and Profile Structures.  Does my server profile have to be hosted on Github?  No, it can be any Public or Private git repository. <p>You are also able to use a Local Directory as your repository, which is convenient for testing and development.</p>"},{"location":"reference/faqs/#product-related","title":"Product related","text":"How do I access various product consoles?  For a Helm-deployed stack, there are two basic ways you can access the consoles.  <p>1. PortForward to the pod to access with localhost.</p> <p> kubectl port-forward &lt;podName&gt; &lt;containerPort&gt;:&lt;localPort&gt;</p> 2. Using Helm, add the ingress definition in the yaml file in order to access the container with a URL. See Creating Ingresses. You must have an ingress controller in your cluster for the ingress to work.  How do I use an existing license?  You can mount the license in the container's opt/in directory. Please see using existing licenses for more information.  Where do I get a license?  How do I obtain a trial license? <p></p> The DevOps team at Ping is not responsible for issuing supported product licenses.  We provide a temporary license through the DevOps program. After signing up, you can use the provided credentials to get a short-term license to use in evaluating Ping products running in containers. <p></p> If you want to use Ping products in production environments, you are required to purchase a valid license. Contact our sales department for more information.  How do I turn off the license verification?  Set the environment variable in PingBase to: MUTE_LICENSE_VERIFICATION=\"true\" <p>For more information about the PingBase environment variables, please visit PingBase.</p>"},{"location":"reference/faqs/#troubleshoot","title":"Troubleshoot","text":"How do I run Collect-Support-Data in the devops environment?  You will need to modify the liveness probe to always exit 0 and the readiness probe to always exit 1. These changes will give you enough time to capture the CSD without it crashing or trying to serve live traffic. <p>For more information about the Collect-Support-Data, please visit CSD.</p> How much overhead memory and CPU is needed to run the Collect-Support-Data tool?  By default, this value is set to 1GB. You would need to add additional memory (1GB to 2GB) to the heap for the server. In terms of CPU, the CSD uses whatever is available. <p>For more information about the Collect-Support-Data, please visit CSD.</p>"},{"location":"reference/hooks/","title":"Using DevOps hooks","text":"<p>Video exploration</p> <p>For more information on hook scripts, see the Ping product image hook script exploration video.</p> <p>Our DevOps hooks are build-specific scripts that are called, or can be called, by the <code>entrypoint.sh</code> script used to start our product containers.</p> <p>Advanced usage</p> <p>Use of the hook scripts is intended only for DevOps professionals familiar with the products.</p> <p>The available hooks are built into the product images and can be found in the <code>hooks</code> subdirectory of each product directory in the Docker Builds repository.</p> <p>In the <code>entrypoint.sh</code> startup script, there is an example (stub) provided for the available hooks for all products.</p> <p>Warning</p> <p>It is critical that the supplied hook names be used if you modify <code>entrypoint.sh</code>. For example, they can be used to make subtle changes to a server profile.</p>"},{"location":"reference/hooks/#using-pre-and-post-hooks","title":"Using .pre and .post hooks","text":"<p>When the hook scripts are called during the <code>entrypoint.sh</code> initialization, any corresponding <code>.pre</code> and <code>.post</code> hooks are also called.</p> <p>The <code>.pre</code> and <code>.post</code> extensions allow you to define custom scripts to be executed before or after any hook that is run in the container. You can include any custom <code>.pre</code> and <code>.post</code> hooks in the <code>hooks</code> directory of your server profile.</p> <p>Hooks with a <code>.pre</code> extension are run before the corresponding hook, and hooks with a <code>.post</code> extension are run after the corresponding hook.</p> <p>For example, a script named <code>80-post-start.sh.pre</code> will execute immediately before the <code>80-post-start.sh</code> hook and a script named <code>80-post-start.sh.post</code> will be run immediately after that hook completes.</p>"},{"location":"reference/k8sBasics/","title":"Kubernetes Basics","text":"<p>Although this document cannot cover all aspects of these tools, new Kubernetes users might find other technical documentation too involved for purposes of using Ping Identity images. This document aims to equip new users with helpful terminology in simple terms, with a focus on relevant commands.</p> <p>Note</p> <p>This overview uses Ping Identity images and practices as a guide, but generally applies to any interactions in Kubernetes. With these assumptions, this document might feel incomplete or inaccurate to veterans. If you would like to contribute to this document, feel free to open a pull request!</p>"},{"location":"reference/k8sBasics/#kubernetes","title":"Kubernetes","text":""},{"location":"reference/k8sBasics/#terms","title":"Terms","text":"<p>Cluster - The ice cube tray</p> <p>You can consider a Kubernetes cluster as a set of resources into which you can deploy containers. A cluster can be as small as your local computer or as large as hundreds of virtual machines (VMs), called nodes, in a data center. Interaction with the cluster is through an API requiring authentication and role-based access control (RBAC) that allows the actions necessary within the cluster.</p> <p>In a cloud provider Kubernetes cluster, such as Amazon Web Services (AWS) EKS, Azure AKS, or Google GKE, the cluster can span multiple Availability Zones (AZs), but only one region. In AWS terms, a cluster can be in the region us-west-2 but have nodes in the AZs us-west-2a, us-west-2b, and us-west-2c. Kubernetes provides high availability by distributing applications with multiple instances of containers, called replicas, across available AZs.</p> <p>Nodes - The individual ice cube spaces in the tray</p> <p>The nodes are the pieces that provide allocatable resources, such as CPU and memory, and make up a cluster. Typically, these are VMs, and for example in AWS, they would be EC2 instances.</p> <p>Namespace - A loosely defined slice of the cluster</p> <p>Namespaces are intended to be a virtual delimiter for deploying grouped applications.  While it is possible for pods to communicate across namespaces, policies can be put in place with third-party services to prevent this communication.</p> <p>Note</p> <p>You can allocate resource limits available to a namespace, but this is not required.</p> <p>Context - A definition in your ~/.kube/config file that specifies the cluster and namespace where your <code>kubectl</code> commands will be executed.</p> <p>Deployments and Statefulsets - The water that fills ice cube spots.</p> <p>Applications are deployed as Deployments or Statefulsets. You can consider both of these objects as controllers that define and manage the following:</p> <ul> <li>Name of an application</li> <li>Number of instances (pods) of an application (replicas)</li> <li>Persistent storage</li> </ul> <p>Though they are similar, Deployments differ from Statefulsets in a few fundamental ways.</p>"},{"location":"reference/k8sBasics/#deployments","title":"Deployments","text":"<ul> <li>Deployments are typically used for stateless applications - if a pod is lost or removed, any other pod in the same deployment can take on the activity the lost pod was performing.</li> <li>Pod names are inconsequential because each pod is identical with no state information required.  As a result, the name of the pod does not matter and names are suffixed with a randomly generated string.</li> <li>The order in which pods are started is also inconsequential.  When starting a deployment all pods are launched at the same time.  </li> <li>When updating a deployment, you can cycle one, many, or all pods at the same time.</li> </ul>"},{"location":"reference/k8sBasics/#statefulsets","title":"Statefulsets","text":"<p>StatefulSets are more structured in the manner in which the pods are handled.  </p> <ul> <li>StatefulSets - as the name implies - are used for applications in which a known state is required.  For example, many clustered products have an instance in the cluster that is considered the leader and all pods in the set need to know which pod is acting in this capacity.  A controlled scale-up and scale-down process is needed to maintain known state as application nodes or instances join or leave the cluster or are restarted.  </li> <li>Pod names are sticky in that each pod in the StatefulSet has a known name, with each pod receving an ordinal indicator (unlike the random pod name found in Deployments).  For example, a StatefulSet will have pod names similar to:  <code>myping-pingdirectory-0</code>, <code>myping-pingdirectory-1</code>, and <code>myping-pingdirectory-2</code></li> <li>Controlled startup with health priority: unlike a Deployment, a StatefulSet deploys the first instance (pod name appended with -0) and waits for it to be healthy before adding another to the group.</li> <li>Updates occur to instances in a rolling fashion, one-at-a-time, starting with the most recent pod (e.g., <code>myping-pingdirectory-2</code>) first.</li> <li>With a known Pod name, persistent storage can be maintained for each pod.  After persistent storage is created and assigned, the same storage object is provided to the same-named pod every time.</li> </ul> <p>Pod - The molecules that make up the water</p> <p>A Deployment/StatefulSet specifies the number of pods to run for a given application. For example, you can have a <code>pingfederate-engine</code> deployment that calls for three replicas with two CPUs and 2 GB of memory, but you cannot make one engine larger or smaller than the others.</p> <p>Like a molecule, a pod can consist of just one container, or it can have multiple containers, called sidecars. For example, your pod can have a PingFederate container as the main process and a sidecar container, such as Splunk Universal Forwarder, to export logs. All containers in a pod, including these sidecars, share a namespace and IP address.</p> <p>Pods are are considered disposable and by default do not persist any data.  To maintain state or data, external storage or a database of some kind is needed.</p> <p>PersistentVolume (PV) and PersistentVolumeClaim (PVC) - A virtual external storage device or definition attached to a Pod</p> <p>The PV is the storage object and PVC is the claim that a given pod makes for that storage. </p> <p>Service - A slim loadbalancer within the cluster</p> <p>Pods can come and go, be disposed of or restarted.  Every time a Pod is started, it will receive an IP address which often changes. In order to access the application hosted in the Pod, a fixed, known location or address is required.</p> <p>Services provide a single IP address and cluster-internal DNS resolution that is placed in front of Deployments and Statefulsets to distribute traffic. For service-to-service communication, such as PingFederate using PingDirectory as a user store, the application should be configured to point to a service name and port rather than the individual pods. Services are given fully-qualified domain names (FQDNs) in a cluster. Within the same namespace, services are accessible by their name (<code>https://myping-pingdirectory:443</code>), but across namespaces, you must be more explicit (<code>https://myping-pingdirectory.&lt;namespace&gt;:443</code>). A FQDN would be <code>https://myping-pingdirectory.&lt;namespace&gt;.svc.cluster.local</code>.</p> <p>Ingress - A network definition used to expose an application external to the cluster. In order for an ingress to work, you need an Ingress Controller.</p> <p>A common pattern is a deployment of Nginx pods fronted by a physical loadbalancer. The client application traffic hits the loadbalancer first, then is forwarded to Nginx. The header information (hostname and path) of the request is evaluated and forwarded to a corresponding application service in the cluster.</p> <p>For example, suppose a PingFederate ingress has a host name of myping-pingfederate-engine.pingdemo.example. If a client application makes a request to <code>https://myping-pingfederate-engine.pingdemo.example/pf/heartbeat.ping</code>, the traffic flow of the request would be:</p> <ul> <li>Client -&gt; LoadBalancer -&gt; Nginx k8s Service -&gt; Nginx Pod -&gt; Pingfederate-engine k8s Service -&gt; Pingfederate-engine pod</li> </ul>"},{"location":"reference/k8sBasics/#commands","title":"Commands","text":"<p>To see which cluster and namespace you are using, use the kubectx tool.</p> <p>Alternatively, you can run the following commands:</p> <pre><code># Retrieve and set context\nkubectl config get-contexts\nkubectl config current-context\nkubectl config use-context my-cluster-name\n\n# Set Namespace\nkubectl config set-context --current --namespace=&lt;namespace&gt;\n</code></pre>"},{"location":"reference/k8sBasics/#viewing-resources","title":"Viewing resources","text":"<p>You can use k9s, which is a UI designed to run in a terminal.</p> <p>If you cannot use k9s, review the standard commands here.</p> <p>You can run <code>kubectl get</code> for any resource type, such as Pods, Deployments, Statefulsets, and PVCs. Many resources have short names:</p> <ul> <li><code>po</code> = pods</li> <li><code>deploy</code> = Deployments</li> <li><code>sts</code> = Statefulsets</li> <li><code>ing</code> = ingresses</li> <li><code>pvc</code> = persistentvolumeclaims</li> </ul> <p>The most common command is <code>get pods</code>:</p> <pre><code>kubectl get pods\n</code></pre> <p>To show anything that the container prints to <code>stdout</code>, use <code>logs</code>:</p> <pre><code>kubectl logs -f &lt;pod-name&gt;\n</code></pre> <p>To show the logs of a pod with multiple containers, specify the container for which you wish to view logs with the <code>-c</code> option:</p> <pre><code>kubectl logs -f &lt;pod-name&gt; -c &lt;container-name&gt;\n</code></pre> <p>To show the logs of a crashed pod (<code>RESTARTS != 0</code>):</p> <pre><code>kubectl logs -f &lt;pod-name&gt; --previous\n</code></pre> <p>To see available host names by ingress:</p> <pre><code>kubectl get ing\n</code></pre>"},{"location":"reference/k8sBasics/#debugging","title":"Debugging","text":"<p>When a pod crashes unexpectedly, you can mine information about the cause with the following commands.</p> <p>To view logs of the crash:</p> <pre><code>kubectl logs -f &lt;pod-name&gt; --previous\n</code></pre> <p>To view the reason for exit:</p> <pre><code>kubectl describe pod &lt;pod-name&gt;\n</code></pre> <p>When looking at <code>describe</code>, there are two main sections of the output to note:</p> <ul> <li>lastState - shows the exit code and the reason for exit.</li> <li>Events - this list is most helpful when your pod is not being created. It might be stuck in pending state if:</li> <li>There are not enough resources available for the pod to be created.</li> <li>Something about the pod definition is incorrect, such as a missing volume or secret.</li> </ul> <p>Common exit codes associated with containers are:</p> Exit Code Description Exit Code 0 Absence of an attached foreground process Exit Code 1 Indicates failure due to application error Exit Code 137 Indicates failure as container received SIGKILL (manual intervention or \u2018oom-killer\u2019 [OUT-OF-MEMORY]) Exit Code 139 Indicates failure as container received SIGSEGV Exit Code 143 Indicates failure as a container received SIGTERM"},{"location":"reference/pingone-config/","title":"PingOne Worker App and User Config","text":""},{"location":"reference/pingone-config/#pingone-worker-app-configuration","title":"PingOne Worker App Configuration","text":"<p>To manage PingOne resources using credentials other than your own, you are required to have a PingOne Worker App.</p> <p>You have 3 options to authenticate to PingOne from pingctl:</p> <ul> <li>Authorization Code (w/ PKCE) Flow (Recommended and most secure) - Via a PingOne Admin User</li> <li>Implicit Flow - Via a PingOne Admin User</li> <li>Client Credentials Flow (Easiest, but most insecure, as a user isn't required)</li> </ul> <p>Additionally, you must set up the proper roles for your Worker App</p>"},{"location":"reference/pingone-config/#authorization-code-w-pkce-flow-settings","title":"Authorization Code (w/ PKCE) Flow Settings","text":"<p>The following shows an example of a Worker App setup for Authorization Code (w/ PKCE) Flow:</p> <p></p>"},{"location":"reference/pingone-config/#implicit-flow-settings","title":"Implicit Flow Settings","text":"<p>The following shows an example of a Worker App setup for Implicit Flow:</p> <p></p>"},{"location":"reference/pingone-config/#client-credentials-flow-settings","title":"Client Credentials Flow Settings","text":"<p>The following shows an example of a Worker App setup for Client Credentials Flow:</p> <p></p>"},{"location":"reference/pingone-config/#worker-app-roles-settings","title":"Worker App Roles Settings","text":"<p>The following shows an example of the minimum roles required.  Typically, these are set up by default.</p> <p></p>"},{"location":"reference/pingone-config/#pingone-user-config","title":"PingOne User Config","text":"<p>When using Authorization Code or Implicit Flows, you need to log in with an Administrative user to use the Worker App.</p> <p>The most important item is to add the proper administrative roles to the user.  The following shows an example of this:</p> <p></p>"},{"location":"reference/profileStructures/","title":"Server Profile Structures","text":"<p>Each of the Docker images use a server profile structure that is specific to each product. The structure (directory paths and data) of the server profile differs between products. Depending on how you Deploy Your Server Profile, it will be pulled or mounted into <code>/opt/in</code> on the container and used to stage your deployment.</p> <p>The following locations are the server profile structures for each of our products with example usage. For help with an example of the basics, see the pingidentity-server-profiles/getting-started examples.</p> <p>Ignore <code>.sec</code> directories in examples</p> <p>In the getting-started profile examples, you should not use the <code>.sec</code> directory when providing passwords to your containers.  These examples are only intended for demonstration purposes. Instead, set an environment variable with your secrets or orchestration later:</p> <pre><code>  PING_IDENTITY_PASSWORD=\"secret\"\n</code></pre>"},{"location":"reference/profileStructures/#pingfederate","title":"PingFederate","text":"<p>See the example at getting-started/pingfederate.</p> Path Location description instance Directories and files that you want to be used at product runtime, in accordance with the directory layout of the product. instance/server/default/data An extracted configuration archive exported from PingFederate. instance/bulk-config/data.json A JSON export from the PingFed admin API <code>/bulk/export</code>. instance/server/default/deploy/OAuthPlayground.war Automatically deploy the OAuthPlayground web application. instance/server/default/conf/META-INF/hivemodule.xml Apply a Hive module config to the container. Used for persisting OAuth clients, grants, and sessions to an external DB. <p>PingFederate Integeration Kits</p> <p>By default, PingFederate is shipped with a handful of integration kits and adapters. If you need other integration kits or adapters in the deployment, manually download them and place them inside <code>server/default/deploy</code> of the server profile. You can find these resources in the product download page here.</p>"},{"location":"reference/profileStructures/#pingaccess","title":"PingAccess","text":"<p>Example at getting-started/pingaccess.</p> Path Location description instance Directories and files that you want to be used at product runtime, in accordance with the directory layout of the product. instance/conf/pa.jwk Used to decrypt a <code>data.json</code> configuration upon import. instance/data/data.json PA 6.1+ A config file that, if found by the container, is uploaded into the container. instance/data/PingAccess.mv.db Database binary that would be ingested at container startup if found. <p>PingAccess Best Practices</p> <p>PingAccess profiles are typically minimalist because the majority of PingAccess configurations can be found within a <code>data.json</code> or <code>PingAccess.mv.db</code> file. You should use <code>data.json</code> for configurations and only use <code>PingAccess.mv.db</code> if necessary. You can easily view and manipulate configurations directly in a JSON file as opposed to the binary <code>PingAccess.mv.db</code> file. This fact makes tracking changes in version control easier as well.</p> <p>PingAccess 6.1.x+ supports using only <code>data.json</code>, even when clustering. However on  6.1.0.3 make sure <code>data.json</code> is only supplied to the admin node.</p> <p>PingAccess 6.1.0+</p> <p>PingAccess now supports native <code>data.json</code> ingestion, which is the recommended method. Place <code>data.json</code> or <code>data.json.subst</code> in <code>instance/conf/data/start-up-deployer</code>.</p> <p>The JSON configuration file for PingAccess must be named <code>data.json</code>.</p> <p>A <code>data.json</code> file that corresponds to earlier PingAccess versions might be accepted. However, after you are on version 6.1.x, the <code>data.json</code> file will be forward compatible. This support means you are able to avoid upgrades for your deployments!</p> <p>PingAccess 6.0.x and earlier</p> <p>The JSON configuration file for PingAccess must be named <code>data.json</code> and located in the <code>instance/data</code> directory.</p> <p>All PingAccess versions</p> <p>A corresponding file named <code>pa.jwk</code> must also exist in the <code>instance/conf</code> directory for the <code>data.json</code> file to be decrypted on import. To get a <code>data.json</code> and <code>pa.jwk</code> that work together, pull them both from the same running PingAccess instance.</p> <p>For example, if PingAccess is running in a local Docker container you can use these commands to export the <code>data.json</code> file and copy the <code>pa.jwk</code> file to your local Downloads directory:</p> <pre><code>    curl -k -u \"Administrator:${ADMIN_PASSWORD}\" -H \"X-Xsrf-Header: PingAccess\" https://localhost:9000/pa-admin-api/v3/config/export -o ~/Downloads/data.json\n\n    docker cp &lt;container_name&gt;:/opt/out/instance/conf/pa.jwk ~/Downloads/pa.jwk\n</code></pre> <p>Password Variables</p> <p>You can find the PingAccess administrator password in <code>PingAccess.mv.db</code>, not in <code>data.json</code>. For this reason, you can use the following environment variables to manage different scenarios:</p> <ul> <li> <p><code>PING_IDENTITY_PASSWORD</code></p> <p>Use this variable if:</p> <ul> <li>You're starting a PingAccess container without any configurations.</li> <li>You're using only a <code>data.json</code> file for configurations.</li> <li>Your <code>PingAccess.mv.db</code> file has a password other than the default \"2Access\".</li> </ul> <p>The <code>PING_IDENTITY_PASSWORD</code> value will be used for all interactions with the PingAccess Admin API (such as importing configurations and creating clustering).</p> </li> <li> <p><code>PA_ADMIN_PASSWORD_INITIAL</code></p> <p>Use this variable in addition to <code>PING_IDENTITY_PASSWORD</code> to change the runtime admin password and override the password in <code>PingAccess.mv.db</code>.</p> <p>If you use only <code>data.json</code> and do notpass <code>PING_IDENTITY_PASSWORD</code>, the password will default to \"2FederateM0re\". Always use <code>PING_IDENTITY_PASSWORD</code>.</p> </li> </ul>"},{"location":"reference/profileStructures/#ping-data-products","title":"Ping Data Products","text":"<p>The Ping Data Products (PingDirectory, PingDataSync, PingAuthorize, PingDirectoryProxy) follow the same structure for server-profiles.</p> <p>Example at getting-started/pingdirectory.</p> Path Location description pd.profile Server profile matching the structure as defined by PingDirectory Server Profiles instance Directories and files that you want to be used at product runtime, in accordance with the layout of the product. In general, this should be non existing or empty. env-vars You can set environment variables used during deployment.  See Variables and Scope for more info.   In general, this should be non existing or empty. extensions You can provide URLs to download Server SDK extensions in a remote.list file.  See Including Extensions in PingData Server Profiles for more info. <p>Ping Data Server Profile Best Practices</p> <ul> <li>In most circumstances, the <code>pd.profile</code> directory should be the only directory in the server profile.</li> <li>All environment variables should be provided through Kubernetes configmaps/secrets and a secret management tool.   Be careful providing an <code>env-vars</code> and if you do, please review Variables and Scope</li> </ul> <p>Creating a <code>pd.profile</code> from scratch</p> <p>Use the <code>manage-profile</code> tool (found in product <code>bin</code> directory) to generate a <code>pd.profile</code> from an existing Ping Data 8.0+ deployment.  An example on creating this <code>pd.profile</code> looks like:</p> <pre><code>manage-profile generate-profile --profileRoot /tmp/pd.profile\nrm /tmp/pd.profile/setup-arguments.txt\n</code></pre> <p>Follow the instructions provided when you run the <code>generate-profile</code> to ensure that you include any additional components, such as <code>encryption-settings</code>.</p>"},{"location":"reference/readOnlyFilesystem/","title":"Running product containers with a read-only root filesystem","text":""},{"location":"reference/readOnlyFilesystem/#overview","title":"Overview","text":"<p>In some environments, there is a requirement that the container filesystem be read-only.  Our product images are maturing to support this capability natively in the future.  In the meantime, this guide will explain the overall concepts and provide an example with PingDirectory.  The other product images can operate in a similar manner.  </p> <p>Example only</p> <p>This guide is intended to provide an example implementation of solving this problem; your situation might require a different approach.</p> <p>Ping image exploration</p> <p>An excellent starting point for understanding what goes on with our containers as they are instantiated can be found in this video.  It is highly recommended that you take the time to view it prior to working through this guide.</p>"},{"location":"reference/readOnlyFilesystem/#high-level-process","title":"High-level process","text":"<p>In Ping product containers, the layered approach of bringing the environment and configuration parameters into the container at launch requires merging of files from server profiles and possibly other locations before the product is launched.  This process means that files are modified at runtime, and a read-only root filesystem blocks this action.  The overall approach is to use emptyDir volumes to overlay the directories that need modification, allowing the hook scripts to run as normal against the volume rather than the container filesystem.  In order to get everything necessary for the scripts in place, an init container (using the same image as the product container) is launched and the files necessary are copied to the shared volume before starting the product container.</p>"},{"location":"reference/readOnlyFilesystem/#prerequisites","title":"Prerequisites","text":"<ul> <li>kustomize CLI utility to serve as a post-renderer for Helm.</li> </ul>"},{"location":"reference/readOnlyFilesystem/#file-explanation","title":"File explanation","text":"<p>In the 30-helm/read-only-filesystem folder of the Getting Started repository is a values file and kustomize directory.  First, we will explore the the <code>pd-values.yaml</code> file; inline comments explain what is going on:</p> pd-values.yaml <pre><code>initContainers:\n  pd-init:\n    name: runtime-init\n    # CHANGEMETAG TO VERSION NEEDED\n    # Init container uses the same image as the product container and therefore versions much match\n    image: pingidentity/pingdirectory:CHANGEMETAG\n    env:\n      # Override the startup command so the product is not launched in the init container\n      - name: STARTUP_COMMAND\n        value: \"ls\"\n      # Use a name different from /opt/staging for holding the copied files from the product image into the emptyDir volume\n      - name: STAGING_DIR\n        value: \"/opt/handoff\"\n      # Just in case there is a .env we will need\n      - name: CONTAINER_ENV\n        value: \"/opt/handoff/.env\"\n      # Another flag for preventing the product from being launched\n      - name: STARTUP_FOREGROUND_OPTS\n        value: \"\"\n    envFrom:\n      # CHANGEMERELEASE TO MATCH HELM RELEASE NAME\n      - configMapRef:\n          name: CHANGEMERELEASE-global-env-vars\n          optional: true\n      - configMapRef:\n          name: CHANGEMERELEASE-env-vars\n          optional: true\n      - configMapRef:\n          name: CHANGEMERELEASE-pingdirectory-env-vars\n      - secretRef:\n          name: devops-secret\n          optional: true\n      - secretRef:\n          name: CHANGEME-pingdirectory-git-secret\n          optional: true\n    volumeMounts:\n      # emptyDir volume: /opt/staging will be copied from the init container to this volume\n      # This volume will be mounted as /opt/staging in the product container\n      - mountPath: /opt/handoff\n        name: staging\n        readOnly: false\n      # The location for the license file varies by product\n      # See https://devops.pingidentity.com/how-to/existingLicense/ for more information\n      # The license file is required for the init container to operate\n      - name: pingdirectory-license\n        mountPath: \"/opt/staging/pd.profile/server-root/pre-setup/PingDirectory.lic\"\n        subPath: PingDirectory.lic\n      # Also an emptyDir\n      - name: tmp\n        mountPath: \"/tmp\"\n        readOnly: false\n      # Also an emptyDir\n      - name: init-runtime\n        mountPath: \"/opt/out\"\n        readOnly: false\n      # Mount the slightly modified versions of the bootstrap and start sequence scripts (see below)\n      - mountPath: /opt/bootstrap.sh\n        name: bootstrap\n        readOnly: true\n        subPath: bootstrap.sh\n        defaultMode: 0555\n      - mountPath: /opt/staging/hooks/10-start-sequence.sh\n        name: init-start\n        readOnly: true\n        subPath: 10-start-sequence.sh\n        defaultMode: 0555\n\nvolumes:\n  # The 3 emptyDir volumes referenced above\n  init-runtime:\n    emptyDir: {}\n  staging:\n    emptyDir: {}\n  tmp:\n    emptyDir: {}\n  # This secret is created from a license file\n  pingdirectory-license:\n    secret:\n      secretName: pingdirectory-license\n  # Make the modified bootstrap and start sequence scripts available as configMaps\n  bootstrap:\n    configMap:\n      items:\n      - key: bootstrap.sh\n        path: bootstrap.sh\n      name: bootstrap\n  init-start:\n    configMap:\n      items:\n      - key: 10-start-sequence.sh\n        path: 10-start-sequence.sh\n      name: init-start\n\nconfigMaps:\n  init-start:\n    data:\n      10-start-sequence.sh: |-\n        #!/usr/bin/env sh\n        echo \"overwriting 10 hook\"\n        #!/usr/bin/env sh\n        #\n        # Ping Identity DevOps - Docker Build Hooks\n        #\n        # Called when it has been determined that this is the first time the container has\n        # been run.\n        #\n\n        ##############################################################################\n        ####### Prevent init container from starting the product normally.  ##########\n        ####### These two lines are the only delta from the default script. ##########\n        ##############################################################################\n        if test ${STARTUP_FOREGROUND_OPTS} != \"\" ; then\n          test \"${VERBOSE}\" = \"true\" &amp;&amp; set -x\n\n          # shellcheck source=./pingcommon.lib.sh\n          . \"${HOOKS_DIR}/pingcommon.lib.sh\"\n\n          echo \"Initializing server for the first time\"\n\n          run_hook \"17-check-license.sh\"\n\n          run_hook \"18-setup-sequence.sh\"\n        fi\n  bootstrap:\n    data:\n      bootstrap.sh: |-\n        #!/usr/bin/env sh\n        ######################################################################################################\n        ####### Make a copy of everything under /opt/staging in the product image to /opt/handoff.  ##########\n        ####### Primarily, this makes the hook scripts available in the emptyDir (writable) volume. ##########\n        ####### This line is the only delta from the default script.                                ##########\n        ######################################################################################################\n        cp -r /opt/staging/* /opt/handoff\n        test \"${VERBOSE}\" = \"true\" &amp;&amp; set -x\n        # shellcheck source=./staging/hooks/pingcommon.lib.sh\n        . \"${HOOKS_DIR}/pingcommon.lib.sh\"\n\n        _userID=$(id -u)\n        _groupID=$(id -g)\n\n        echo \"### Bootstrap\"\n        if test \"${_userID}\" -eq 0; then\n            echo_yellow \"### Warning: running container as root user\"\n        else\n            echo \"### Using the default container user and group\"\n\n            _effectiveGroupName=$(awk 'BEGIN{FS=\":\"}$3~/^'\"${_groupID}\"'$/{print $1}' /etc/group)\n            test -z \"${_effectiveGroupName}\" &amp;&amp; _effectiveGroupName=\"undefined group\"\n\n            _effectiveUserName=$(awk 'BEGIN{FS=\":\"}$3~/^'\"${_userID}\"'$/{print $1}' /etc/passwd)\n            test -z \"${_effectiveUserName}\" &amp;&amp; _effectiveUserName=\"undefined user\"\n\n            echo \"### Container user and group\"\n            echo \"###     user : ${_effectiveUserName} (id: ${_userID})\"\n            echo \"###     group: ${_effectiveGroupName} (id: ${_groupID})\"\n        fi\n\n        # if the current process id is not 1, tini needs to register as sub-reaper\n        if test $$ -ne 1; then\n            _subReaper=\"-s\"\n        fi\n\n        # shellcheck disable=SC2086,SC2048\n        exec \"${BASE}/tini\" ${_subReaper} -- \"${BASE}/entrypoint.sh\" ${*}\n\npingdirectory:\n  enabled: true\n  envs:\n    MUTE_LICENSE_VERIFICATION: \"yes\"\n    ORCHESTRATION_TYPE: \"NONE\"\n    VERBOSE: \"true\"\n # (Optional) Specify a particular tag by uncommenting these two lines and naming the tag to use.\n # Otherwise, you will get the latest from Docker Hub.\n # If a particular tag is used, be sure the init container tag matches above\n # image:\n #   tag: \"2306\"\n  includeInitContainers:\n  # Use the init container specification above at pod startup\n    - pd-init\n  # Share the volumes between the init container and the product container\n  includeVolumes:\n    - staging\n    - tmp\n    - pingdirectory-license\n    - bootstrap\n    - init-start\n    - init-runtime\n  volumeMounts:\n    # The emptyDir mounted at /opt/handoff in the init container is mounted to /opt/staging here\n    # Hook scripts and product startup will operate as with a read/write filesystem\n    - mountPath: /opt/staging\n      name: staging\n      readOnly: false\n    - name: pingdirectory-license\n      mountPath: \"/opt/staging/pd.profile/server-root/pre-setup/PingDirectory.lic\"\n      subPath: PingDirectory.lic\n    - name: tmp\n      mountPath: \"/tmp\"\n      readOnly: false\n</code></pre> <p>Kustomize</p> <p>Kustomize is used as the Ping helm charts do not support setting the readOnlyFileSystem value to the securityContext of a container at this time.</p> <p>In the 30-helm/read-only-filesystem/kustomize subdirectory is a kustomize script and definition file.</p> <p>The script simply runs kustomize: <pre><code>#!/bin/bash\n\ncat &lt;&amp;0 &gt; kustomize/all.yaml\n\nkustomize build kustomize &amp;&amp; rm kustomize/all.yaml\n</code></pre></p> <p>The <code>kustomization.yaml</code> file sets the securityContext for the each container's root file system to read-only:</p> <pre><code>resources:\n  - all.yaml\n\npatches:\n  - target:\n      group: apps\n      version: v1\n      kind: StatefulSet\n    patch: |-\n      - op: add\n        path: /spec/template/spec/containers/0/securityContext\n        value:\n          readOnlyRootFilesystem: true\n      - op: add\n        path: /spec/template/spec/initContainers/0/securityContext\n        value:\n          readOnlyRootFilesystem: true\n</code></pre>"},{"location":"reference/readOnlyFilesystem/#process","title":"Process","text":"<p>To use the example files to deploy PingDirectory with a read-only root filesystem, follow the steps here:</p> <ol> <li> <p>Generate a license file and create a secret.  If you have an existing license file, you can use it here:</p> <pre><code># Generate the license file\npingctl license pingdirectory 9.2 &gt; PingDirectory.lic\n\n# Create the secret to match the name in the values file:\nkubectl create secret generic pingdirectory-license --from-file=./PingDirectory.lic\n</code></pre> </li> <li> <p>Update the <code>30-helm/read-only-filesystem/pd-values.yaml</code> file with the appropriate image tag and release name to be used with Helm.  Afterward, use Helm to deploy the release:</p> <pre><code># For this example, the release name of 'rofs' is used\ncd 30-helm/read-only-filesystem\nhelm upgrade --install rofs pingidentity/ping-devops -f './pd-values.yaml' \\\n      --post-renderer kustomize/kustomize\n</code></pre> </li> </ol>"},{"location":"reference/readOnlyFilesystem/#diagram","title":"Diagram","text":"<p>This image provides an overview of what happens.  It is best viewed in a separate tab:</p> <p></p> <p>/etc/motd</p> <p>One issue we encountered is that the <code>/etc/motd</code> file can be modified at startup by hook scripts, but /etc/ is read-only.  We are exploring ways to address this in some way, but at this time, it appears one possible solution is to treat <code>/etc/</code> in the same manner as /opt/staging (copying to an emptyDir) if the <code>motd</code> file is to be updated.  However, <code>/etc</code> has many more files and directories and such a solution is not practical.  Baking it into a custom image is another possibility.</p>"},{"location":"reference/troubleshooting/","title":"Troubleshooting","text":""},{"location":"reference/troubleshooting/#getting-started","title":"Getting started","text":""},{"location":"reference/troubleshooting/#examples-not-working","title":"Examples Not Working","text":"<p>Many common errors using Ping containers arise from using stale images. Our development is highly dynamic and Docker images can rapidly change.</p> <p>To avoid issues with stale images, remove all local images from your local cache.  Doing so will force Docker to pull the latest images:</p> <pre><code>docker rmi $(docker images \"pingidentity/*\" -q)\n</code></pre> <p>Moving tag</p> <p>Even though you might have a local image tagged \"latest\", this tag does not guarantee it is the newest image in the Docker hub registry.  This tag is reapplied for each release image.</p>"},{"location":"reference/troubleshooting/#misconfigured-pingctl","title":"Misconfigured <code>pingctl</code>","text":"<p>If your containers cannot pull a license based on your DevOps user name and key, there might be some misconfiguration in your <code>pingctl config</code> file.</p> <p>Possible solutions:</p> <ol> <li> <p>If you have ran <code>pingctl config</code> for the first time, see the Environment Configuration Documentation on how to export configured variables to your environment.</p> </li> <li> <p>Run <code>pingctl info</code> and make sure the configured variables in the utility are correct. See the utility's documentation for more information.</p> </li> </ol>"},{"location":"reference/troubleshooting/#unable-to-retrieve-evaluation-license","title":"Unable To Retrieve Evaluation License","text":"<p>If a product instance or instances cannot retrieve the evaluation license, you might receive an error similar to this:</p> <pre><code>----- Starting hook: /opt/staging/hooks/17-check-license.sh\nPulling evaluation license from Ping Identity for:\n              Prod License: PD - v7.3\n              DevOps User: some-devops-user@example.com...\nUnable to download evaluation product.lic (000), most likely due to invalid PING_IDENTITY_DEVOPS_USER/PING_IDENTITY_DEVOPS_KEY\n\n##################################################################################\n############################        ALERT        #################################\n##################################################################################\n#\n# No Ping Identity License File (PingDirectory.lic) was found in the server profile.\n# No Ping Identity DevOps User or Key was passed.\n#\n#\n# More info on obtaining your DevOps User and Key can be found at:\n#     https://devops.pingidentity.com/how-to/devopsRegistration/\n#\n##################################################################################\nCONTAINER FAILURE: License File absent\nCONTAINER FAILURE: Error running 17-check-license.sh\nCONTAINER FAILURE: Error running 10-start-sequence.sh\n</code></pre> <p>This error can be caused by:</p> <ol> <li> <p>An invalid DevOps user name or key (as noted in the error). This failure is usually caused by some issue with the variables being passed in. To verify the variables in the <code>pingctl</code> configuration are correct for running Docker commands, run the following command:</p> <pre><code>pingctl info\n</code></pre> </li> <li> <p>A bad Docker image. Pull the Docker image again to verify.</p> </li> <li> <p>Network connectivity to the license server is blocked. To test this, on the machine that is running the container, run:</p> <pre><code>curl -k https://license.pingidentity.com/devops/license\n</code></pre> <p>If the license server is reachable, you will receive an error similar to this example:</p> <pre><code>{ \"error\":\"missing devops-user header\" }\n</code></pre> </li> </ol>"},{"location":"reference/usingCertificates/","title":"Using Certificates with Images","text":"<p>This page provides details for using certificates with the Ping Identity images. Specifically, it outlines the preferred locations to place the certificate and PIN/key files to provide best security practices and enable use by the underlying Ping Identity product.</p> <p>Currently, certificates can be provided to the PingData products when the containers are started.</p>"},{"location":"reference/usingCertificates/#before-you-begin","title":"Before you begin","text":"<p>You must:</p> <ul> <li>Complete Get started to set up your DevOps environment and run a test deployment of the products.</li> <li>Strongly recommended: Have a secrets management system, such as Hashicorp Vault, that holds your certificate and places them into your SECRETS_DIR (/run/secrets).</li> </ul> <p>For information on using a vault, if you have one, see Using Hashicorp Vault.</p>"},{"location":"reference/usingCertificates/#about-this-topic","title":"About this topic","text":"<p>The following examples explain how to deploy a certificate/PIN combination to an image in a secure way.</p>"},{"location":"reference/usingCertificates/#pingdata-image-certificates","title":"PingData Image Certificates","text":"<p>The PingData products (PingDirectory, PingDataSync, PingAuthorize, and PingDirectoryProxy) use a file location to determine certificates/PIN files:</p> <ul> <li>It is best practice to use a non-persistent location, such as /run/secrets, to store these files.</li> <li>If no certificate is provided, the container/product will generate a self-signed certificate.</li> </ul> <p>The default location for certificates and associated files are listed below, assuming a default SECRETS_DIR variable of <code>/run/secrets</code>.</p> Variable Used Default Location/Value/run/secrets... Notes Keystore (JKS) KEYSTORE_FILE keystore Java KeyStore (JKS) Format. Set as default in absence of .p12 suffix. Keystore (PKCS12) KEYSTORE_FILE keystore.p12 PKCS12 Format Keystore Type KEYSTORE_TYPE jks, pkcs12, pem, or bcfks Based on suffix of KEYSTORE_FILE.Only use BCFKS in FIPS mode. Keystore PIN KEYSTORE_PIN_FILE keystore.pin Truststore (JKS) TRUSTSTORE_FILE truststore Set as default in absence of .p12 suffix. Truststore (PKCS12) TRUSTSTORE_FILE truststore.p12 PKCS12 Format Truststore Type TRUSTSTORE_TYPE jks, pkcs12, pem, or bcfks Based on suffix of TRUSTSTORE_FILE.Only use BCFKS in FIPS mode. Truststore PIN TRUSTSTORE_PIN_FILE truststore.pin Certificate Nickname CERTIFICATE_NICKNAME see below <p>CERTIFICATE_NICKNAME Setting</p> <p>There is an additional certificate-based variable used to identity the certificate alias used within the <code>KEYSTORE_FILE</code>. That variable is called <code>CERTIFICATE_NICKNAME</code>, which identifies the certificate to use by the server in the <code>KEYSTORE_FILE</code>. If a value is not provided, the container will look at the list certs found in the <code>KEYSTORE_FILE</code> and if one - and only one - certificate is found of type <code>PrivateKeyEntry</code>, that alias will be used.</p> <p>Specifying your own location for a certificate</p> <p>If you are relying on certificates to be mounted to a different locations than the SECRET_DIR location or a different filename, you can provide your own values for those variables identified above. As an example:</p> <pre><code>KEYSTORE_FILE=/my/path/to/certs/cert-file\nKEYSTORE_PIN_FILE=/my/path/to/certs/cert.pin\nKEYSTORE_TYPE=jks\nCERTIFICATE_NICKNAME=development-cert\n</code></pre>"},{"location":"reference/usingCertificates/#pingdata-image-certificate-rotation","title":"PingData image certificate rotation","text":"<p>The certificate rotation process for PingData products varies depending on which product is being configured and whether that product is in a topology. For products that are not in a topology, certificates can be rotated by simply updating the environment variables. For products in a topology, certificate rotation must be done via a command-line call with the servers in the topology online.</p>"},{"location":"reference/usingCertificates/#rotating-the-listener-certificate-by-adjusting-environment-variables","title":"Rotating the listener certificate by adjusting environment variables","text":"<p>The process described in this section can be used for PingAuthorize, PingDirectoryProxy, and standalone (single-server) instances of PingDirectory or PingDataSync.</p> <p>Warning</p> <p>If PingDirectory or PingDataSync is deployed with multiple servers, use the process described in the next section.</p> <p>As mentioned above, for the PingData products there are variables defining the server truststore and keystore. To change certificates, you will need to update the contents of the truststore or keystore in your server profile or secret store. After you update the contents, restart the container. The changes will be picked up automatically when the server restarts. If you have multiple certificates in the keystore, you can use the above-mentioned CERTIFICATE_NICKNAME variable to specify the certificate. The container will pick up that certificate from those stored in the keystore. For updating the product to use the new certificates, perform a rolling update. This action ensures that other servers will remain available as each pod is cycled.</p> <p>Rolling Update</p> <p>Verify that remaining pods in the cluster have sufficient capacity to handle the increased load during the rolling update.</p>"},{"location":"reference/usingCertificates/#rotating-the-listener-certificate-with-the-replace-certificate-command-line-tool","title":"Rotating the listener certificate with the replace-certificate command-line tool","text":"<p>If multiple PingDataSync or PingDirectory servers are running in a topology, then the servers must be online when updating the listener certificate. Updates to certificates with one or more servers offline (such as rolling updates) can lead to connection issues with the other members of the topology when those servers come back online. Use the PingData <code>replace-certificate</code> command-line tool to update certificates with the server online.</p> <p>Shell into the running instance that needs to be updated, and ensure the keystore containing the needed certificate is mounted on the container. Then, run <code>replace-certificate</code>. Replace the <code>--key-manager-provider</code> and <code>--trust-manager-provider</code> values if necessary when using a non-JKS keystore, as well as the <code>--source-certificate-alias</code> value if necessary.</p> <pre><code>replace-certificate replace-listener-certificate \\\n    --key-manager-provider JKS \\\n    --trust-manager-provider JKS \\\n    --source-key-store-file /run/secrets/newkeystore \\\n    --source-key-store-password-file /run/secrets/newkeystore.pin \\\n    --source-certificate-alias server-cert \\\n    --reload-http-connection-handler-certificates\n</code></pre> <p>For more information on this command, run <pre><code>replace-certificate replace-listener-certificate --help\n</code></pre></p> <p>Running the first command will replace the listener certificate and notify other servers in the topology that this server's certificate has changed.</p> <p>To update certificates for the other servers in the topology, follow this same process, shelling into each individual instance.</p> <p>Once this is done, the running pods have been updated. To ensure a restart does not undo these changes, verify that your server profile and orchestration environment variables are updated to point to the new certificates. For example, if you have modified your server configuration to point to <code>/run/secrets/newkeystore</code>, then you must update your KEYSTORE_FILE environment variable to point to that new keystore after you have completed the <code>replace-certificate</code> process on each server.</p>"},{"location":"reference/usingCertificates/#non-pingdata-image-certificates","title":"Non-PingData image certificates","text":"<p>For non-PingData images, such as PingAccess and PingFederate, the certificates are managed within the product configurations.</p>"},{"location":"reference/variableScoping/","title":"Variables and scope","text":"<p>Variables provide a way to store and reuse values with our Docker containers which are ultimately used by our Docker image hooks to customize configurations.</p> <p>It's important to understand:</p> <ul> <li>The different levels at which you can set variables</li> <li>How you should use variables</li> <li>Where you should set and use variables</li> </ul> <p>The following diagram shows the different scopes in which variables can be set and applied.</p> <p></p> <p>Assume that you are viewing this diagram as a pyramid with the container at the top. The order of precedence for variables is top-down. Generally, you set variables having an orchestration scope.</p>"},{"location":"reference/variableScoping/#image-scope","title":"Image scope","text":"<p>Variables having an image scope are assigned using the values set for the Docker image (for example, from Dockerfiles). These variables are often set as defaults, allowing scopes with a higher level of precedence to override them.</p> <p>To see the default environment variables available with any Docker image, enter:</p> <pre><code>docker run pingidentity/&lt;product-image&gt;:&lt;tag&gt; env | sort\n</code></pre> <p>Where &lt;product-image&gt; is the name of one of our products, and &lt;tag&gt; is the release tag (such as <code>edge</code>).</p> <p>For the environment variables available for all products (PingBase) or individual products, see the Docker Images Reference.</p>"},{"location":"reference/variableScoping/#orchestration-scope","title":"Orchestration scope","text":"<p>Variables having orchestration scope are assigned at the orchestration layer. Typically, these environment variables are set using Docker commands, Docker Compose or Helm values. For example:</p> <ul> <li> <p>Using <code>docker run</code> with <code>--env</code>:</p> <pre><code>docker run --env SCOPE=env \\\n  pingidentity/pingdirectory:edge env | sort\n</code></pre> </li> <li> <p>Using <code>docker run</code> with <code>--env-file</code>:</p> <pre><code>echo \"SCOPE=env-file\"  &gt; /tmp/scope.properties\n\ndocker run --env-file /tmp/scope.properties \\\n  pingidentity/pingdirectory:edge env | sort\n</code></pre> </li> <li> <p>Using Docker Compose (docker-compose.yaml):</p> <pre><code>environment:\n  - SCOPE=compose\n    env_file:\n  - /tmp/scope.properties\n</code></pre> </li> <li> <p>Using Kubernetes:</p> <pre><code>env:\n  - name: SCOPE\n    value: kubernetes\n</code></pre> </li> <li> <p>Using Helm variables:</p> <pre><code>global:\n  envs:\n    PING_IDENTITY_ACCEPT_EULA: \"YES\"\n    PING_IDENTITY_PASSWORD: \"2Federate\"\n  ...\n</code></pre> </li> </ul>"},{"location":"reference/variableScoping/#server-profile-scope","title":"Server profile scope","text":"<p>Variables having server profile scope are supplied using property files in the server-profile repository.  You need to be careful setting variables at this level because the settings can override variables already having an image or orchestration scope value set.</p> <p>You can use the following masthead in your <code>env_vars</code> files to provide examples of setting variables and how they might override variables having a scope with a lower level of precedence. It will also suppress a warning when processing the env_vars file:</p> <pre><code># .suppress-container-warning\n#\n# NOTICE: Settings in this file will override values set at the\n#         image or orchestration layers of the container.  Examples\n#         include variables that are specific to this server profile.\n#\n# Options include:\n#\n# ALWAYS OVERRIDE the value in the container\n#   NAME=VAL\n#\n# SET TO DEFAULT VALUE if not already set\n#   export NAME=${NAME:=myDefaultValue}  # Sets to string of \"myDefaultValue\"\n#   export NAME=${NAME:-OTHER_VAR}       # Sets ot value of OTHER_VAR variable\n#\n</code></pre>"},{"location":"reference/variableScoping/#container-scope","title":"Container scope","text":"<p>Variables having a container scope are assigned in the hook scripts and will overwrite variables that are set elsewhere. Variables that need to be passed to other hook scripts must be appended to the file assigned to <code>${CONTAINER_ENV}</code>, (which defaults to <code>/opt/staging/.env</code>). This file is sourced by every hook script.</p>"},{"location":"reference/variableScoping/#scoping-example","title":"Scoping example","text":""},{"location":"reference/yamlFiles/","title":"Customizing YAML files","text":"<p>Docker Compose uses YAML files to configure a stack's containers on startup. You can customize our YAML files or use them as the basis for creating your own. For more information, see the Docker Compose File Reference.</p> <p>To customize our YAML files, you can:</p> <ul> <li>Add or change environment variables to use different server profiles.</li> <li>Add references to a file or files containing environment variables to pass to the container on startup.</li> <li>Change the <code>wait-for</code> times used to control the startup sequence of containers.</li> <li>Change the port mappings for a container.</li> <li>Change the release tag used for the Docker images (all product containers in the stack must use the same release tag). See Using Release Tags for more information.</li> </ul> <p>You will find the YAML files for deploying individual Ping product containers in the <code>${HOME}/projects/devops/pingidentity-devops-getting-started/11-docker-compose</code> directory.</p>"},{"location":"reference/yamlFiles/#yaml-file-format","title":"YAML file format","text":"<p>This format is used for the YAML files:</p> <pre><code>version: \"3.9\"\n\nservices:\n    &lt;ping-product&gt;:\n      image: pingidentity/&lt;ping-product&gt;:${PING_IDENTITY_DEVOPS_TAG}\n      command: wait-for &lt;another-ping-product&gt;:&lt;startup-port&gt; -t &lt;time-to-wait&gt; -- entrypoint.sh start-server\n      environment:\n        - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git\n        - SERVER_PROFILE_PATH=baseline/&lt;ping-product&gt;\n        - PING_IDENTITY_ACCEPT_EULA=YES\n      env_file:\n        - ~/.pingidentity/config\n      #volumes:\n        # - ${HOME}/projects/devops/volumes/full-stack.&lt;ping-product&gt;:/opt/out\n        # - ${HOME}/projects/devops/pingidentity-server-profiles/baseline/&lt;ping-product&gt;:/opt/in\n      ports:\n        - &lt;host-port&gt;:&lt;container-port&gt;\n        - &lt;host-port&gt;:&lt;container-port&gt;\n      networks:\n        - pingnet-dmz\n\nnetworks:\n    pingnet-internal:\n    pingnet-dmz:\n</code></pre> Entry Description <code>version</code> The Docker Compose file specification version used. <code>&lt;ping-product&gt;</code> The name of the Ping Identity product container. <code>image</code> The build image of the product used for the container and the build tag to use (defaults to value assigned to <code>PING_IDENTITY_DEVOPS_TAG</code> in the <code>~/.pingidentity/config</code> file. <code>command</code> We use the <code>wait-for</code> script to control the startup order, where <code>&lt;startup-port&gt;</code> is the port to check for whether <code>&lt;another-ping-product&gt;</code> container has started. The <code>&lt;time-to-wait&gt;</code> argument is the number of seconds to wait before executing the <code>entrypoint.sh</code> script with the <code>start-server</code> command. If you find a container is timing out while waiting for another container to start, try increasing the <code>&lt;time-to-wait&gt;</code> value. <code>environment</code> The environment variables being set. See Customizing Server Profiles for more information. The <code>PING_IDENTITY_ACCEPT_EULA</code> environment variable is set to \"YES\" when you complete the DevOps registration. This variable assignment appears here by default but could also be in your <code>~/.pingidentity/config</code> file. <code>env_file</code> A file or files containing environment variable settings. The DevOps environment settings are stored in your <code>~/.pingidentity/config</code> file. You also can specify additional files containing environment settings. For more information, see Customizing Server Profiles. <code>volumes</code> Commented out by default. The location bind mounted to the  <code>/opt/out</code> volume is used to persist product container state and data. The location bind mounted to the <code>/opt/in</code> volume is used to supply server profile information to the container on startup. For more information, see Modify a server profile using local directories in Customizing Server Profiles. <code>ports</code> The port mappings between the host and the product container. For more information, see the Ports topic in the Docker Compose File Reference. <code>networks</code> One or more of the networks listed under the top-level <code>networks</code> key that the product container can use for Docker network communications. top level <code>networks</code> The Docker networks available for assignment to the containers in the stack. Our stacks are built to use an internal Docker network for communications between product containers (<code>pingnet-internal</code>) and an external-facing DMZ for external network communications (<code>pingnet-dmz</code>)."},{"location":"release-notes/currentRelease/","title":"Version 2407 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/currentRelease/#devops-docker-builds-version-2407-aug-5-2024","title":"DevOps Docker Builds, Version 2407 (Aug 5 2024)","text":""},{"location":"release-notes/currentRelease/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingFederate 12.1.0 \u2192 12.1.1</li> <li>PingFederate 12.0.3 \u2192 12.0.4</li> <li>PingAccess 8.1.0 \u2192 8.1.1</li> <li>PingAccess 8.0.3 \u2192 8.0.4</li> <li>PingData products 10.1.0.0 \u2192 10.1.0.1<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> <li>PingData products 10.0.0.2 \u2192 10.0.0.3<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> </ul>"},{"location":"release-notes/currentRelease/#enhancements","title":"Enhancements","text":"<ul> <li>Apache Tomcat 9.0.90 \u2192 9.0.91</li> <li>Redhat UBI9-minimal 9.4-1134 \u2192 9.4-1194</li> <li>Alpine 3.20.1 \u2192 3.20.2</li> <li>Liberica JDK11 11.0.23+12 \u2192 11.0.24+9</li> <li>Liberica JDK17 17.0.11+12 \u2192 17.0.12+10</li> </ul>"},{"location":"release-notes/currentRelease/#documentation","title":"Documentation","text":"<ul> <li>(PDI-1885) Add notification of deprecation of PingIntelligence Docker images</li> </ul>"},{"location":"release-notes/currentRelease/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2003/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2003/#devops-docker-builds-version-2003","title":"DevOps Docker Builds, Version 2003","text":""},{"location":"release-notes/relnotes-2003/#new-features","title":"New Features","text":"<ul> <li> <p>PingDirectoryProxy</p> <p>The PingDirectoryProxy Docker image is now available. See the Ping Identity Docker Hub</p> </li> <li> <p>PingCentral</p> <p>The PingCentral Docker image is now available. See the Ping Identity Docker Hub</p> </li> <li> <p>Docker Compose Port Mappings</p> <p>We now support the Docker Compose best practice of quoting all port mappings.</p> </li> <li> <p>Docker Images (Tag: edge)</p> <p>We've built a pipeline to support nightly public builds of all Ping Identity Docker images using the <code>edge</code> tag.</p> </li> <li> <p>PingDirectory</p> <p>We've upgraded the PingDirectory Docker image to the current product version 8.0.0.1.</p> </li> <li> <p>PingFederate Version 10.1.0</p> <p>We've built a beta PingFederate 10.1.0 Docker image.</p> </li> <li> <p>PingAccess Version 6.1.0</p> <p>We've built a beta PingAccess 6.1.0 Docker image.</p> </li> <li> <p>Ping Tool Kit</p> <p>The Ping Tool Kit Docker image is now available. See Ping Identity Docker Hub. Both <code>kubectl</code> and <code>kustomize</code> are supported in the image.</p> </li> <li> <p>PingFederate Version 9.3</p> <p>We've updated the PingFederate 9.3 Docker image to include the latest product patches.</p> </li> <li> <p>The ping-devops Utility</p> <p>We've added Kubernetes license secret generation, and server profile generation for PingDirectory  to the ping-devops utility. See The ping-devops utility.</p> </li> <li> <p>A New Hook</p> <p>We've added a security start-up hook notifying administrators of keys and secrets found in the server profile.</p> </li> <li> <p>DevOps Evaluation License</p> <p>We've added retry functionality to attempt getting the DevOps evaluation license if the initial request fails.</p> </li> <li> <p>Product Artifacts and Extensions</p> <p>We've created operations to retrieve product artifacts and extensions using the DevOps credentials.</p> </li> <li> <p>Java 11</p> <p>We've migrated all Alpine-based Docker images to Java 11 (Azul).</p> </li> <li> <p>PingDirectory Replication Timing</p> <p>We've added a profile and reference example to test PingDirectory replication timing. See the pingidentity-devops-getting-started Repo.</p> </li> <li> <p>Docker Base Image Security</p> <p>We've documented an evaluation of Docker base image security. See Evaluation of Docker Base Image Security.</p> </li> </ul>"},{"location":"release-notes/relnotes-2003/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-85) Resolved an issue where PingAccess 6.0 loaded a 5.2 license.</li> <li>(GDO-87) Resolved an issue where Data Console wasn't allowing users to authenticate (edge tag).</li> <li>(GDO-124) Resolved an issue in with pipeline where starting containers using Docker-Compose timed out.</li> <li>(GDO-89) Resolved an issue where <code>*.subst</code> template files were able to overwrite the server profile configuration.</li> <li>(GDO-72) Resolved an issue where <code>motd.json</code> did not parse correctly when the product was missing.</li> <li>(GDO-88) Resolved an issue where PingFederate profile metadata did not expand <code>hostname</code>, breaking OAuth flows.</li> </ul>"},{"location":"release-notes/relnotes-2003/#changed","title":"Changed","text":"<ul> <li>(GDO-97) Removed WebConsole HTTP servlet from the baseline server profile. See the pingidentity-server-profiles repo.</li> </ul>"},{"location":"release-notes/relnotes-2003/#qualified","title":"Qualified","text":"<ul> <li>(GDO-42) Verified the ability to run our Docker containers as a non-root user. See Securing the Containers.</li> </ul>"},{"location":"release-notes/relnotes-2004/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2004/#devops-docker-builds-version-2004","title":"DevOps Docker Builds, Version 2004","text":""},{"location":"release-notes/relnotes-2004/#new-features","title":"New Features","text":"<ul> <li> <p>PingCentral</p> <p>The PingCentral Docker image is now available. See the Ping Identity Docker hub.</p> </li> <li> <p>Docker Compose</p> <p>We've standardized our Docker Compose references.</p> </li> <li> <p>Performance</p> <p>We've built a performance framework.</p> </li> <li> <p>PingFederate version 10.0.2</p> <p>We've updated the PingFederate 10 Docker image for the 10.0.2 release.</p> </li> <li> <p>The ping-devops utility</p> <p>We've added major enhancements to our ping-devops utility. See The ping-devops Utility.</p> </li> <li> <p>PingDirectory replication</p> <p>We've added support for PingDirectory replication using Docker Compose.</p> </li> <li> <p>Variables and scope</p> <p>We've added documentation to help with understanding the effective scope of variables. See Variables and Scope.</p> </li> </ul>"},{"location":"release-notes/relnotes-2004/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-1) Resolved issue where users were unable to override root and admin user passwords (PingDirectory).</li> <li>(GDO-129) Removed the console from Ping Data products when the server profile isn't specified.</li> <li>(GDO-54) Resolved PingDataGovernance issues within the baseline server profile.</li> <li>(GDO-138) Resolved issue regarding PingDataGovernance Policy Administration Point (PAP) launch.</li> <li>(GDO-189) Resolved issue with PingAccess heartbeat check.</li> <li>(GDO-196) Replaced nslookup with getent due to issues running in Alpine.</li> <li>(GDO-180) Resolved issue where extension signature verification may return a false positive.</li> <li>(GDO-169) Resolved issues with Ping Data Console by upgrading to Tomcat 9.0.34.</li> <li>(GDO-166) Resolved issue with make-ldif template processing.</li> </ul>"},{"location":"release-notes/relnotes-2005/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2005/#devops-docker-builds-version-2005-may-2020","title":"DevOps Docker Builds, Version 2005 (May 2020)","text":""},{"location":"release-notes/relnotes-2005/#new-features","title":"New Features","text":"<ul> <li> <p>PingDelegator Docker Image</p> <p>The PingDelegator Docker image is now available. View on Docker Hub for more information.</p> <p>Test drive PingDelegator using the supplied docker-compose file in our Simple-Stack example.</p> </li> <li> <p>PingAccess Image Version 6.0.2</p> <p>We've updated the PingAccess Image to version 6.0.2.</p> </li> <li> <p>PingFederate Version 9.3.3</p> <p>We've updated the PingFederate 9.3.3 Docker image to include patch 4.</p> </li> <li> <p>Docker Builds Pipeline</p> <p>We've made a number of CI/CD enhancements to improve Image qualification (smoke/integration tests).</p> </li> <li> <p>Image Enhancements</p> <p>Improved the <code>wait-for</code> command to optionally wait for a path or file to become available.</p> </li> </ul>"},{"location":"release-notes/relnotes-2005/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-187) Resolved issue where MAX_HEAP_SIZE wasn't applied during container restart.</li> <li>(GDO-220) Resolved issue where log message didn't contain log file source name.</li> <li>(GDO-238) Resolved issue where ping-devops kubernetes start fails if DNS_ZONE variable not set.</li> <li>(GDO-245) Resolved issue where PingAccess didn't exit when configuration import failed.</li> <li>(GDO-263) Resolved issue within deploy_docs.sh which had resulted in some documentation to not be pushed to GitHub.</li> <li>(GDO-278) Resolved issue with PingAccess clustering Server Profile.</li> </ul>"},{"location":"release-notes/relnotes-2006/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2006/#devops-docker-builds-version-2006-june-2020","title":"DevOps Docker Builds, Version 2006 (June 2020)","text":""},{"location":"release-notes/relnotes-2006/#new-features","title":"New Features","text":"<ul> <li> <p>Docker Compose Volumes</p> <p>Applications that create and manage configuration now have mounted volumes in Docker-Compose Examples, ensuring that your configuration changes are persisted across restarted.</p> </li> <li> <p>PingAccess Image Enhancements</p> <p>We've updated the PingAccess Image to support the new features available in version 6.1.</p> </li> <li> <p>Customer Support Data Collection</p> <p>Included in this release is the Java diagnostic tool to enable embedded customer support data collection. This tool set includes jstat, jmap and jhat.</p> </li> </ul>"},{"location":"release-notes/relnotes-2006/#new-product-versions","title":"New Product Versions","text":"<p>The following new product versions are available using edge, latest and 2006 image tags:</p> <ul> <li>PingFederate 10.1.0</li> <li>PingAccess 6.1.0</li> <li>PingDirectory 8.1.0.0</li> <li>PingDirectoryProxy 8.1.0.0</li> <li>PingDataGovernance 8.1.0.0</li> <li>PingDataGovernance 8.1.0.0 PAP</li> <li>PingDataSync 8.1.0.0</li> <li>PingCentral 1.4.0</li> </ul>"},{"location":"release-notes/relnotes-2006/#improvements","title":"Improvements","text":"<ul> <li> <p>Liveness Check</p> <p>We've made improvements to PingDirectory's liveness check to better inform dependant services on the status of the Directory service.</p> </li> <li> <p>Docker Build Pipeline</p> <ul> <li> <p>We've published Documentation on how to build a Ping Identity Docker Image using a local zip artifact.</p> </li> <li> <p>We have improved our reference pipeline to allow for the build of a single product.</p> </li> <li> <p>We've made several CI/CD enhancements to improve Image qualification (smoke/integration tests).</p> </li> </ul> </li> <li> <p>Configuration Substitution</p> <p>We've made enhancements to explicitly send the variables to be substituted.</p> </li> </ul>"},{"location":"release-notes/relnotes-2006/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-218) Resolved an issue where PingDirectory threw an error on manage-profile during setup.</li> <li>(GDO-289) Resolved an issue where Alpine based image couldn't install pip3.</li> <li>(GDO-329) Resolved an issue where PingCentral docs were not syncing to GitHub.</li> </ul>"},{"location":"release-notes/relnotes-2007/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2007/#devops-docker-builds-version-2007-july-2020","title":"DevOps Docker Builds, Version 2007 (July 2020)","text":""},{"location":"release-notes/relnotes-2007/#new-features","title":"New Features","text":"<ul> <li> <p>Signed Docker Images</p> <p>All DockerHub Images are now signed and conform to the Docker Content Trust Specification.</p> </li> <li> <p>Variablize PingAccess Ports</p> <p>We've updated the PingAccess start up hooks to allow users to customize application ports.</p> </li> <li> <p>PingAccess Upgrade Utility</p> <p>The PingAccess upgrade utility is now part of Docker Image.</p> </li> <li> <p>Certificate Management</p> <p>Add consistency and flexibility with the injection of certs/pins.</p> </li> <li> <p>Docker Image Startup Flexibility</p> <p>We've added the ability for end users to customize the startup sequence for Docker Images using pre and post hooks. See our Documentation for implementation details.</p> </li> </ul>"},{"location":"release-notes/relnotes-2007/#improvements","title":"Improvements","text":"<ul> <li> <p>Docker Build Pipeline</p> <p>We've made several CI/CD enhancements to improve Image qualification (smoke/integration tests).</p> </li> </ul>"},{"location":"release-notes/relnotes-2007/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-345) Resolved issue where PingDelegator was using PRIVATE rather than PUBLIC hostnames.</li> <li>(GDO-346) Resolved issue regarding the default minimum heap for PingDirectory.</li> <li>(GDO-380) Resolved issue within PingAccess Clustering (Admin Console) Kubernetes examples.</li> <li>(GDO-371) Resolved issue where PingDelegator wouldn't start using non-privileged user.</li> </ul>"},{"location":"release-notes/relnotes-2008/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2008/#devops-docker-builds-version-2008-august-2020","title":"DevOps Docker Builds, Version 2008 (August 2020)","text":""},{"location":"release-notes/relnotes-2008/#new-features","title":"New Features","text":"<ul> <li> <p>Secret Management</p> <p>A number of key enhancements have been made to natively support secret management within our Docker Images. See Documentation for implementation details.</p> </li> <li> <p>DevOps Development Mode</p> <p>We've added a 'Continue on Failure' option to all Docker Images. This allows the Container to say alive while any potential issues are being investigated.</p> </li> <li> <p>DevOps Program Registration</p> <p>Signing up for the Ping DevOps program is now self-service! Simply follow the instructions found Here.</p> </li> </ul>"},{"location":"release-notes/relnotes-2008/#improvements","title":"Improvements","text":"<ul> <li> <p>Ping-DevOps Utility</p> <p>We've added secret management commands to ping-devops, allowing you to quickly integrate secrets into your deployments.</p> </li> <li> <p>Image Restart State</p> <p>A number of enhancements have been made to improve the overall restart flow in our Docker Images.</p> </li> </ul>"},{"location":"release-notes/relnotes-2008/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-352) Resolved restart issue in PingDataGovernance PAP.</li> <li>(GDO-392) Resolved issue within PingDelegator when DS_PORT variable was undefined.</li> <li>(GDO-395) Resolved issue within PingDirectory restart when Java versions changed.</li> <li>(GDO-397) Resolved issue where PingFederate failed to start in Kubernetes using the full-stack example.</li> <li>(GDO-404) Resolved issue where some users were unable to log into the PingAccess console using the Image edge tag and Baseline server profile.</li> </ul>"},{"location":"release-notes/relnotes-2009/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2009/#devops-docker-builds-version-2009-september-2020","title":"Devops Docker Builds, Version 2009 (September 2020)","text":""},{"location":"release-notes/relnotes-2009/#new-features","title":"New Features","text":"<ul> <li> <p>PingDataSync Clustering</p> <p>Within PingDataSync 8.2.0.0-EA we've introduced clustering, ensuring your deployment is highly available.</p> </li> <li> <p>Certificate Management Usage</p> <p>We've added documentation for DevOps Certificate Management.</p> </li> </ul>"},{"location":"release-notes/relnotes-2009/#pingaccess-release","title":"PingAccess Release","text":"<p>PingAccess 6.1.2 is now available using edge, latest and 2009 image tags</p>"},{"location":"release-notes/relnotes-2009/#product-betas-and-release-candidates","title":"Product Betas and Release Candidates","text":"<p>Looking to see what the next official product release will contain? Start using the beta and early access builds today.</p> <ul> <li>PingFederate 10.2.0-Beta</li> <li>PingAccess 6.2.0-Beta</li> <li>PingDirectory 8.2.0.0-EA</li> <li>PingDirectoryProxy 8.2.0.0-EA</li> <li>PingDataGovernance 8.2.0.0-EA</li> <li>PingDataGovernance 8.2.0.0-EA PAP</li> <li>PingDataSync 8.2.0.0-EA</li> </ul>"},{"location":"release-notes/relnotes-2009/#improvements","title":"Improvements","text":"<ul> <li> <p>Image Hardening</p> <p>We've updated our Image hardening Guide to help secure your production deployments.</p> </li> </ul>"},{"location":"release-notes/relnotes-2010/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2010/#devops-docker-builds-version-2010-october-2020","title":"Devops Docker Builds, Version 2010 (October 2020)","text":""},{"location":"release-notes/relnotes-2010/#new-features","title":"New Features","text":"<ul> <li> <p>PingIdentity Helm Charts</p> <p>Looking to deploy the PingDevOps stack into your Kubernetes cluster? We've published our Helm Charts to help streamline deployment.</p> </li> <li> <p>PingIntelligence (ASE) Docker Image</p> <p>PingIntelligence (ASE) is now available on DockerHub! Pull the 4.3 ASE image Here.</p> </li> <li> <p>PingFederate Bulk API Configuration Management</p> <p>We've added tooling and documentation for managing PingFederate configuration using the build API export and import. View the latest documentation Here.</p> </li> </ul>"},{"location":"release-notes/relnotes-2010/#enhancements","title":"Enhancements","text":"<ul> <li> <p>PingFederate</p> <ul> <li>Version 10.0.6 now available.</li> <li>Image now includes tcp.xml.subst for cluster parameterization.</li> <li>Updated image to support easier enablement/use of Bouncy Castle FIPS provider with PingFederate.</li> </ul> </li> <li> <p>PingAccess</p> <ul> <li>Version 6.1.3 is now available.</li> </ul> </li> <li> <p>LDAP SDK</p> <ul> <li>Updated to version 5.1.1</li> </ul> </li> <li> <p>ping-devops CLI</p> <ul> <li>Added functionality to generate K8s license and version secret directly from the evaluation license service.</li> <li>Added ACCEPT_EULA value to K8s devops-secret.</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2010/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-411) Resolved issue where access token was logged when using private Git repository.</li> <li>(GDO-444) Resolved PingDirectory issue with keystore exception on restart.</li> <li>(GDO-491) Removed GPG from base Docker image.</li> <li>(GDO-495) Removed gosu from base Docker image.</li> <li>(GDO-513) Resolved issue with replication topology list on PingDirectory restart.</li> </ul>"},{"location":"release-notes/relnotes-2011/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2011/#devops-docker-builds-version-2011-november-2020","title":"Devops Docker Builds, Version 2011 (November 2020)","text":""},{"location":"release-notes/relnotes-2011/#new-features","title":"New Features","text":"<ul> <li> <p>Internal XRay Scanning</p> <p>We've automated the process to scan all Sprint Release Docker Images for CVE's</p> </li> </ul>"},{"location":"release-notes/relnotes-2011/#enhancements","title":"Enhancements","text":"<ul> <li> <p>PingFederate</p> <ul> <li>Version 10.1.3 now available.</li> <li>Parameterized run.properties, ldap.properties and tcp.xml now included in Docker Image</li> </ul> </li> <li> <p>Helm Charts</p> <ul> <li>We added a number of enhancements to our Helm charts. See the Helm Release Notes for details.</li> </ul> </li> <li> <p>Misc.</p> <ul> <li>Updated EULA check to be case insensitive</li> <li>Add Java back into pingtoolkit Image</li> <li>Updated example docker run commands in Dockerfile documentation</li> <li>Info message when Server Profile URLs are not present</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2011/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-549) - Resolved issue where SCIM Swagger test pages don't work in PingDataGovernance Docker Image</li> <li>(GDO-567) - Resolved issue where changes made to PingDirectory's java.properties were erased on container restart</li> <li>(GDO-599) - Change wait-for localhost to use IP address</li> <li>(GDO-604) - Modified simple-sync server profile to work in Kubernetes environment with different service names</li> <li>(GDO-606) - Resolved issue where copy of server bits throws errors when running under non-root security context</li> </ul>"},{"location":"release-notes/relnotes-2012/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2012/#devops-docker-builds-version-2012-december-2020","title":"Devops Docker Builds, Version 2012 (December 2020)","text":""},{"location":"release-notes/relnotes-2012/#new-features","title":"New Features","text":"<ul> <li> <p>DevOps Documentation</p> <p>We've moved from GitBook to MKDocs to provide a richer DevOps documentation experience.</p> </li> </ul>"},{"location":"release-notes/relnotes-2012/#enhancements","title":"Enhancements","text":"<ul> <li> <p>PingFederate</p> <ul> <li>Version 10.2 now available.</li> </ul> </li> <li> <p>PingAccess</p> <ul> <li>Version 6.2 is now available.</li> </ul> </li> <li> <p>PingDirectory</p> <ul> <li>Version 8.2.0 is now available.</li> </ul> </li> <li> <p>PingDataGovernance</p> <ul> <li>Version 8.2.0 is now available.</li> </ul> </li> <li> <p>PingDataSync</p> <ul> <li>Version 8.2.0 is now available.</li> </ul> </li> <li> <p>PingCentral</p> <ul> <li>Version 1.6.0 is now available.</li> </ul> </li> <li> <p>LDAP SDK</p> <ul> <li>Version 5.1.3 is now available.</li> <li>Updated to latest Tomcat version.</li> </ul> </li> <li> <p>PingData Console SSO Example</p> <ul> <li>We've provided an example of running the Admin Console in Docker with SSO configured.</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2012/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-362) Resolved issue where PingDirectory instances become active prior to being fully synchronized.</li> <li>(GDO-502) Resolved potential vulnerability by updating Ping Data products to Spring Framework v4.3.29.</li> <li>(GDO-544) Resolved issue where PingDataGovernance PAP images' MAX_HEAP_SIZE variable had no effect.</li> <li>(GDO-618) Resolved issue where base layer was missing JMX agent.</li> <li>(GDO-640) Resolved issue where wait-for command didn't honor timeout when waiting for host:port.</li> </ul>"},{"location":"release-notes/relnotes-2012/#product-build-matrix","title":"Product Build Matrix","text":"<p>The following table includes product versions and their accompanying Image build status for this release.</p> Product Active Build Build EOL PingAccess 6.2.06.1.3 6.0.4 PingCentral 1.6.01.5.0 PingDataConsole 8.2.0.08.1.0.0 8.0.0.1 PingDataGovernance 8.2.0.08.1.0.0 8.0.0.1 PingDataGovernance PAP 8.2.0.08.1.0.0 8.0.0.1 PingDataSync 8.2.0.08.1.0.0 8.0.0.1 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.08.1.0.0 8.0.0.1 PingDirectoryProxy 8.2.0.08.1.0.0 8.0.0.1 PingFederate 10.2.010.1.3 10.1.2 PingIntelligence 4.4 4.3 <p>Build Matrix Info</p> <ul> <li>Bolded product version number is version within 'latest' image tag.</li> <li>Build EOL denotes product versions that are no longer built as of this release.</li> </ul>"},{"location":"release-notes/relnotes-2101/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2101/#devops-docker-builds-version-2101-january-2021","title":"DevOps Docker Builds, Version 2101 (January 2021)","text":""},{"location":"release-notes/relnotes-2101/#enhancements","title":"Enhancements","text":"<ul> <li> <p>PingFederate</p> <ul> <li>Versions 10.2.1 and 10.1.4 are now available.</li> </ul> </li> <li> <p>PingDirectory</p> <ul> <li>Versions 8.2.0.1 and 8.1.0.3 are now available.</li> <li>PingDirectory now delays its readiness state until replication has completed (Kubernetes).</li> <li>Improved container restart time by regenerating java.properties only when changes are made to JVM or JVM options.</li> </ul> </li> <li> <p>PingDataGovernance</p> <ul> <li>Versions 8.2.0.1 and 8.1.0.3 are now available.</li> </ul> </li> <li> <p>PingDataSync</p> <ul> <li>Versions 8.2.0.1 and 8.1.0.3 are now available.</li> </ul> </li> <li> <p>PingDelegator 4.4.1</p> <ul> <li>Version 4.4.1 is now available.</li> </ul> </li> <li> <p>LDAP SDK</p> <ul> <li>Version 5.1.3 is now available.</li> </ul> </li> <li> <p>Container Secrets</p> <ul> <li>Sourcing of secret_envs is now recursive.</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2101/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-577) - Resolved issue to suppress environment variables in cn=monitor for PingData products.</li> <li>(GDO-658) - Enhanced error messages returned by the evaluation license service.</li> <li>(GDO-659) - Resolved issue where evaluation license server used incorrect calculation for checking image expiration.</li> <li>(GDO-668) - Resolved issue where remnants of previous server profile remained in place when restarting a container.</li> <li>(GDO-674) - Resolved issue where hashing contents of the SECRETS_DIR risked leaving passwords stored insecurely on the container filesystem.</li> </ul>"},{"location":"release-notes/relnotes-2101/#product-build-matrix","title":"Product Build Matrix","text":"<p>The following table includes product versions and their accompanying Image build status for this release.</p> Product Active Build Build EOL PingAccess 6.2.06.1.3 PingCentral 1.6.01.5.0 PingDataConsole 8.2.0.18.1.0.3 8.2.0.08.1.0.0 PingDataGovernance 8.2.0.18.1.0.3 8.2.0.08.1.0.0 PingDataGovernance PAP 8.2.0.18.1.0.3 8.2.0.08.1.0.0 PingDataSync 8.2.0.18.1.0.3 8.2.0.08.1.0.0 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.18.1.0.3 8.2.0.08.1.0.0 PingDirectoryProxy 8.2.0.18.1.0.3 8.2.0.08.1.0.0 PingFederate 10.2.110.1.4 10.2.0 10.1.3 PingIntelligence 4.4 <p>Build Matrix Info</p> <ul> <li>Bolded product version number is version within 'latest' image tag.</li> <li>Build EOL denotes product versions that are no longer built as of this release.</li> </ul>"},{"location":"release-notes/relnotes-2102/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2102/#devops-docker-builds-version-2102-february-2021","title":"DevOps Docker Builds, Version 2102 (February 2021)","text":""},{"location":"release-notes/relnotes-2102/#enhancements","title":"Enhancements","text":"<ul> <li> <p>PingFederate</p> <ul> <li>Support for creation and loading of certificates for admin.</li> <li>Version 10.2.2 is now available.</li> </ul> </li> <li> <p>PingAccess</p> <ul> <li>Baseline now has clustering support.</li> <li>Version 6.1.4 is now available.</li> </ul> </li> <li> <p>PingDirectory</p> <ul> <li>Improve speed of replace-profile process during PingDirectory restart.</li> <li>Indexes are automatically rebuilt upon server restart.</li> <li>Version 8.2.0.2 is now available.</li> </ul> </li> <li> <p>PingDataGovernance</p> <ul> <li>Helm charts have been added for the PingDataGovernance policy editor.</li> <li>Version 8.2.0.2 is now available.</li> </ul> </li> <li> <p>PingDataSync</p> <ul> <li>Version 8.2.0.2 is now available.</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2102/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-382) - Resolved issue where PingDirectory is unable to restart when upgrading 7.3 to 8.1 due to a license error.</li> <li>(GDO-543) - Updated \"Related Docker Images\" documentation in PAP Dockerfile.</li> <li>(GDO-672) - Resolved issue with 'manage-profile setup' signaling a dsconfig error.</li> <li>(GDO-680) - Resolved issue with PingDirectory set_server_available and set_server_unavailable methods being very.</li> <li>(GDO-311) - Updated 05-expand-templates.sh to no longer build data.zip if a data.zip directory is found in the profile.</li> </ul>"},{"location":"release-notes/relnotes-2102/#product-build-matrix","title":"Product Build Matrix","text":"<p>The following table includes product versions and their accompanying Image build status for this release.</p> Product Active Build Build EOL PingAccess 6.2.06.1.4 6.1.3 PingCentral 1.6.01.5.0 PingDataConsole 8.2.0.28.1.0.3 8.2.0.1 PingDataGovernance 8.2.0.28.1.0.3 8.2.0.1 PingDataGovernance PAP 8.2.0.28.1.0.3 8.2.0.1 PingDataSync 8.2.0.28.1.0.3 8.2.0.1 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.28.1.0.3 8.2.0.1 PingDirectoryProxy 8.2.0.28.1.0.3 8.2.0.1 PingFederate 10.2.210.1.4 10.2.1  PingIntelligence 4.4 <p>Build Matrix Info</p> <ul> <li>Bolded product version number is version within 'latest' image tag.</li> <li>Build EOL denotes product versions that are no longer built as of this release.</li> </ul>"},{"location":"release-notes/relnotes-2103/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2103/#devops-docker-builds-version-2103-march-2021","title":"DevOps Docker Builds, Version 2103 (March 2021)","text":""},{"location":"release-notes/relnotes-2103/#new-features","title":"New Features","text":"<ul> <li> <p>Images run as non-privileged user by default</p> <p>Critical: We've greatly improved the security of our images by having them run as a non-privileged user by default.  See Migrating to Unprivileged Images for information about migrating existing deployments.</p> </li> <li> <p>Layer simplification</p> <p>We've consolidated layers in our images where possible.</p> </li> </ul>"},{"location":"release-notes/relnotes-2103/#enhancements","title":"Enhancements","text":"<ul> <li> <p>PingFederate</p> <ul> <li>The baseline image now uses data.json instead of the former use of the /data folder.</li> <li>New variables have been added to run.properties for controlling provisioning failover and grace period.</li> <li>Versions 10.1.5 and 10.3-Beta are now available.</li> </ul> </li> <li> <p>PingAccess</p> <ul> <li>Versions 6.2.1 and 6.3-Beta are now available.</li> </ul> </li> <li> <p>PingCentral</p> <ul> <li>Versions 1.7.0 is now available.</li> </ul> </li> <li> <p>PingDirectory</p> <ul> <li>The number of layers present in the image has been reduced and simplified.</li> <li>Version 8.2.0.3 is now available.</li> </ul> </li> <li> <p>PingDataGovernance</p> <ul> <li>Version 8.2.0.3 is now available.</li> </ul> </li> <li> <p>PingDataSync</p> <ul> <li>Version 8.2.0.3 is now available.</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2103/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-742) - Resolved issue which may cause permissions errors creating files under /run/secrets during PingDirectory setup</li> <li>(GDO-746) - Resolved issue in which PingDirectory cannot rejoin its replication topology after restart</li> <li>(GDO-749) - Addressed documentation issue in which bulleted lists are not printed correctly</li> </ul>"},{"location":"release-notes/relnotes-2103/#product-build-matrix","title":"Product Build Matrix","text":"<p>The following table includes product versions and their accompanying Image build status for this release.</p> Product Active Build Build EOL PingAccess 6.2.16.1.4 6.2.0 PingCentral 1.7.01.6.0 1.5.0 PingDataConsole 8.2.0.38.1.0.3 8.2.0.2 PingDataGovernance 8.2.0.38.1.0.3 8.2.0.2 PingDataGovernance PAP 8.2.0.38.1.0.3 8.2.0.2 PingDataSync 8.2.0.38.1.0.3 8.2.0.2 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.38.1.0.3 8.2.0.2 PingDirectoryProxy 8.2.0.38.1.0.3 8.2.0.2 PingFederate 10.2.210.1.5 10.1.4  PingIntelligence 4.4 <p>Build Matrix Info</p> <ul> <li>Bolded product version number is version within 'latest' image tag.</li> <li>Build EOL denotes product versions that are no longer built as of this release.</li> </ul>"},{"location":"release-notes/relnotes-2104/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2104/#devops-docker-builds-version-2104-april-2021","title":"DevOps Docker Builds, Version 2104 (April 2021)","text":""},{"location":"release-notes/relnotes-2104/#new-features","title":"New Features","text":"<ul> <li>Early Access and Beta Release Docker Images<ul> <li>PingAccess 6.3.0-Beta</li> <li>PingAuthorize 8.3.0.0-EA</li> <li>PingAuthorize PAP 8.3.0.0-EA</li> <li>PingDataConsole 8.3.0.0-EA</li> <li>PingDataSync 8.3.0.0-EA</li> <li>PingDirectory 8.3.0.0-EA</li> <li>PingDirectoryProxy 8.3.0.0-EA</li> <li>PingFederate 10.3.0-Beta</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2104/#enhancements","title":"Enhancements","text":"<ul> <li> <p><code>watch-fs-changes</code></p> <ul> <li>We've updated the <code>watch-fs-changes</code> utility to accept command-line parameters to watch additional locations</li> </ul> </li> <li> <p>Startup Time Performance</p> <ul> <li>We've updated the start-server.sh script to improve container start up times for all PingData products.</li> </ul> </li> <li> <p>Helm Charts for PingDirectoryProxy</p> <ul> <li>PingDirectoryProxy has been integrated into Ping's Helm Charts</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2104/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-649) - Resolved issue where the provided self-signed certificates for PingDataConsole didn't function in Chrome on MacOS</li> <li>(GDO-770) - Resolved issue where PingDataConsole didn't log console messages by default</li> <li>(GDO-773) - Resolved issue where the collect-support-data tool couldn't find the required JDK</li> </ul>"},{"location":"release-notes/relnotes-2104/#product-build-matrix","title":"Product Build Matrix","text":"<p>The following table includes product versions and their accompanying Image build status for this release.</p> Product Active Build Build EOL PingAccess 6.2.16.1.4 PingCentral 1.7.01.6.0 PingDataConsole 8.2.0.38.1.0.3 PingDataGovernance 8.2.0.38.1.0.3 PingDataGovernance PAP 8.2.0.38.1.0.3 PingDataSync 8.2.0.38.1.0.3 PingDelegator 4.4.0 PingDirectory 8.2.0.38.1.0.3 PingDirectoryProxy 8.2.0.38.1.0.3 PingFederate 10.2.210.1.5 PingIntelligence 4.4 <p>Build Matrix Info</p> <ul> <li>Bolded product version number is version within 'latest' image tag.</li> <li>Build EOL denotes product versions that are no longer built as of this release.</li> </ul>"},{"location":"release-notes/relnotes-2105/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2105/#devops-docker-builds-version-2105-june-3-2021","title":"DevOps Docker Builds, Version 2105 (June 3 2021)","text":""},{"location":"release-notes/relnotes-2105/#new-features","title":"New Features","text":"<ul> <li> <p>PingFederate</p> <ul> <li>PingFederate 10.2.3 is now available on Dockerhub</li> </ul> </li> <li> <p>PingDelegator</p> <ul> <li>PingDelegator 4.5.0 is now available on Dockerhub</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2105/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-813) - Resolved issue where OAuth APIS were broken using baseline server profile and pingfederate:edge</li> <li>(GDO-818) - Resolved issue where users were unable to build images locally due to a file permission error</li> <li>(GDO-829) - Resolved issue where a <code>dsconfig</code> command was unable to run due to a quoting error</li> </ul>"},{"location":"release-notes/relnotes-2105/#product-build-matrix","title":"Product Build Matrix","text":"<p>The following table includes product versions and their accompanying Image build status for this release.</p> Product Active Build Build EOL PingAccess 6.3.0-Beta6.2.16.1.4 PingCentral 1.7.01.6.0 PingDataConsole 8.3.0.0-EA8.2.0.38.1.0.3 PingDataGovernance 8.2.0.38.1.0.3 PingDataGovernance PAP 8.2.0.38.1.0.3 PingDataSync 8.3.0.0-EA8.2.0.38.1.0.3 PingDelegator 4.5.04.4.14.2.1 PingDirectory 8.3.0.0-EA8.2.0.48.2.0.38.1.0.3 PingDirectoryProxy 8.3.0.0-EA8.2.0.38.1.0.3 PingFederate 10.3.0-Beta10.2.310.1.5 10.2.2 PingIntelligence 4.4 <p>Build Matrix Info</p> <ul> <li>Bolded product version number is version within 'latest' image tag.</li> <li>Build EOL denotes product versions that are no longer built as of this release.</li> </ul>"},{"location":"release-notes/relnotes-2106/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2106/#devops-docker-builds-version-2106-july-6-2021","title":"DevOps Docker Builds, Version 2106 (July 6 2021)","text":""},{"location":"release-notes/relnotes-2106/#new-features","title":"New Features","text":"<ul> <li> <p>ARM-Based Images</p> <ul> <li>Ping Identity now offers ARM-based Docker images!</li> <li>These images are currently experimental and are not intended for production deployment</li> <li>View the available tags on Dockerhub</li> </ul> </li> <li> <p>PingFederate</p> <ul> <li>PingFederate 10.3.0 and 10.2.4 are now available on Dockerhub</li> </ul> </li> <li> <p>PingAccess</p> <ul> <li>PingAccess 6.3.0 is now available on Dockerhub</li> </ul> </li> <li> <p>PingDirectory</p> <ul> <li>PingDirectory 8.3.0 and 8.2.0.5 are now available on Dockerhub</li> </ul> </li> <li> <p>PingAuthorize</p> <ul> <li>PingAuthorize 8.3.0 is now available on Dockerhub</li> </ul> </li> <li> <p>PingCentral</p> <ul> <li>PingCentral 1.8 is now available on Dockerhub</li> </ul> </li> <li> <p>PingDelegator</p> <ul> <li>PingDelegator 4.6.0 is now available on Dockerhub</li> </ul> </li> <li> <p>PingIntelligence (ASE)</p> <ul> <li>PingIntelligence 5.0 is now available on Dockerhub</li> </ul> </li> <li> <p>LDAP SDK</p> <ul> <li>LDAP SDK 6.0.0 is now available on Dockerhub</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2106/#enhancements","title":"Enhancements","text":"<ul> <li> <p>PingFederate</p> <ul> <li>Allow logging level to be set via an environment variable (PF_LOG_LEVEL)</li> <li>Added property <code>pf.admin.baseurl</code> to run.properties configuration file</li> <li>Added ability to generate the run.properties and jvm-memory.options files based on supplied environment variables</li> </ul> </li> <li> <p>HEAP Awareness</p> <ul> <li>All Ping Identity Docker images can now calculate the heap based on the memory allocated to the container</li> </ul> </li> <li> <p>Java Tools</p> <ul> <li>Added jcmd, jstat, jinfo, jmap, jps, jstack tools to images</li> </ul> </li> <li> <p>Docker-Compose</p> <ul> <li>Added tmpfs secrets directory to all of the docker-compose examples in the Getting-Started repository</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2106/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-657) - Resolved PingDelegator self-signed certificate issue</li> <li>(GDO-834) - Resolved issue where PingDataConsole doesn't build correctly when providing a local product.zip file</li> <li>(GDO-836) - Resolved issue where PingDirectory restart failed due to startup hook syntax error</li> <li>(GDO-885) - Resolved HTTPS/LDAPS port variables in PingAuthorize profiles to support Helm charts</li> </ul>"},{"location":"release-notes/relnotes-2106/#product-build-matrix","title":"Product Build Matrix","text":"<p>The following table includes product versions and their accompanying Image build status for this release.</p> Product Active Build Build EOL PingAccess 6.3.06.2.1 6.3.0-Beta6.1.4 PingAuthorize 8.3.0.0 PingAuthorize PAP 8.3.0.0 PingCentral 1.8.01.7.0 1.6.0 PingDataConsole 8.3.0.08.2.0.5 8.3.0.0-EA8.2.0.38.1.0.3 PingDataGovernance 8.2.0.5 8.2.0.38.1.0.3 PingDataGovernance PAP 8.2.0.5 8.2.0.38.1.0.3 PingDataSync 8.3.0.08.2.0.5 8.3.0.0-EA8.2.0.38.1.0.3 PingDelegator 4.6.04.4.1 4.5.04.4.14.2.1 PingDirectory 8.3.0.08.2.0.5 8.3.0.0-EA8.2.0.38.1.0.3 PingDirectoryProxy 8.3.0.08.2.0.5 8.3.0.0-EA8.2.0.38.1.0.3 PingFederate 10.3.010.2.4 10.3.0-Beta10.2.310.1.5 PingIntelligence 5.04.4.1 4.4 <p>Build Matrix Info</p> <ul> <li>Bolded product version number is version within 'latest' image tag.</li> <li>Build EOL denotes product versions that are no longer built as of this release.</li> </ul>"},{"location":"release-notes/relnotes-2107/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2107/#devops-docker-builds-version-2107-august-4-2021","title":"DevOps Docker Builds, Version 2107 (August 4 2021)","text":""},{"location":"release-notes/relnotes-2107/#new-features","title":"New Features","text":"<ul> <li> <p>PingFederate</p> <ul> <li>Added support for pf.admin.baseurl within baseline Server Profile</li> </ul> </li> <li> <p>PingAccess</p> <ul> <li>PingAccess 6.2.2 is now available on Dockerhub</li> </ul> </li> <li> <p>PingDirectory</p> <ul> <li>PingDirectory 8.3.0.1 is now available on Dockerhub</li> </ul> </li> <li> <p>PingDirectoryProxy</p> <ul> <li>PingDirectoryProxy 8.3.0.1 is now available on Dockerhub</li> </ul> </li> <li> <p>PingDataSync</p> <ul> <li>PingDirectory 8.3.0.1 is now available on Dockerhub</li> </ul> </li> <li> <p>PingAuthorize</p> <ul> <li>PingAuthorize 8.3.0.1 is now available on Dockerhub</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2107/#enhancements","title":"Enhancements","text":"<ul> <li> <p>PingDelegator</p> <ul> <li>Baseline now works on both local and Kubernetes environments</li> </ul> </li> <li> <p>Helm Charts</p> <ul> <li>Release 0.6.8 - Probes &amp; Ingress</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2107/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-860) - Resolved issue where the PingAuthorize Policy Editor  auto-generated documentation uses wrong ports</li> <li>(GDO-907) - Restored functionality for prepending the name of the log file to each log line</li> <li>(GDO-887) - All Docker images are now signed</li> </ul>"},{"location":"release-notes/relnotes-2107/#product-build-matrix","title":"Product Build Matrix","text":"<p>The following table includes product versions and their accompanying Image build status for this release.</p> Product Active Build Build EOL PingAccess 6.3.06.2.2 6.2.1 PingAuthorize 8.3.0.1 8.3.0.0 PingAuthorize PAP 8.3.0.1 8.3.0.0 PingCentral 1.8.01.7.0 PingDataConsole 8.3.0.18.2.0.5 8.3.0.0 PingDataGovernance 8.2.0.5 PingDataGovernance PAP 8.2.0.5 PingDataSync 8.3.0.18.2.0.5 8.3.0.0 PingDelegator 4.6.04.4.1 PingDirectory 8.3.0.18.2.0.5 8.3.0.0 PingDirectoryProxy 8.3.0.18.2.0.5 8.3.0.0 PingFederate 10.3.010.2.4 PingIntelligence 5.04.4.1 <p>Build Matrix Info</p> <ul> <li>Bolded product version number is version within 'latest' image tag.</li> <li>Build EOL denotes product versions that are no longer built as of this release.</li> </ul>"},{"location":"release-notes/relnotes-2108/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2108/#devops-docker-builds-version-2108-august-27-2021","title":"DevOps Docker Builds, Version 2108 (August 27 2021)","text":""},{"location":"release-notes/relnotes-2108/#new-features","title":"New Features","text":"<ul> <li> <p>PingFederate</p> <ul> <li>PingFederate 10.3.1 and 10.2.5 are now available on Dockerhub</li> </ul> </li> <li> <p>PingAccess</p> <ul> <li>PingAccess 6.3.1 is now available on Dockerhub</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2108/#enhancements","title":"Enhancements","text":"<ul> <li> <p>Documentation</p> <ul> <li>Our DevOps documentation now supports both light and dark modes. Toggle between the two by clicking the icon in the top navigation bar.</li> </ul> </li> <li> <p>Docker Images</p> <ul> <li>Upgraded the Image OS from Alpine 3.13 to 3.14</li> </ul> </li> <li> <p>Helm Charts</p> <ul> <li>View the detailed release notes for Ping's Helm Charts here<ul> <li>Release 0.7.0 - ServiceAccount/Role/RoleBinding for testFramework</li> <li>Release 0.7.1 - Public hostname/ports</li> <li>Release 0.7.2 - PingFederate PF_ADMIN_PUBLIC_BASEURL variable</li> <li>Release 0.7.3 - Support full definition of initContainers attributes in testSteps and finalStep</li> <li>Release 0.7.4 - Set initContainer settings from values.yaml instead of hard coded templates</li> </ul> </li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2108/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-945) - Resolved issue where PingCentral was unable to communicate with PingAccess in the docker-compose full-stack example.</li> <li>(GDO-872) - Resolved issue in tooling when building images locally (<code>serial_build.sh</code>).</li> </ul>"},{"location":"release-notes/relnotes-2108/#product-build-matrix","title":"Product Build Matrix","text":"<p>The following table includes product versions and their accompanying Image build status for this release.</p> Product Active Build Build EOL PingAccess 6.3.16.2.2 6.3.0 PingAuthorize 8.3.0.1 PingAuthorize PAP 8.3.0.1 PingCentral 1.8.01.7.0 PingDataConsole 8.3.0.18.2.0.5 PingDataGovernance 8.2.0.5 PingDataGovernance PAP 8.2.0.5 PingDataSync 8.3.0.18.2.0.5 PingDelegator 4.6.04.4.1 PingDirectory 8.3.0.18.2.0.5 PingDirectoryProxy 8.3.0.18.2.0.5 PingFederate 10.3.110.2.5 10.3.010.2.4 PingIntelligence 5.04.4.1 <p>Build Matrix Info</p> <ul> <li>Bolded product version number is version within 'latest' image tag.</li> <li>Build EOL denotes product versions that are no longer built as of this release.</li> </ul>"},{"location":"release-notes/relnotes-2109/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2109/#devops-docker-builds-version-2109-october-06-2021","title":"DevOps Docker Builds, Version 2109 (October 06 2021)","text":"<p>Notice</p> <ul> <li>PingFederate deployments prior to sprint release 2108 (Aug 27th, 2021) may be at risk. Please visit here for details on impacted and patched versions.</li> </ul>"},{"location":"release-notes/relnotes-2109/#new-features","title":"New Features","text":"<ul> <li> <p>PingFederate</p> <ul> <li>PingFederate 10.3.2 and 11.0 Beta are now available on Dockerhub</li> </ul> </li> <li> <p>PingAccess</p> <ul> <li>PingAccess 6.3.1 and 7.0 Beta are now available on Dockerhub</li> </ul> </li> <li> <p>PingDirectory</p> <ul> <li>PingDirectory 8.3.0.2, 8.2.0.6, and 9.0 EA are now available on Dockerhub</li> </ul> </li> <li> <p>PingAuthorize</p> <ul> <li>PingAuthorize 8.3.0.2, 8.2.0.6, and 9.0 EA are now available on Dockerhub</li> </ul> </li> <li> <p>PingIntelligence</p> <ul> <li>PingIntelligence 5.0.1 is now available on Dockerhub</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2109/#enhancements","title":"Enhancements","text":"<ul> <li> <p>Documentation</p> <ul> <li>Improved Introduction to Image/Container anatomy</li> </ul> </li> <li> <p>Docker Images</p> <ul> <li>JDK Liberica 11.0.12+7 is now supported</li> <li>Images now include a startup probe script /opt/startup.sh</li> </ul> </li> <li> <p>Helm Charts</p> <ul> <li>View the detailed release notes for Ping's Helm Charts here<ul> <li>Release 0.7.6  - Support for scheduler name on pods</li> </ul> </li> </ul> </li> <li> <p>Kubernetes</p> <ul> <li>PingDirectory now waits for its pod DNS hostname to match expected K8 pod IP</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2109/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(GDO-896) - Resolved issue where PingDirectory failed to pick up the product license during deployment</li> <li>(GDO-989) - Resolved issue in which PingDirectory seed failure in multi-region topology causes a replication island</li> </ul>"},{"location":"release-notes/relnotes-2109/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2110/","title":"Release Notes","text":""},{"location":"release-notes/relnotes-2110/#devops-docker-builds-version-2110-november-01-2021","title":"DevOps Docker Builds, Version 2110 (November 01 2021)","text":""},{"location":"release-notes/relnotes-2110/#new-features","title":"New Features","text":"<ul> <li> <p>PingFederate</p> <ul> <li>PingFederate 10.3.3 and 10.2.7 are now available on Dockerhub.</li> </ul> </li> <li> <p>PingDirectory</p> <ul> <li>PingDirectory 8.3.0.3 is now available on Dockerhub.</li> </ul> </li> <li> <p>PingAuthorize</p> <ul> <li>PingAuthorize 8.3.0.3 is now available on Dockerhub.</li> </ul> </li> <li> <p>PingCentral</p> <ul> <li>PingCentral 1.9 is now available on Dockerhub.</li> </ul> </li> <li> <p>UnboundID LDAP SDK</p> <ul> <li>UnboundID LDAP SDK tool set 6.0.2 is now available on Dockerhub.</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2110/#enhancements","title":"Enhancements","text":"<ul> <li> <p>Documentation</p> <ul> <li>Improved documenation around certificate rotation for PingDirectory.</li> <li>Update DevOps support policy statement.</li> </ul> </li> <li> <p>Docker Images</p> <ul> <li>Images that include Apache Tomcat have been updated to 9.0.54.</li> <li>Startup time for PingDirectory has been improved.</li> <li>PF_LDAP_USERNAME and PF_LDAP_PASSWORD variables are now required with PingFederate to promote best security practices.</li> </ul> </li> <li> <p>Helm Charts</p> <ul> <li>View the detailed release notes for Ping's Helm Charts here<ul> <li>Release 0.7.7 - Update default security context group id to root.</li> <li>Release 0.7.8 - Server profile updates, generate master password for Ping services.</li> </ul> </li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2110/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-72) - Resolved issue in which numbers were not rendered correctly in some cases in public docs.</li> <li>(BRASS-71) - Resolved issue in which PingDirectory seed name is not rendered correctly.</li> </ul>"},{"location":"release-notes/relnotes-2110/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2111-1/","title":"Version 2111.1 Release Notes","text":""},{"location":"release-notes/relnotes-2111-1/#devops-docker-builds-version-21111-december-16-2021","title":"DevOps Docker Builds, Version 2111.1 (December 16 2021)","text":""},{"location":"release-notes/relnotes-2111-1/#new-product-releases","title":"New Product Releases","text":"<ul> <li> <p>PingAccess</p> <ul> <li>PingAccess 7.0.1 is now available on Dockerhub.</li> </ul> </li> <li> <p>PingCentral</p> <ul> <li>PingCentral 1.8.1 is available on Dockerhub.</li> </ul> </li> <li> <p>PingFederate</p> <ul> <li>PingFederate 11.0.0 is available on Dockerhub.</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2111-1/#enhancements","title":"Enhancements","text":"<ul> <li>Docker Images<ul> <li>Applied the log4j2 patch updated zip files to PingAccess and PingFederate per recommendation of the Ping Identity CVE knowledge article.</li> <li>The applied patches are available on the Ping Identity CVE knowledge article.</li> <li>All images tagged with the sprint 2111.1 do not contain the Log4j2 vulnerability CVE-2021-44228.</li> <li>Purged all DockerHub images vulnerable to the Log4j2 vulnerability CVE-2021-44228. This is to ensure all PingIdentity images published do not have the Log4j2 vulnerabilities.</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2111-1/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2111/","title":"Version 2111 Release Notes","text":""},{"location":"release-notes/relnotes-2111/#devops-docker-builds-version-2111-december-06-2021","title":"DevOps Docker Builds, Version 2111 (December 06 2021)","text":""},{"location":"release-notes/relnotes-2111/#new-product-releases","title":"New Product Releases","text":"<ul> <li> <p>PingFederate</p> <ul> <li>PingFederate 10.3.4 is available on Dockerhub.</li> </ul> </li> <li> <p>PingAccess</p> <ul> <li>PingAccess 6.3.2 is now available on Dockerhub.</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2111/#enhancements","title":"Enhancements","text":"<ul> <li> <p>Tools</p> <ul> <li>The pingctl tool is now POSIX compliant in preparation for the pingdevops tool's deprecation.</li> </ul> </li> <li> <p>Docker Images</p> <ul> <li>PingDirectory, PingDirectoryProxy, and PingDataSync will now start without a server profile.</li> <li>Update JDK to 11.0.13+8.</li> <li>Azul JVM has been deprecated in Favor of Liberica JVM.</li> <li>PingData images are now killed with a TERM signal.</li> <li>Update Apache Tomcat to Version 9.0.55.</li> <li>Update Alpine to 3.15 and UBI8 to 8.5.</li> <li>PingFederate PF_LDAP_USERNAME and PF_LDAP_PASSWORD variables are no longer required by default.</li> </ul> </li> <li> <p>Helm Charts</p> <ul> <li>View the detailed release notes for Ping's Helm Charts here.<ul> <li>Release 0.7.9<ul> <li>Support for HPA Scaling Behavior</li> <li>Support for shareProcessNamespace in pod spec</li> <li>Helm test image pull policy no longer hard-coded in helm-charts/charts/ping-devops/templates/pinglib/_tests/tpl</li> <li>Cluster service for pingaccess-admin</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2111/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-80) - 07-apply-server-profile hook now handles PingDirectory restart correctly.</li> <li>(BRASS-172) - Default values have been added for PF_LDAP_USERNAME and PF_LDAP_PASSWORD to work around startup errors for PingFederate images.</li> </ul>"},{"location":"release-notes/relnotes-2111/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2112/","title":"Version 2112 Release Notes","text":""},{"location":"release-notes/relnotes-2112/#devops-docker-builds-version-2112-january-05-2022","title":"DevOps Docker Builds, Version 2112 (January 05 2022)","text":""},{"location":"release-notes/relnotes-2112/#new-product-releases","title":"New Product Releases","text":"<ul> <li> <p>PingFederate</p> <ul> <li>PingFederate 11.0 is now available on Dockerhub.</li> </ul> </li> <li> <p>PingAccess</p> <ul> <li>PingAccess 7.0 is now available on Dockerhub.</li> </ul> </li> <li> <p>PingDirectory</p> <ul> <li>PingDirectory 9.0 is now available on Dockerhub.</li> </ul> </li> <li> <p>PingDelegator</p> <ul> <li>PingDelegator 4.8 is now available on Dockerhub.</li> </ul> </li> <li> <p>PingDirectoryProxy</p> <ul> <li>PingDirectoryProxy 9.0 is now available on Dockerhub.</li> </ul> </li> <li> <p>PingDataSync</p> <ul> <li>PingDataSync 9.0 is now available on Dockerhub.</li> </ul> </li> <li> <p>PingAuthorize</p> <ul> <li>PingAuthorize 9.0 is now available on Dockerhub.</li> </ul> </li> <li> <p>PingIntelligence</p> <ul> <li>PingIntelligence 5.1 is now available on Dockerhub.</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2112/#enhancements","title":"Enhancements","text":"<ul> <li> <p>Docker Images</p> <ul> <li>Apache Tomcat to Version 9.0.56</li> <li>jmeter image to 5.4.3</li> <li>LDAP SDK to 6.0.3</li> <li>PingAccess to log4j 2.12.2, 2.12.3 patch</li> <li>Set UNBOUNDID_SKIP_START_PRECHECK_NODETACH environment variable to true for PingData</li> </ul> </li> <li> <p>Helm Charts     #### Release 0.8.3 ####</p> <ul> <li>Features<ul> <li>Document supported values</li> </ul> </li> <li>Issues Resolved<ul> <li>Issue #233 Ingress - semverCompare now retrieves correct K8 version for applying the correct apiVersion <pre><code>{{- if semverCompare \"&gt;=1.19.x\" $top.Capabilities.KubeVersion.Version }}\n</code></pre></li> <li>Issue #254 Update default global.image.tag to 2112</li> </ul> </li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2112/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-60) - Bulk Config Tool Document has been deprecated. Building a PingFederate profile has taken its place.</li> <li>(BRASS-181) - Update to PingDirectory liveness and readiness probes to use     timeoutSeconds 5 and failureThreshold 3. Update to PingDirectory readiness probes to use readiness.sh.</li> </ul>"},{"location":"release-notes/relnotes-2112/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2201/","title":"Version 2201 Release Notes","text":""},{"location":"release-notes/relnotes-2201/#devops-docker-builds-version-2201-february-07-2022","title":"DevOps Docker Builds, Version 2201 (February 07 2022)","text":""},{"location":"release-notes/relnotes-2201/#new-product-releases","title":"New Product Releases","text":"<ul> <li> <p>PingFederate</p> <ul> <li>PingFederate 11.0.1 is now available on Dockerhub.</li> </ul> </li> <li> <p>PingAccess</p> <ul> <li>PingAccess 6.3.3 is now available on Dockerhub.</li> </ul> </li> <li> <p>PingDirectory</p> <ul> <li>PingDirectory 8.3.0.5 is now available on Dockerhub.</li> </ul> </li> <li> <p>PingDataConsole</p> <ul> <li>PingDataConsole 8.3.0.5 is now available on Dockerhub.</li> </ul> </li> <li> <p>PingDirectoryProxy</p> <ul> <li>PingDirectoryProxy 8.3.0.5 is now available on Dockerhub.</li> </ul> </li> <li> <p>PingDataSync</p> <ul> <li>PingDataSync 8.3.0.5 is now available on Dockerhub.</li> </ul> </li> <li> <p>PingAuthorize</p> <ul> <li>PingAuthorize 8.3.0.5 is now available on Dockerhub.</li> </ul> </li> <li> <p>PingAuthorizePAP</p> <ul> <li>PingAuthorizePAP 8.3.0.5 is now available on Dockerhub.</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2201/#enhancements","title":"Enhancements","text":"<ul> <li> <p>Docker Images</p> <ul> <li>Apache Tomcat to Version 9.0.58</li> <li>Liberica JDK to 11.0.14+9</li> </ul> </li> <li> <p>Helm Charts     #### Release 0.8.5 ####</p> <ul> <li>Features</li> <li>PingCentral now supported. Example values application found here</li> <li>Issues Resolved</li> <li>Issue #119 Workload template not honoring false values from values.yaml. Previously, false did not overwrite true in the Ping Identity Helm Chart template. This fix in _merge-util.tpl will resolve multiple cases within the Ping Identity Helm Chart.     <pre><code>{{- $globalValues := deepCopy $top.Values.global -}}\n{{- $prodValues := deepCopy (index $top.Values $prodName) -}}\n{{- $mergedValues := mergeOverwrite $globalValues $prodValues -}}\n</code></pre></li> <li>Issue #264 Update default global.image.tag to 2201</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2201/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-315) - PingFederate server profiles in getting-started and baseline no longer contain an invalid runtime certificate</li> </ul>"},{"location":"release-notes/relnotes-2201/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2202/","title":"Version 2202 Release Notes","text":""},{"location":"release-notes/relnotes-2202/#devops-docker-builds-version-2202-march-03-2022","title":"DevOps Docker Builds, Version 2202 (March 03 2022)","text":""},{"location":"release-notes/relnotes-2202/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingCentral<ul> <li>PingCentral 1.9.3 is now available on Dockerhub.</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2202/#enhancements","title":"Enhancements","text":"<ul> <li> <p>Docker Images</p> <ul> <li>Apache Tomcat to Version 9.0.59</li> <li>Liberica JDK to 11.0.14.1+1</li> </ul> </li> <li> <p>Documentation</p> <ul> <li>DevOps Getting Started GitHub Repo has been updated<ul> <li>Complex Docker Compose examples deprecated and removed</li> </ul> </li> </ul> </li> <li> <p>Helm Charts Release 0.8.6</p> <ul> <li>Issues Resolved</li> <li>Update default global.image.tag to 2202</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2202/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2203/","title":"Version 2203 Release Notes","text":""},{"location":"release-notes/relnotes-2203/#devops-docker-builds-version-2203-april-01-2022","title":"DevOps Docker Builds, Version 2203 (April 01 2022)","text":""},{"location":"release-notes/relnotes-2203/#new-product-releases","title":"New Product Releases","text":"<ul> <li> <p>PingFederate</p> <ul> <li>PingFederate 11.0.2 is now available on Dockerhub.</li> </ul> </li> <li> <p>Documentation</p> <ul> <li>Helm and Kustomize Documents added DevOps Getting Started GitHub Repo has been updated<ul> <li>20-kubernetes directory has been renamed to 20-kustomize, as well as kustomize examples reduced</li> <li>30-helm directory added with examples included</li> </ul> </li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2203/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-313) - Update docs and pingdataconsole server profile(s) for breaking application.yaml change between 8.3 and 9.0</li> </ul>"},{"location":"release-notes/relnotes-2203/#helm-chart-releases","title":"Helm Chart Releases","text":"<ul> <li>Release 0.9.0</li> <li>Release 0.8.9</li> <li>Release 0.8.8</li> <li>Release 0.8.7</li> </ul>"},{"location":"release-notes/relnotes-2203/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2204/","title":"Version 2204 Release Notes","text":""},{"location":"release-notes/relnotes-2204/#devops-docker-builds-version-2204-may-05-2022","title":"DevOps Docker Builds, Version 2204 (May 05 2022)","text":""},{"location":"release-notes/relnotes-2204/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingAccess<ul> <li>PingAccess 6.3.4 is now available on Dockerhub.</li> </ul> </li> <li>PingIntelligence<ul> <li>PingIntelligence 5.1.1 is now available on Dockerhub.</li> </ul> </li> <li>pingctl<ul> <li>pingctl 1.0.5 released Release Notes </li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2204/#documentation","title":"Documentation","text":"<ul> <li>Environment Considerations added<ul> <li>Workaround for NFS and PingData</li> </ul> </li> <li>Getting Started Examples updated<ul> <li>Getting-started docker-compose examples now use default registry and image tag values</li> <li>docker-compose examples now use pingctl's config file over deprecated ping-devops's config file</li> </ul> </li> <li>Pingctl configuration updated<ul> <li>Instructions to export environment variables if wanted</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2204/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-389) - Fix an issue with the getSemanticImageVersion function that was causing \"bad number\" errors during hook scripts.</li> <li>(BRASS-397) - Updated PingDirectory and PingFederate server profiles to remove hard-coded instances of dc=example,dc=com and replace them with the USER_BASE_DN environment variable.</li> </ul>"},{"location":"release-notes/relnotes-2204/#enhancements","title":"Enhancements","text":"<ul> <li>Docker Images<ul> <li>Apache Tomcat to Version 9.0.62</li> <li>Alpine to version 3.15.4</li> <li>Liberica JDK to 11.0.15+10</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2204/#features","title":"Features","text":"<ul> <li>(BRASS-393) PingDirectory supports multiple base DNS for replication<ul> <li>Updated the PingDirectory image to support enabling and initializing replication for multiple base DNs with the REPLICATION_BASE_DNS variable, in addition to the USER_BASE_DN variable. Multiple DNs can be delimited with a ';' character.  <ul> <li>For example:  REPLICATION_BASE_DNS=dc=additional,dc=com;dc=another,dc=com</li> </ul> </li> </ul> </li> <li>(BRASS-394) PingDataSync supports persistent volume for restarts<ul> <li>Updated the PingDataSync restart logic to include use of the manage-profile replace-profile command to support running PingDataSync with a persistent volume. This allows for updating the PingDataSync configuration on container restart, without requiring deploying a fresh container or volume.</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2204/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2205/","title":"Version 2205 Release Notes","text":""},{"location":"release-notes/relnotes-2205/#devops-docker-builds-version-2205-june-02-2022","title":"DevOps Docker Builds, Version 2205 (June 02 2022)","text":""},{"location":"release-notes/relnotes-2205/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingAccess<ul> <li>PingAccess 7.0.4 is now available on Dockerhub.</li> </ul> </li> <li>PingData products<ul> <li>Updated all PingData products to build 8.3.0.6<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> </ul> </li> <li>PingFederate<ul> <li>PingFederate 11.0.3 and 10.3.7 are now available on Dockerhub.</li> </ul> </li> <li>PingIdentity LDAPSDK<ul> <li>PingIdentity LDAPSDK upgraded to 6.0.5 in Docker image Dockerhub.</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2205/#documentation","title":"Documentation","text":"<ul> <li>Sidecar Example created<ul> <li>Page with details and recommendations for using a sidecar, with example</li> </ul> </li> <li>Migrating root-based deployment documentation updated<ul> <li>Refined this page with recommendations of pre-migration steps</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2205/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-402) - Updates to PingAccess and PingFederate</li> <li>Updated the PingAccess and PingFederate builds to generate a run.properties.subst.default file based on the product default run.properties file pulled from /opt/server. This ensures that any other defaults in the product are included in the default run.properties. This change allows for setting the PingAccess operational mode through the OPERATIONAL_MODE environment variable, without requiring a server profile.</li> <li>(BRASS-428) - PingAccess FIPS mode properties issues</li> <li>Updated the default FIPS properties file for PingAccess to use the correct filename and the correct property name to enable FIPS mode.</li> </ul>"},{"location":"release-notes/relnotes-2205/#enhancements","title":"Enhancements","text":"<ul> <li>Docker Images<ul> <li>Apache Tomcat to Version 9.0.63</li> <li>Alpine to version 3.16.0</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2205/#features","title":"Features","text":"<ul> <li>(BRASS-434) Support Null SecurityContext in Helm Charts for Openshift<ul> <li>Enables the helm charts to generate with workload.securityContext as null, permitting the Openshift environment to generate the security context properly.</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2205/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2206/","title":"Version 2206 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2206/#devops-docker-builds-version-2206-july-01-2022","title":"DevOps Docker Builds, Version 2206 (July 01 2022)","text":""},{"location":"release-notes/relnotes-2206/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingData products 9.1.0.0 released and PingData products 8.3.x are no longer built<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> <li>PingFederate 11.1.0 released and PF 10.3.x no longer built  (Dockerhub)</li> <li>PingAccess 7.1.0 released and PA 6.3.x no longer built  (Dockerhub)</li> <li>PingCentral 1.10.0 released  (Dockerhub)</li> <li>PingDelegator 4.10.0 released and PDA 4.9.x no longer built  (Dockerhub)</li> </ul>"},{"location":"release-notes/relnotes-2206/#documentation","title":"Documentation","text":"<ul> <li>How-to documentation for PingIntelligence</li> <li>Added page for Openshift configuration</li> </ul>"},{"location":"release-notes/relnotes-2206/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-464) Fixed PingAuthorize baseline server profile overwriting certain environment variables from the orchestration layer</li> </ul>"},{"location":"release-notes/relnotes-2206/#enhancements","title":"Enhancements","text":"<ul> <li>Liberica JDK -&gt; 11.0.15.1+2</li> <li>Apache Tomcat in PingDataConsole -&gt; 9.0.64</li> <li>Apache JMeter -&gt; 5.5</li> </ul>"},{"location":"release-notes/relnotes-2206/#features","title":"Features","text":"<ul> <li>(BRASS-440) Kubernetes deployment script of Ping Intelligence</li> </ul>"},{"location":"release-notes/relnotes-2206/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2207/","title":"Version 2207 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2207/#devops-docker-builds-version-2207-august-05-2022","title":"DevOps Docker Builds, Version 2207 (August 05 2022)","text":""},{"location":"release-notes/relnotes-2207/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingFederate 11.1.1 released (Dockerhub)</li> <li>PingData products 9.0.0.2 released<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2207/#documentation","title":"Documentation","text":"<ul> <li>Added documentation for using extensions with PingData server profiles</li> <li>Split the Helm example pingaccess-cluster.yaml to more accurately represent what the file accomplishes, and added a pingaccess-pingfederate-integration.yaml example.</li> <li>(BRASS-486) Added clarification for pingtoolkit initContainer security context to the Openshift documentation in the Helm portal</li> <li>(BRASS-497) Added documentation about container logging</li> <li>(BRASS-501) Added new Helm examples to replace old docker compose and kustomize examples.<ul> <li>pingauthorize-pingdirectory.yaml</li> <li>pingdataconsole-pingone-sso.yaml</li> <li>pingdatasync-failover.yaml</li> <li>pingcentral-external-mysql-db</li> <li>pingdirectory-upgrade-partition</li> </ul> </li> <li>(BRASS-490) The documentation portal was extensively updated for grammar, examples, and validation</li> </ul>"},{"location":"release-notes/relnotes-2207/#enhancements","title":"Enhancements","text":"<ul> <li>(BRASS-508) PingFederate and PingAccess now use the heartbeat from the API for the startup success check (rather than a fixed timeout). Added a new variable $ADMIN_WAITFOR_TIMEOUT (default 300 seconds) as a backstop against a hung startup process in the 80-post-start.sh script.</li> <li>Liberica JDK -&gt; 11.0.16+8</li> <li>Alpine -&gt; 3.16.1</li> <li>Apache Tomcat in PingDataConsole -&gt; 9.0.65</li> </ul>"},{"location":"release-notes/relnotes-2207/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2208/","title":"Version 2208 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2208/#devops-docker-builds-version-2208-september-01-2022","title":"DevOps Docker Builds, Version 2208 (September 01 2022)","text":""},{"location":"release-notes/relnotes-2208/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingAccess 7.1.1 and 7.0.5 (Dockerhub)</li> <li>PingFederate 11.0.4 (Dockerhub)</li> </ul>"},{"location":"release-notes/relnotes-2208/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-346) Added documentation for setting the PingFederate provisioner node ID</li> <li>(BRASS-366) Added .default to any .subst files built into the image that did not have the extension. This prevents .subst files from overwriting the equivalent files when defined in a server profile.</li> <li>(BRASS-469) PingFederate Upgrade Documentation has been updated</li> <li>(BRASS-484) Fixed layered profile documentation page referring to a profile that no longer exists.</li> <li>(BRASS-516) Updated documentation with new recommended process for PingData certificate rotation</li> </ul>"},{"location":"release-notes/relnotes-2208/#enhancements","title":"Enhancements","text":"<ul> <li>Liberica JDK 11.0.16+8 -&gt; 11.0.16.1+1</li> <li>Liberica JDK 17.0.4+8 -&gt; 17.0.4.1+1</li> <li>Alpine 3.16.1 -&gt; 3.16.2</li> </ul>"},{"location":"release-notes/relnotes-2208/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2209/","title":"Version 2209 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2209/#devops-docker-builds-version-2209-october-04-2022","title":"DevOps Docker Builds, Version 2209 (October 04 2022)","text":""},{"location":"release-notes/relnotes-2209/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingAccess 7.1.2 and 7.0.6. EOL 7.1.1 and 7.0.5 (Dockerhub)</li> <li>LdapSDK to 6.0.6 (Dockerhub)</li> </ul>"},{"location":"release-notes/relnotes-2209/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-545) - Refined the Operating Patterns document for clarity and grammar: (Deployment Patterns)</li> <li>(BRASS-556) - Corrected link to the product support matrix in recent release notes</li> </ul>"},{"location":"release-notes/relnotes-2209/#enhancements","title":"Enhancements","text":"<ul> <li>Apache-Tomcat 9.0.65 -&gt; 9.0.67</li> </ul>"},{"location":"release-notes/relnotes-2209/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2210/","title":"Version 2210 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2210/#devops-docker-builds-version-2210-november-02-2022","title":"DevOps Docker Builds, Version 2210 (November 02 2022)","text":""},{"location":"release-notes/relnotes-2210/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingFederate 11.1.2 (Dockerhub)</li> <li>PingAccess 7.1.3, EOL 7.1.2 and 7.2.0-Beta (Dockerhub)</li> </ul>"},{"location":"release-notes/relnotes-2210/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-392) - Configure baseline server profile pf-connected-identities for DA configuration</li> </ul>"},{"location":"release-notes/relnotes-2210/#enhancements","title":"Enhancements","text":"<ul> <li>Added support for the necessary dsreplication commands and arguments to deploy an entry-balanced PingDirectory topology.</li> <li>Use the RESTRICTED_BASE_DNS environment variable to define the restricted base DNs for the topology. The multi-region environment variables (such as K8S_CLUSTER and K8S_SEED_CLUSTER) must also be defined when using entry balancing</li> <li>com.unboundid.directory.server.MaintainConfigArchive=false has been set in the PingData images</li> </ul>"},{"location":"release-notes/relnotes-2210/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2211/","title":"Version 2211 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2211/#devops-docker-builds-version-2211-december-09-2022","title":"DevOps Docker Builds, Version 2211 (December 09 2022)","text":""},{"location":"release-notes/relnotes-2211/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingDirectory 9.1.0.1 and EOL 9.1.0.0 (Dockerhub)</li> <li>PingFederate 11.2.0 and EOL 11.0.5 (Dockerhub)</li> <li>PingFederate 11.1.2 -&gt; 11.1.3 (Dockerhub)</li> </ul>"},{"location":"release-notes/relnotes-2211/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-615) Remove DevOps User/Key Requirement from PingFederate Upgrade Script.</li> <li>(BRASS-570) Updated the 90-shutdown-sequence.sh hook script to not attempt to remove the seed server from its topology.</li> </ul>"},{"location":"release-notes/relnotes-2211/#documentation","title":"Documentation","text":"<ul> <li>Added Ping Product Docker Image Exploration</li> <li>Added Hook Script Exploration</li> <li>Added CICD Demonstration</li> </ul>"},{"location":"release-notes/relnotes-2211/#enhancements","title":"Enhancements","text":"<ul> <li>Apache Tomcat 9.0.70 and EOL 9.0.69</li> <li>LDAPSDK 6.0.7 and EOL 6.0.6</li> <li>Alpine 3.16.2 -&gt; 3.17.0</li> <li>Apache Tomcat 9.0.68 -&gt; 9.0.69</li> <li>OpenSSL 1.1.1 -&gt; 3.0.7</li> <li>Added support for creating a PingAccess cluster using Helm without a server-profile</li> </ul>"},{"location":"release-notes/relnotes-2211/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2212/","title":"Version 2212 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2212/#devops-docker-builds-version-2212-january-03-2023","title":"DevOps Docker Builds, Version 2212 (January 03 2023)","text":""},{"location":"release-notes/relnotes-2212/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingAccess 7.2.0 releases and PingAccess products 7.0.x are no longer built (Dockerhub)</li> <li>PingData products 9.2.0.0 released and PingData products 9.0.0.x are no longer built<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2212/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-522) Remove PingDataGovernance/PingDataGovernancePAP images and documentation</li> <li>(BRASS-604) Update our server profiles for PF to not use tcp.xml.subst</li> </ul>"},{"location":"release-notes/relnotes-2212/#documentation","title":"Documentation","text":"<ul> <li>Added Reference CICD Pipeline Demonstration</li> </ul>"},{"location":"release-notes/relnotes-2212/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2301/","title":"Version 2301 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2301/#devops-docker-builds-version-2301-february-07-2023","title":"DevOps Docker Builds, Version 2301 (February 07 2023)","text":""},{"location":"release-notes/relnotes-2301/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingFederate 11.2.0 -&gt; 11.2.2 and 11.1.3 -&gt; 11.1.5 (Dockerhub)</li> </ul>"},{"location":"release-notes/relnotes-2301/#enhancements","title":"Enhancements","text":"<ul> <li>Alpine 3.17.0 -&gt; 3.17.1</li> <li>Apache Tomcat 9.0.70 -&gt; 9.0.71</li> <li>Liberica JDK11 11.0.17+7 -&gt; 11.0.18+10</li> <li>Liberica JDK17 17.0.5+8 -&gt; 17.0.6+10</li> </ul>"},{"location":"release-notes/relnotes-2301/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-646) Support PingData multi-region multi-loadbalancer use case</li> <li>(BRASS-660) Update helm charts to support K8s 1.25+ in</li> <li>(BRASS-674) Fixed error output problem from tags 2211 or 2212 with 2211.1 or 2212.1</li> <li>(BRASS-683) Updated PingDataConsole baseline server profile to handle spring.mvc.pathmatch.matching-strategy=ant_path_matcher</li> <li>(BRASS-688) Fixed issue where PingDirectoryProxy was unable to start with mounted license</li> <li>(BRASS-705) Handle Restarts in PingAccess 81 Hook Script</li> </ul>"},{"location":"release-notes/relnotes-2301/#documentation","title":"Documentation","text":"<ul> <li>Added Openshift Local Demonstration</li> <li>Added Migrating from privileged images to unprivileged-by-default images</li> <li>Added Deploy a local Kubernetes Cluster</li> <li>Added Deploy a Local Openshift Cluster</li> <li>Added Migrating cluster discovery settings</li> </ul>"},{"location":"release-notes/relnotes-2301/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2302/","title":"Version 2302 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2302/#devops-docker-builds-version-2302-march-01-2023","title":"DevOps Docker Builds, Version 2302 (March 01 2023)","text":""},{"location":"release-notes/relnotes-2302/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingFederate 11.2.2 -&gt; 11.2.3 and 11.1.5 -&gt; 11.1.6 (Dockerhub)</li> </ul>"},{"location":"release-notes/relnotes-2302/#enhancements","title":"Enhancements","text":"<ul> <li>(BRASS-288) Pin every product to IPv4 for consistent liveness performance in dual-stack settings</li> <li>(BRASS-752) Added a check for indeterminate CRLF characters in sourced files in hook scripts</li> </ul>"},{"location":"release-notes/relnotes-2302/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-598) Fix setting custom java.properties for PingData images functionality</li> <li>(BRASS-743) Added support for PA clustered admin configuration apart from Ping Helm charts</li> <li>(BRASS-744) Hook script update to check if the start-up-deployer was used to configure PA admin and respond accordingly</li> </ul>"},{"location":"release-notes/relnotes-2302/#features","title":"Features","text":"<ul> <li>(BRASS-544) Support new logging level settings of PF 11.2</li> <li>(BRASS-591) Allow PingDirectoryProxy to join PingDirectory's topology with automatic backend discovery</li> <li>(BRASS-767) Updated the example PingDirectory backup script to run only on the master of the topology</li> </ul>"},{"location":"release-notes/relnotes-2302/#documentation","title":"Documentation","text":"<ul> <li>Added FAQ on removal of direct Docker integration support from Kubernetes</li> <li>Added Restoring a Multi Region PingDirectory Deployment on Seed Region Failure</li> <li>Replaced references to the configuration of the old devops tool(~/.pingidentity/devops), with the configuration for pingctl tool (~/.pingidentity/config)</li> </ul>"},{"location":"release-notes/relnotes-2302/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2303/","title":"Version 2303 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2303/#devops-docker-builds-version-2303-april-04-2023","title":"DevOps Docker Builds, Version 2303 (April 04 2023)","text":""},{"location":"release-notes/relnotes-2303/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingAccess 7.2.0 -&gt; 7.2.1 and build 7.3.0-Beta (Dockerhub)</li> <li>PingFederate 11.2.3 -&gt; 11.2.4 and build 11.3.0-beta EOL 11.0.1 and 10.3.5 (Dockerhub)</li> <li>PingData products 9.1.0.1 -&gt; 9.1.0.2 EOL 9.0.0.0<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> <li>PingIdentity LDAPSDK 6.0.7 -&gt; 6.0.8 (Dockerhub)</li> <li>Apache Tomcat 9.0.72 -&gt; 9.0.73 (Dockerhub)</li> </ul>"},{"location":"release-notes/relnotes-2303/#enhancements","title":"Enhancements","text":"<ul> <li>Alpine 3.17.2 -&gt; 3.17.3</li> </ul>"},{"location":"release-notes/relnotes-2303/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-650) Updated helm template tests to check array values</li> <li>(BRASS-735) Added integration-tests for arm pods of pingauthorize and pingauthorizepap</li> <li>(BRASS-762) Fixed broken Kubernetes links on the Ping Identity devops site</li> <li>(BRASS-808) Set Proxy topology host and port correctly</li> <li>(BRASS-835) Fix replication result code not being recorded correctly in VERBOSE mode</li> <li>(BRASS-839) Added Splunk log formatting examples for PingFederate and PingDirectory</li> <li>(BRASS-842) Update defaultDomain to pingdemo.example across the board</li> </ul>"},{"location":"release-notes/relnotes-2303/#features","title":"Features","text":"<ul> <li>(BRASS-806) Add default matchLabels values for topologySpreadConstraints</li> <li>(BRASS-645) Enable pf.cluster.bind.address to be set with variable</li> </ul>"},{"location":"release-notes/relnotes-2303/#documentation","title":"Documentation","text":"<ul> <li>Added Forward PingFederate and PingAccess logs to Splunk</li> <li>Added Upgrading PingAccess</li> <li>Added Multi-node local K8s cluster guide</li> <li>Added Clarification for 3rd party software support</li> <li>Added Deploy a Local Openshift Cluster</li> <li>Added Splunk Logging Example</li> <li>Updated FAQs page to describe how to get notified of a release</li> </ul>"},{"location":"release-notes/relnotes-2303/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2304/","title":"Version 2304 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2304/#devops-docker-builds-version-2304-may-04-2023","title":"DevOps Docker Builds, Version 2304 (May 04 2023)","text":""},{"location":"release-notes/relnotes-2304/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingAccess EOL 7.0.3 (Dockerhub)</li> <li>PingFederate EOL 11.0.2 and 10.3.6 (Dockerhub)</li> <li>PingIntelligence EOL 5.1.0 (Dockerhub)</li> </ul>"},{"location":"release-notes/relnotes-2304/#enhancements","title":"Enhancements","text":"<ul> <li>Liberica JDK11 11.0.18+10 -&gt; 11.0.19+7</li> <li>Liberica JDK17 17.0.6+10 -&gt; 17.0.7+7</li> <li>Apache Tomcat 9.0.73 -&gt; 9.0.74</li> <li>Docker in docker (dind) 18.09 -&gt; 23.0.2</li> </ul>"},{"location":"release-notes/relnotes-2304/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-893) Update Hook Scripts to Work with Curl 8</li> <li>(BRASS-979) Prevent PingDirectory container readiness probe from succeeding before replication configuration was complete on restart</li> </ul>"},{"location":"release-notes/relnotes-2304/#features","title":"Features","text":"<ul> <li>(BRASS-869) Preserve docker cache when using serial_build.sh to build images locally.</li> </ul>"},{"location":"release-notes/relnotes-2304/#documentation","title":"Documentation","text":"<ul> <li>Added video to Deploy a robust local Kubernetes Cluster</li> <li>Added Forwarding PingFederate and PingAccess logs to Splunk</li> </ul>"},{"location":"release-notes/relnotes-2304/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2305/","title":"Version 2305 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2305/#devops-docker-builds-version-2305-june-01-2023","title":"DevOps Docker Builds, Version 2305 (June 01 2023)","text":""},{"location":"release-notes/relnotes-2305/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingFederate 11.2.4 \u2192 11.2.5 and 11.1.6 \u2192 11.1.7 EOL 11.0.2 and 10.3.6 (Dockerhub)</li> <li>PingAccess EOL 7.0.3 (Dockerhub)</li> <li>PingData products 9.2.0.0 -&gt; 9.2.0.1 EOL 8.3.0.5<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2305/#enhancements","title":"Enhancements","text":"<ul> <li>Apache Tomcat 9.0.74 \u2192 9.0.75</li> <li>Alpine 3.17.3 \u2192 3.18.0</li> </ul>"},{"location":"release-notes/relnotes-2305/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-1024) Update PingAccess 51-add-engine.sh hook script to correctly handle engine crashes and restarts</li> <li>(BRASS-897) Improve check for the current topology in PingDirectory to fail if the server can't reach itself</li> <li>(BRASS-1033) Fix incorrect result code capture in hook scripts</li> </ul>"},{"location":"release-notes/relnotes-2305/#features","title":"Features","text":"<ul> <li>(BRASS-833) Update images to include product license at /licenses.</li> </ul>"},{"location":"release-notes/relnotes-2305/#documentation","title":"Documentation","text":"<ul> <li>Added Ping Identity Support Portal as a support link for our Ping Identity customers</li> <li>Added PingDevOps Community as a support link for our Non-Ping Identity customers</li> <li>Fix missing CERTIFICATE_NICKNAME row from product image documentation on devops site</li> </ul>"},{"location":"release-notes/relnotes-2305/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2306/","title":"Version 2306 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2306/#devops-docker-builds-version-2306-june-30-2023","title":"DevOps Docker Builds, Version 2306 (June 30 2023)","text":""},{"location":"release-notes/relnotes-2306/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingFederate 11.3.0 released and PingFederate 11.1.x no longer built 11.2.5 \u2192 11.2.6 EOL 10.3.7 and 11.3.0-Beta (Dockerhub)</li> <li>PingAccess 7.3.0 released and PingAccess 7.1.x no longer built EOL 6.3.4 and 7.3.0-Beta (Dockerhub)</li> <li>PingCentral 1.12.0 released and PingCentral 1.10.x no longer built EOL 1.9.3 and 1.8.2 (Dockerhub)</li> <li>PingData products 9.3.0.0 released and PingData products 9.1.0.x are no longer built EOL 8.3.0.6<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> <li>PingDelegator EOL 4.6.0</li> </ul>"},{"location":"release-notes/relnotes-2306/#enhancements","title":"Enhancements","text":"<ul> <li>Apache JMeter 5.5 \u2192 5.6</li> <li>LDAP SDK 6.0.8 \u2192 6.0.9</li> <li>Apache Tomcat 9.0.75 \u2192 9.0.76</li> <li>Alpine 3.18.0 \u2192 3.18.2</li> </ul>"},{"location":"release-notes/relnotes-2306/#documentation","title":"Documentation","text":"<ul> <li>Updated Create a simple local Kubernetes Cluster kind and minikube examples to reflect latest releases and supported backends</li> <li>Updated Deploy a robust local Kubernetes Cluster 3-VM Kubernetes example to use Ansible</li> </ul>"},{"location":"release-notes/relnotes-2306/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2307-1/","title":"Version 2307.1 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2307-1/#devops-docker-builds-version-23071-sept-13-2023","title":"DevOps Docker Builds, Version 2307.1 (Sept 13 2023)","text":""},{"location":"release-notes/relnotes-2307-1/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-1197) Fix /opt file system permissions defect</li> </ul>"},{"location":"release-notes/relnotes-2307-1/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2307/","title":"Version 2307 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2307/#devops-docker-builds-version-2307-aug-2-2023","title":"DevOps Docker Builds, Version 2307 (Aug 2 2023)","text":""},{"location":"release-notes/relnotes-2307/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingAccess 7.2.1 \u2192 7.2.2 (Dockerhub)</li> <li>PingFederate 11.3.0 \u2192 11.3.1 and 11.2.6 \u2192 11.2.7 (Dockerhub)</li> <li>PingDirectory Terraform Provider v0.9.0 released </li> </ul>"},{"location":"release-notes/relnotes-2307/#enhancements","title":"Enhancements","text":"<ul> <li>Apache Tomcat 9.0.76 \u2192 9.0.78 </li> <li>Apache JMeter 5.6 \u2192 5.6.2 </li> <li>Liberica JDK11 11.0.19+7 \u2192 11.0.20+8 </li> <li>Liberica JDK17 17.0.7+7 \u2192 17.0.8+7</li> </ul>"},{"location":"release-notes/relnotes-2307/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-1088) Fix defect in PingDirectory Backup Script</li> <li>(BRASS-1121) Fix helm ingress demo YAML annotation</li> <li>(BRASS-1124) Fix MOTD retrieval and notification script.</li> <li>(BRASS-1143) Update 85-import PA and PF script to have better error messaging upon import failure.</li> </ul>"},{"location":"release-notes/relnotes-2307/#features","title":"Features","text":"<ul> <li>(BRASS-1115 and BRASS-1116) Added new pingaccess-env-config and pingfederate-env-config server profiles that allow managing PingAccess and PingFederate configuration using environment variables in place of certain properties files.</li> </ul>"},{"location":"release-notes/relnotes-2307/#documentation","title":"Documentation","text":"<ul> <li>Updated VPC Peering Configuration to Support Multi-Region AWS EKS Deployments to define VPC peering configuration for Multi-region AWS EKS Deployments.</li> <li>Added documentation on Running product containers with a read-only root filesystem\u00b6.</li> </ul>"},{"location":"release-notes/relnotes-2307/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2308-1/","title":"Version 2308.1 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2308-1/#devops-docker-builds-version-23081-sept-13-2023","title":"DevOps Docker Builds, Version 2308.1 (Sept 13 2023)","text":""},{"location":"release-notes/relnotes-2308-1/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-1197) Fix /opt file system permissions defect</li> </ul>"},{"location":"release-notes/relnotes-2308-1/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2308/","title":"Version 2308 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2308/#devops-docker-builds-version-2308-sep-5-2023","title":"DevOps Docker Builds, Version 2308 (Sep 5 2023)","text":""},{"location":"release-notes/relnotes-2308/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingFederate EOL 11.0.3</li> <li>PingAccess EOL 7.0.4 and 7.1.0</li> <li>PingData products 9.2.0.1 \u2192 9.2.0.2 and 9.3.0.0 \u2192 9.3.0.1<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2308/#enhancements","title":"Enhancements","text":"<ul> <li>Apache Tomcat 9.0.78 \u2192 9.0.80</li> <li>Alpine 3.18.2 \u2192 3.18.3</li> <li>Liberica JDK11 11.0.20+8 \u2192 11.0.20.1+1 </li> <li>Liberica JDK17 17.0.8+7 \u2192 17.0.8.1+1</li> </ul>"},{"location":"release-notes/relnotes-2308/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-1087) Update PD_FORCE_DATA_REIMPORT variable name to match documentation</li> </ul>"},{"location":"release-notes/relnotes-2308/#features","title":"Features","text":"<ul> <li>(BRASS-1122) Added support for imagePullSecrets in Helm charts</li> <li>(BRASS-1050) Test image integration on openshift cluster</li> <li>(BRASS-853) Add \"DEBUG\" option for intermediate logging, for now just around curl calls in product containers</li> </ul>"},{"location":"release-notes/relnotes-2308/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2309/","title":"Version 2309 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2309/#devops-docker-builds-version-2309-oct-3-2023","title":"DevOps Docker Builds, Version 2309 (Oct 3 2023)","text":""},{"location":"release-notes/relnotes-2309/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingFederate 11.3.1 \u2192 11.3.2</li> <li>PingAccess 7.3.0 \u2192 7.3.1. EOL 7.1.1 and 7.0.5</li> <li>PingCentral 1.14.0 Release and PingCentral 1.11.0 no longer built. EOL 1.10.0 and 1.9.4</li> <li>PingData products 9.2.0.2 \u2192 9.2.0.3<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2309/#enhancements","title":"Enhancements","text":"<ul> <li>Alpine 3.18.3 \u2192 3.18.4</li> <li>LDAP SDK 6.0.9 \u2192 6.0.10</li> </ul>"},{"location":"release-notes/relnotes-2309/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-1087) Update PD_FORCE_DATA_REIMPORT variable name to match documentation</li> <li>(BRASS-286) PingAccess Cluster - handle orphaned engines</li> </ul>"},{"location":"release-notes/relnotes-2309/#features","title":"Features","text":"<ul> <li>(BRASS-1123) Publish images to openshift certified registry</li> </ul>"},{"location":"release-notes/relnotes-2309/#documentation","title":"Documentation","text":"<ul> <li>Added Upgrading PingCentral</li> <li>Added helm examples for imagePullSecrets to Deploy Ping DevOps Charts using Helm</li> <li>Provided clarity on EFS/EBS support with Ping products running on AWS in Recent portal updates</li> </ul>"},{"location":"release-notes/relnotes-2309/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2310/","title":"Version 2310 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2310/#devops-docker-builds-version-2310-nov-1-2023","title":"DevOps Docker Builds, Version 2310 (Nov 1 2023)","text":""},{"location":"release-notes/relnotes-2310/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingAccess 7.3.1 -&gt; 7.3.2</li> <li>PingIntelligence 5.1.1 -&gt; 5.1.3</li> <li>PingData products 9.3.0.1 -&gt; 9.3.0.2<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2310/#enhancements","title":"Enhancements","text":"<ul> <li>Apache Tomcat 9.0.80 -&gt; 9.0.82</li> <li>Liberica JDK11 11.0.20.1+1 -&gt; 11.0.21+10</li> <li>Liberica JDK17 17.0.8.1+1 -&gt; 17.0.9+11</li> </ul>"},{"location":"release-notes/relnotes-2310/#features","title":"Features","text":"<ul> <li> <p>(BRASS-1303) Added support for PingDirectoryProxy joining a multi-region PingDirectory topology.</p> <ul> <li> <p>The PingDirectoryProxy servers can also be across multiple regions. The PingDirectoryProxy   servers should not be started until the PingDirectory topology is up and ready.</p> </li> <li> <p>The same variables used to run PingDirectory in multiple regions   are used by PingDirectoryProxy. In addition, the following variables   must be defined:</p> <pre><code>JOIN_PD_TOPOLOGY: Set to true to join a PingDirectory topology\nPINGDIRECTORY_HOSTNAME: Hostname of a PingDirectory server in the topology\nPINGDIRECTORY_LDAPS_PORT: LDAPS port of the PingDirectory server to join\n\nAdded the LOAD_BALANCING_ALGORITHM_NAMES variable to PingDirectory,\nwhich allows defining what load balancing algorithms to set on the server instance,\nseparated by semicolons. This variable is only needed when using PingDirectoryProxy\nautomatic server discovery\n\nThis change also removes the requirement for PingDirectory to identify\na topology master server when enabling replication. It will instead\nalways join via the seed server.\n</code></pre> </li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2310/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-1287) Updated helm chart to support removed batch API endpoint designation in Kubernetes 1.25</li> <li>(BRASS-1239) Corrected broken links on Ping DockerHub repositories</li> </ul>"},{"location":"release-notes/relnotes-2310/#documentation","title":"Documentation","text":"<ul> <li>(BRASS-1217) Added demonstration   for Archiving and Retrieving Backups from S3</li> </ul>"},{"location":"release-notes/relnotes-2310/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2311/","title":"Version 2311 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2311/#devops-docker-builds-version-2311-dec-1-2023","title":"DevOps Docker Builds, Version 2311 (Dec 1 2023)","text":""},{"location":"release-notes/relnotes-2311/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingCentral 1.14.0 \u2192 1.14.1</li> <li>PingFederate 11.3.2 \u2192 11.3.3</li> <li>UnboundID LDAP SDK 6.0.10 \u2192 6.0.11</li> <li>PingData products 9.3.0.2 -&gt; 9.3.0.3<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> <li>PingData products 9.2.0.3 -&gt; 9.2.0.4<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2311/#enhancements","title":"Enhancements","text":"<ul> <li>Apache Tomcat 9.0.82 \u2192 9.0.83</li> </ul>"},{"location":"release-notes/relnotes-2311/#features","title":"Features","text":"<ul> <li>(BRASS-1291) Added PF_LDAP_TYPE environment variable to support ldap.properties new field 'ldap.type' in PF 11.3.</li> <li>(BRASS-1354) Our RHEL UBI9-minimal images now come with tar commandline utility installed by default.</li> </ul>"},{"location":"release-notes/relnotes-2311/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(BRASS-1331) Fixed issue in our Helm charts where the replicas field was being set when autoscaling was enabled.</li> <li>(BRASS-1334) Updated the Helm Chart checks for semantic version for apiVersion to use a capability check rather than    verifying specific versions. This fixes issues with comparing prerelease versions.</li> <li>(BRASS-1345) Update the PingAuthorizePAP readiness check.</li> </ul>"},{"location":"release-notes/relnotes-2311/#documentation","title":"Documentation","text":"<ul> <li>(BRASS-1318) Added FAQ concerning trial licenses versus engaging support.</li> <li>(BRASS-1329) Reviewed and updated getting-started examples, creating clusters and using kind or minikube documentation.</li> </ul>"},{"location":"release-notes/relnotes-2311/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2312/","title":"Version 2312 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2312/#devops-docker-builds-version-2312-dec-29-2023","title":"DevOps Docker Builds, Version 2312 (Dec 29 2023)","text":""},{"location":"release-notes/relnotes-2312/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingFederate 11.3.3 \u2192 11.3.4</li> <li>PingAccess 8.0.0 and EOL 7.2.x</li> <li>PingCentral 2.0.0 and EOL 1.12.x</li> <li>PingDelegator 5.0.0 and EOL 4.8.x</li> <li>PingData products 10.0.0.0 and EOL 9.2.0.x<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2312/#enhancements","title":"Enhancements","text":"<ul> <li>Apache Tomcat 9.0.83 \u2192 9.0.84</li> <li>Alpine 3.18.4 \u2192 3.19.0</li> </ul>"},{"location":"release-notes/relnotes-2312/#documentation","title":"Documentation","text":"<ul> <li>(PDI-1211) Added video demonstration of PingCentral upgrade process in containers</li> <li>(PDI-1359) Added FAQ on supported OS shims and JDK version in images</li> <li>(PDI-1361) Updated PingCentral upgrade instructions to latest release</li> </ul>"},{"location":"release-notes/relnotes-2312/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2401/","title":"Version 2401 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2401/#devops-docker-builds-version-2401-jan-29-2024","title":"DevOps Docker Builds, Version 2401 (Jan 29 2024)","text":""},{"location":"release-notes/relnotes-2401/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingCentral 2.0.0 \u2192 2.0.1</li> <li>PingData products 9.3.0.3 \u2192 9.3.0.4<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2401/#enhancements","title":"Enhancements","text":"<ul> <li>Apache Tomcat 9.0.84 \u2192 9.0.85</li> <li>Apache JMeter 5.6.2 \u2192 5.6.3</li> <li>Redhat UBI9-minimal 9.1 \u2192 9.3-1552</li> <li>Liberica JDK11 11.0.21+10 \u2192 11.0.22+12</li> <li>Liberica JDK17 17.0.9+11 \u2192 17.0.10+13</li> <li>Alpine 3.19.0 \u2192 3.19.1</li> </ul>"},{"location":"release-notes/relnotes-2401/#features","title":"Features","text":"<ul> <li>(PDI-1358) Add support for environment variables in utility sidecar in helm charts</li> <li>(PDI-1367) Add global annotation support for PVC definitions</li> <li>(PDI-1461) Add support for secondary port to PF image</li> </ul>"},{"location":"release-notes/relnotes-2401/#documentation","title":"Documentation","text":"<ul> <li>(PDI-1432) Initial documentation for the PingFederate Terraform provider on https://terraform.pingidentity.com/</li> </ul>"},{"location":"release-notes/relnotes-2401/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2402/","title":"Version 2402 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2402/#devops-docker-builds-version-2402-feb-29-2024","title":"DevOps Docker Builds, Version 2402 (Feb 29 2024)","text":""},{"location":"release-notes/relnotes-2402/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingFederate 12.0.0 \u2192 12.0.1</li> <li>PingFederate 11.3.4 \u2192 11.3.5</li> <li> <ul> <li>PingData products 10.0.0.0 \u2192 10.0.0.1</li> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> <li>PingData products 9.3.0.4 \u2192 9.3.0.5<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2402/#enhancements","title":"Enhancements","text":"<ul> <li>Apache Tomcat 9.0.85 \u2192 9.0.86</li> </ul>"},{"location":"release-notes/relnotes-2402/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(PDI-1473) Fixed PingDataSync pods not configuring failover when REMOTE_SERVER_REPLICATION_PORT environment variable was not set. This variable now is not necessary for configuring failover.</li> <li>(PDI-1474) Updated PingData restart hook scripts to handle updating the java.properties file. If you need to set any custom values in java.properties, provide the entire file in your server profile at <code>instance/config/java.properties</code>. Note that this is outside of the <code>pd.profile</code> folder.</li> <li>(PDI-1476) Fixed an issue with processing env variable json files in the secrets directory where keys with special characters were not handled.</li> </ul>"},{"location":"release-notes/relnotes-2402/#documentation","title":"Documentation","text":"<ul> <li>(PDI-1477) Updated ingress definitions and how-to examples and guides to latest versions and formatting</li> <li>(PDI-1478) Removed stale helm repo files as well as updated helm documentation</li> <li>(PDI-1587) Remove extraneous reference to components now in baseline server profile.</li> <li>(PDI-1589) Update old link on the server profiles how-to page.</li> </ul>"},{"location":"release-notes/relnotes-2402/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2403/","title":"Version 2403 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2403/#devops-docker-builds-version-2403-mar-29-2024","title":"DevOps Docker Builds, Version 2403 (Mar 29 2024)","text":""},{"location":"release-notes/relnotes-2403/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingAccess 7.3.2 \u2192 7.3.3</li> <li>PingAccess 8.0.0 \u2192 8.0.1</li> <li>PingData products 10.0.0.1 -&gt; 10.0.0.2<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2403/#enhancements","title":"Enhancements","text":"<ul> <li>Apache Tomcat 9.0.86 \u2192 9.0.87</li> <li>LDAPSDK 6.0.11 -&gt; 7.0.0</li> </ul>"},{"location":"release-notes/relnotes-2403/#resolved-defects","title":"Resolved Defects","text":"<ul> <li>(PDI-1505) Fixed an issue where environment variables pulled in from Vault secrets were not available to the server process</li> </ul>"},{"location":"release-notes/relnotes-2403/#documentation","title":"Documentation","text":"<ul> <li>(PDI-1475) Remove example for setting up Prometheus in GitHub server profile</li> </ul>"},{"location":"release-notes/relnotes-2403/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2404/","title":"Version 2404 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2404/#devops-docker-builds-version-2404-may-1-2024","title":"DevOps Docker Builds, Version 2404 (May 1 2024)","text":""},{"location":"release-notes/relnotes-2404/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingAccess 8.0.1 \u2192 8.0.2</li> <li>PingFederate 12.0.1 \u2192 12.0.2</li> <li>PingFederate 11.3.5 \u2192 11.3.6</li> <li>PingCentral 2.0.1 \u2192 2.0.2</li> </ul>"},{"location":"release-notes/relnotes-2404/#enhancements","title":"Enhancements","text":"<ul> <li>Apache Tomcat 9.0.87 \u2192 9.0.88</li> <li>Liberica JDK17 17.0.10+13 \u2192 17.0.11+10</li> <li>Liberica JDK11 11.0.22+12 \u2192 11.0.23+10</li> </ul>"},{"location":"release-notes/relnotes-2404/#documentation","title":"Documentation","text":"<ul> <li>(PDI-1634) Update the monitoring implementation on K8s in DevOps portal</li> </ul>"},{"location":"release-notes/relnotes-2404/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2405/","title":"Version 2405 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2405/#devops-docker-builds-version-2405-jun-4-2024","title":"DevOps Docker Builds, Version 2405 (Jun 4 2024)","text":""},{"location":"release-notes/relnotes-2405/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingFederate 12.0.2 \u2192 12.0.3</li> <li>PingFederate 11.3.6 \u2192 11.3.7</li> <li>PingAccess 8.0.2 \u2192 8.0.3</li> <li>PingAccess 7.3.3 \u2192 7.3.4</li> </ul>"},{"location":"release-notes/relnotes-2405/#enhancements","title":"Enhancements","text":"<ul> <li>Apache Tomcat 9.0.88 \u2192 9.0.89</li> <li>Redhat UBI9-minimal 9.3-1552 \u2192 9.4-949.1716471857</li> <li>Alpine 3.19.1 \u2192 3.20.0</li> <li>Liberica JDK11 11.0.23+10 \u2192 11.0.23+12</li> <li>Liberica JDK17 17.0.11+10 \u2192 17.0.11+12</li> </ul>"},{"location":"release-notes/relnotes-2405/#features","title":"Features","text":"<ul> <li>(PDI-1673) This sprint release adds RHEL UBI9 minimal images for all supported PingAccess and PingFederate versions.</li> </ul>"},{"location":"release-notes/relnotes-2405/#documentation","title":"Documentation","text":"<ul> <li>(PDI-1851) Update the Openshift Local documentation in the portal</li> <li>(PDI-1854) Updated examples to align with graphics on PF Clustering documentation of the portal.</li> </ul>"},{"location":"release-notes/relnotes-2405/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2406/","title":"Version 2406 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2406/#devops-docker-builds-version-2406-jul-2-2024","title":"DevOps Docker Builds, Version 2406 (Jul 2 2024)","text":""},{"location":"release-notes/relnotes-2406/#new-product-releases","title":"New Product Releases","text":"<ul> <li>LDAPSDK 7.0.0 \u2192 7.0.1</li> <li>PingFederate 12.1.0</li> <li>PingFederate EOL 11.3.X</li> <li>PingAccess 8.1.0</li> <li>PingAccess EOL 7.3.X</li> <li>PingCentral 2.1.0</li> <li>PingCentral EOL 1.14.X</li> <li>PingData products 10.1.0.0<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> <li>PingData EOL 9.3.X.X</li> </ul>"},{"location":"release-notes/relnotes-2406/#enhancements","title":"Enhancements","text":"<ul> <li>Apache Tomcat 9.0.89 \u2192 9.0.90</li> <li>Redhat UBI9 Minimal 9.4-949.1716471857 \u2192 9.4-1134</li> <li>Alpine 3.20.0 \u2192 3.20.1</li> </ul>"},{"location":"release-notes/relnotes-2406/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>(PDI-1843) Fixed issue where '.pre' and '.post' hooks could not be used for the '81-after-start-process' hook for PingAccess and PingFederate</li> <li>(PDI-1867) Updated build-jvm.sh script to use curl rather than wget to avoid an IPV6 issue with some OS shims</li> </ul>"},{"location":"release-notes/relnotes-2406/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"release-notes/relnotes-2407/","title":"Version 2407 Release Notes","text":"<p>Product release notes</p> <p>For information about product changes, refer to the release notes that can be found on each product's download page.</p>"},{"location":"release-notes/relnotes-2407/#devops-docker-builds-version-2407-aug-5-2024","title":"DevOps Docker Builds, Version 2407 (Aug 5 2024)","text":""},{"location":"release-notes/relnotes-2407/#new-product-releases","title":"New Product Releases","text":"<ul> <li>PingFederate 12.1.0 \u2192 12.1.1</li> <li>PingFederate 12.0.3 \u2192 12.0.4</li> <li>PingAccess 8.1.0 \u2192 8.1.1</li> <li>PingAccess 8.0.3 \u2192 8.0.4</li> <li>PingData products 10.1.0.0 \u2192 10.1.0.1<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> <li>PingData products 10.0.0.2 \u2192 10.0.0.3<ul> <li>PingDirectory (Dockerhub)</li> <li>PingDirectory Proxy (Dockerhub)</li> <li>PingDataSync (Dockerhub)</li> <li>PingAuthorize (Dockerhub)</li> <li>PingDataConsole (Dockerhub)</li> </ul> </li> </ul>"},{"location":"release-notes/relnotes-2407/#enhancements","title":"Enhancements","text":"<ul> <li>Apache Tomcat 9.0.90 \u2192 9.0.91</li> <li>Redhat UBI9-minimal 9.4-1134 \u2192 9.4-1194</li> <li>Alpine 3.20.1 \u2192 3.20.2</li> <li>Liberica JDK11 11.0.23+12 \u2192 11.0.24+9</li> <li>Liberica JDK17 17.0.11+12 \u2192 17.0.12+10</li> </ul>"},{"location":"release-notes/relnotes-2407/#documentation","title":"Documentation","text":"<ul> <li>(PDI-1885) Add notification of deprecation of PingIntelligence Docker images</li> </ul>"},{"location":"release-notes/relnotes-2407/#supported-product-releases","title":"Supported Product Releases","text":"<ul> <li>See the Product Version, Image Release Matrix   for currently supported image and product versions.</li> </ul>"},{"location":"tools/ldapsdkUtil/","title":"The <code>ldap-sdk-tools</code> utility","text":"<p>The <code>ldap-sdk-tools</code> Docker image gives you easy access to our LDAP Client SDK tools for use with PingDirectory.</p> <p>For complete documentation, see the <code>pingidentity/ldapsdk</code> repository.</p>"},{"location":"tools/ldapsdkUtil/#setting-up-the-utility","title":"Setting up the utility","text":"<ol> <li> <p>From your local <code>pingidentity-devops-getting-started</code> directory, enter:</p> <pre><code>./ldapsdk\n</code></pre> <p>When you run the <code>ldapsdk</code> script for the first time, you're prompted to configure your settings.</p> <p>To edit the settings in the future, enter:</p> <p><code>sh  ldapsdk configure</code></p> </li> <li> <p>To start the <code>ldap-sdk-tools</code> Docker image, enter:</p> <pre><code>docker run  -it --rm  --network pingnet  pingidentity/ldap-sdk-tools:latest\n</code></pre> </li> <li> <p>To list the available tools, enter <code>ls</code></p> </li> </ol>"},{"location":"tools/pingDevopsUtil_Deprecated/","title":"The <code>ping-devops</code> Utility (Deprecated)","text":"<p><code>ping-devops</code> was our general DevOps command-line utility, but has been superseded by the pingctl tool. This page is maintained for referential knowledge only, and the tool is no longer actively maintained or supported. Users are recommended to migrate to the <code>pingctl</code> product for continued support.</p>"},{"location":"tools/pingDevopsUtil_Deprecated/#dependent-utilities","title":"Dependent Utilities","text":"<p>To perform all of its operations, <code>ping-devops</code> has a dependency on the following utilities:</p> <ul> <li>openssl</li> <li>base64</li> <li>kustomize</li> <li>kubectl</li> <li>envsubst</li> <li>jq</li> </ul>"},{"location":"tools/pingDevopsUtil_Deprecated/#ping-devops-usage","title":"<code>ping-devops</code> Usage","text":"<p>Enter <code>ping-devops</code> in a terminal to display the commands listing, which is shown in the following example.</p> <pre><code>#####################################################################\n#  Ping Identity DevOps (version 0.7.2)\n#\n#  Documentation: https://devops.pingidentity.com\n#   GitHub Repos: https://github.com/topics/ping-devops\n#####################################################################\n\nGeneral Usage:\n  ping-devops config\n  ping-devops info [-v]\n  ping-devops version\n  ping-devops clean\n  ping-devops topic [ {topic-name} ]\n\nGenerate Kubernetes/Kustomize/License Resources:\n  ping-devops generate devops-secret\n  ping-devops generate tls-secret {domain}\n  ping-devops generate ssh-id-secret {ssh id_rsa file}\n  ping-devops generate license {product} {ver}\n  ping-devops generate license-secret {license file}\n  ping-devops generate license-secret {product} {ver}\n  ping-devops generate kustomization.yaml\n\nRunning Docker/Kubernetes Environments:\n  ping-devops docker     [info|start|stop|rm|clean]\n  ping-devops kubernetes [info|start|rm|clean]\n\nHashicorp Vault:\n  ping-devops vault get-token\n  ping-devops vault create-annotations {secret}\n\nFurther help:\n  https://github.com/pingidentity/ping-devops\n</code></pre>"},{"location":"tools/pingDownloader_Deprecated/","title":"The <code>PingDownloader</code> Image (Deprecated)","text":"<p>PingDownloader Deprecation</p> <p>Due to internal infrastructure changes to our license server, we no longer provide product installation files through the PingDownloader image. The production installation files are still available to all customers located at Ping Identity's Downloads Page.</p> <p><code>PingDownloader</code> was an image used to simplify the download process of Ping Identity product installation files and license files.</p>"},{"location":"tools/pingDownloader_Deprecated/#how-to-get-product-installation-files-without-pingdownloader","title":"How to get Product Installation Files Without PingDownloader","text":"<ol> <li> <p>Navigate to Ping Identity's Download webpage.</p> </li> <li> <p>Select a Product download page, for example: PingFederate Download Page.</p> </li> <li> <p>Click on the download button for the desired installation method and product version.</p> </li> <li> <p>If prompted to sign in, please sign in and the download will begin. Alternatively, Sign In Here.</p> </li> <li> <p>If you do not have a Ping Identity account, you can create one on the Account Creation Page.</p> </li> </ol>"},{"location":"tools/pingDownloader_Deprecated/#how-to-get-product-license-files-without-pingdownloader","title":"How to get Product License Files Without PingDownloader","text":"<ol> <li> <p>Install our command-line utility <code>pingctl</code>. Installation steps are described in the utility's Documentation.</p> </li> <li> <p>Run <code>pingctl config</code> to configure the command line tool with your DevOps User and Key information. See our Introduction Documentation for obtaining and using your DevOps User and Key.</p> </li> <li> <p>Run <code>pingctl license {product} {version}</code> to obtain the license file. The <code>{version}</code> portion of the command expects the format to be <code>{major}.{minor}</code>. For example, the following command retrieves a license for PingFederate 11.0.3: <code>pingctl license pingfederate 11.0</code></p> </li> </ol>"},{"location":"tools/pingctlUtil/","title":"The <code>pingctl</code> Utility","text":"<p><code>pingctl</code> is our general DevOps command-line utility.</p> <p>Deprecated file name</p> <p>The deprecated ping-devops utility used a configuration file located at ~/.pingidentity/devops.  pingctl, however, uses ~/.pingidentity/config.  While the files generated are compatible, be aware that some older utilities or documentation might refer to the older configuration name.</p>"},{"location":"tools/pingctlUtil/#dependent-utilities","title":"Dependent Utilities","text":"<p>To perform all of its operations, <code>pingctl</code> has a dependency on the following utilities:</p> <ul> <li>openssl</li> <li>base64</li> <li>kubectl</li> <li>envsubst</li> <li>jq</li> <li>jwt</li> </ul>"},{"location":"tools/pingctlUtil/#installation-and-upgrades","title":"Installation and Upgrades","text":"<p>Using Homebrew to install <code>pingctl</code> on MacOS, Windows via Windows Subsystem for Linux, or Linux.</p> <ol> <li> <p>To install, enter:</p> <pre><code>brew install pingidentity/tap/pingctl\n</code></pre> </li> <li> <p>To upgrade, enter:</p> <pre><code>brew upgrade pingidentity/tap/pingctl\n</code></pre> </li> <li> <p>To check for upgrades, run the following command.</p> <p>Check regularly</p> <p>Check for upgrades regularly.</p> <pre><code>pingctl version\n</code></pre> <p>The dependent utilities for <code>pingctl</code> are also installed or upgraded during this process.</p> </li> </ol> <p>Using sh to install <code>pingctl</code> on Linux and WSL.</p> <ol> <li> <p>To install or upgrade, enter:     <pre><code>curl -sL https://bit.ly/pingctl-install | sh\n</code></pre></p> </li> <li> <p>Ensure you have the dependent utilities for <code>pingctl</code> installed.</p> </li> </ol>"},{"location":"tools/pingctlUtil/#usage","title":"Usage","text":"<pre><code>pingctl &lt;command&gt; [options]\n\nAvailable Commands:\n    info            Print pingctl config\n    config          Manage pingctl config\n    version         Version Details and Check\n    clean           Remove ~/.pingidentity/pingctl\n\n    kubernetes      Kubernetes Tools\n    license         Ping Identity Licensing Tools\n    pingone         PingOne Tools\n</code></pre> <p>Use <code>pingctl</code> for info on available commands.</p> <p>Use <code>pingctl &lt;command&gt;</code> for info on a specific command.</p>"},{"location":"tools/pingctlUtil/#options","title":"Options","text":"<p>-h</p> <p>Provide usage details.</p>"},{"location":"tools/pingctlUtil/#available-commands","title":"Available Commands","text":"<ul> <li>kubernetes</li> <li>license</li> <li>pingone</li> <li> <p>info</p> <p>Provides a summary of variables defined with pingctl.</p> </li> <li> <p>config</p> <p>Provides an interactive process in which the user can provide all the <code>pingctl</code> standard variables (i.e. PingOne and Ping DevOps) as well as custom variables</p> </li> <li> <p>version</p> <p>Displays the current version of the tool, and checks to see if an update is available.</p> </li> <li> <p>clean</p> <p>Cleans the cached pingctl work directory containing the latest PingOne Access Token</p> </li> </ul>"},{"location":"tools/commands/kubernetes/","title":"pingctl kubernetes","text":""},{"location":"tools/commands/kubernetes/#description","title":"Description","text":"<p>Manage Ping related Kubernetes resources.</p> <ul> <li>Generate <code>devops-secret</code> secret containing Ping DevOps variables <code>PING_IDENTITY_DEVOPS_KEY</code> and <code>PING_IDENTITY_DEVOPS_SECRET</code></li> <li>Generate <code>tls-secret</code> secret containing a self-signed certificate and key for a specified domain.</li> <li>Generate <code>ssh-id-secret</code> secret containing a file with ssh id (i.e. ~/.ssh/id_rsa)</li> <li>Generate <code>license-secret</code> secret containing a Ping Identity product license file or generated eval license</li> <li>Provide details about a cached kubectl oidc token</li> <li>Display the entire jwt token</li> <li>Display a specific claim</li> <li>Clear the kubectl oidc cache</li> </ul>"},{"location":"tools/commands/kubernetes/#usage","title":"Usage","text":"<pre><code>pingctl kubernetes generate devops-secret\npingctl kubernetes generate tls-secret {domain}\npingctl kubernetes generate ssh-id-secret {ssh id_rsa file}\npingctl kubernetes generate license-secret {license file}\npingctl kubernetes generate license-secret {product} {ver}\n\npingctl kubernetes oidc clear\npingctl kubernetes oidc {claim}\npingctl kubernetes oidc info\n</code></pre>"},{"location":"tools/commands/kubernetes/#options","title":"Options","text":""},{"location":"tools/commands/license/","title":"pingctl license","text":""},{"location":"tools/commands/license/#description","title":"Description","text":"<p>Provides access to Ping Identity evaluation license keys.</p> <ul> <li>Retrieve license based on product name and version</li> </ul>"},{"location":"tools/commands/license/#usage","title":"Usage","text":"<pre><code>pingctl license {product} {ver}\n</code></pre>"},{"location":"tools/commands/license/#options","title":"Options","text":"<ul> <li> <p>product: name of the product</p> <p>This name is generally a collapsed one-word representation of the product name. For example: Ping Federate is <code>pingfederate</code></p> </li> <li> <p>ver: version of the product</p> <p>This value is the <code>major.minor</code> representation of the version of the product in question.  For example, if a product had a point release of <code>10.2.3</code> you would provide <code>10.2</code></p> </li> </ul>"},{"location":"tools/commands/pingone/","title":"pingctl pingone","text":""},{"location":"tools/commands/pingone/#description","title":"Description","text":"<p>Provides ability to manage PingOne environments.  Capabilities of this command include:</p> <ul> <li>Listing, searching and retrieving PingOne resources (i.e. user, populations, groups)</li> <li>Adding PingOne resources</li> <li>Deleting PingOne resources</li> </ul>"},{"location":"tools/commands/pingone/#usage","title":"Usage","text":"<pre><code>pingctl pingone get                  # Get PingOne resource(s)\npingctl pingone add                  # Add PingOne resource\npingctl pingone delete               # Delete PingOne resource\n\npingctl pingone add-user-group       # Add group to user\npingctl pingone delete-user-group    # Delete group from user\n\npingctl pingone token                # Obtain access token\n</code></pre>"},{"location":"tools/commands/pingone/#options","title":"Options","text":""},{"location":"tools/commands/pingone/#all-subcommands","title":"All subcommands","text":"<pre><code>-r\n    Provide REST Calls\n</code></pre>"},{"location":"tools/commands/pingone/#get","title":"get","text":"<pre><code>-o [ table | csv | json ]\n    Output format (default: table)\n    also set with env variable: PINGCTL_DEFAULT_OUTPUT\n\n-i {id}\n    Search based on object guid\n\n-n {name}\n    Search based on exact filter\n\n-f {filter}\n    PingOne filter (SCIM based)\n        ex: '.name.given eq \"john\"'\n            '.email sw \"john\"'\n\n-c {columns}\n    Columns to output based on \"heading:jsonAttr\"\n    An example of available jsonAttrs can be found by using a json output first.\n        ex: 'LastName:name.family,FirstName:name.given'\n\n-s {sort column}\n    Columns to sort output on based on \"jsonAttr\"\n    The jsonAttr MUST be listed in the list of columns (-c option).\n        ex: 'name.family'\n\n-p {population name}\n    Population from which to retrieve a user/group\n    If not provided, the 'Default' population is used\n</code></pre>"},{"location":"tools/commands/pingone/#add","title":"add","text":"<pre><code>-p {population name}\n    Population into which to add a user/group\n    If not provided, the 'Default' population is used\n</code></pre>"},{"location":"videos/videos/","title":"Devops Videos","text":"<p>You can also find all of these devops videos on the Ping Website</p>"},{"location":"videos/videos/#foundational-information","title":"Foundational Information","text":"<ul> <li>Getting Started Walkthrough<ul> <li>Introduction on how to prepare a local Kubernetes environment to deploy Ping products. The video will also show how to use our Helm Charts to deploy our products and how to access product consoles through the ingress controller.</li> </ul> </li> <li>CICD Demonstration<ul> <li>This 30-minute video provides a high-level overview of Continuous Integration/Continous Deployment (CICD) principles. Included is a demonstration of a fully operational CICD pipeline using Gitea, Jenkins and Kubernetes.  The viewer can obtain the code used from Github to run the demonstration themselves on a local Docker Desktop installation with Kubernetes enabled.</li> </ul> </li> <li>CICD Reference Pipeline<ul> <li>This 30-minute video builds on the CICD Demonstration. Watch as a branch of a Github server profile repository uses pipelines to update a stack of Ping products at each code push.  Merging the branch to prod then updates the production environment accordingly.  The video and repository can serve as a launching point for using your own processes and tools for a similar experience.</li> </ul> </li> </ul>"},{"location":"videos/videos/#product-docker-images","title":"Product Docker Images","text":"<ul> <li>Upgrade PingCentral<ul> <li>This video demonstrates the upgrade of PingCentral in a containerized environment.</li> </ul> </li> <li>Build a Ping product image locally<ul> <li>This video demonstrates building a custom Ping product image using the Ping-provided build scripts and a downloaded product .zip file.</li> </ul> </li> <li>Ping product Docker image exploration<ul> <li>After viewing this video, you will have a deeper understanding of the structure of a Ping product Docker image filesystem. In addition, you will be shown ways of injecting configuration and customization into the image during the Kubernetes/Helm launch process along with an overview of the hook script functionality used by our images.  This video is intended for users with some familiarity with Ping product images.</li> </ul> </li> <li>Ping product image hook scripts<ul> <li>This video, intended for users with some familiarity with Ping product images, covers the hook script functionality included in all Ping product Docker images for managing product lifecycle in a containerized environment.</li> </ul> </li> </ul>"},{"location":"videos/videos/#platform","title":"Platform","text":"<ul> <li>Deploy a Local Openshift Cluster<ul> <li>This 12-minute video demonstrates the process of deploying an Openshift Local cluster to your environment, followed by installing a full stack of Ping products using our Helm charts.</li> </ul> </li> <li>Splunk Logging Example<ul> <li>This 20-minute video demonstrates the use of a Splunk Universal Forwarder (UF) sidecar to forward logs from PingAccess and PingFederate to a local Splunk instance.</li> </ul> </li> <li>Robust Local Kubernetes Cluster<ul> <li>This 25-minute video demonstrates creating a 3-node local Kubernetes cluster on VMs, including a load balancer, block storage, ingress and service mesh for testing and development.</li> </ul> </li> </ul>"}]}